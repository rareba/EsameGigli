install.packages('glmnet')
install.packages('mgcv')
install.packages('rpart')
install.packages('ROCR')
## Version: 2017.07.02 ## ## Remarks:  ## ################################################################################ ################################################################################ ## Clean ################################################################################ #### rm(list=ls(all=TRUE)) ################################################################################ ## Libraries and functions ################################################################################ library(MASS) library(glmnet) library(mgcv) library(rpart) library(ROCR) source("~/Cipo/Teaching/Master/MaBiDa/R/MBD2016-Functions-20160503.R") ################################################################################ ## Inputs ################################################################################ #### file.data <- "~/Cipo/data/Large/BankMarketing/bank-additional-full.csv" ################################################################################ ## Data ################################################################################ #### Variables # Bank client data: # 1 - age (numeric) # 2 - job: type of job (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown') # 3 - marital: marital status (categorical: 'divorced', 'married', 'single', 'unknown'; note: 'divorced' means divorced or widowed) # 4 - education (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown') # 5 - default: has credit in default? (categorical: 'no', 'yes', 'unknown') # 6 - housing: has housing loan? (categorical: 'no', 'yes', 'unknown') # 7 - loan: has personal loan? (categorical: 'no', 'yes', 'unknown') # # related with the last contact of the current campaign: # 8 - contact: contact communication type (categorical: 'cellular','telephone')  # 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec') # 10 - day_of_week: last contact day of the week (categorical: 'mon', 'tue', 'wed', 'thu', 'fri') # 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model. # Other attributes: # 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact) # 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted) # 14 - previous: number of contacts performed before this campaign and for this client (numeric) # 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success') # Social and economic context attributes # 16 - emp.var.rate: employment variation rate - quarterly indicator (numeric) # 17 - cons.price.idx: consumer price index - monthly indicator (numeric)  # 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)  # 19 - euribor3m: euribor 3 month rate - daily indicator (numeric) # 20 - nr.employed: number of employees - quarterly indicator (numeric) # Output variable (desired target): # 21 - y - has the client subscribed a term deposit? (binary: 'yes','no') #### Read full dataset data <- read.table(file = file.data, header = TRUE, sep = ";",    na.strings = "NA", colClasses = NA, check.names = FALSE, comment.char = "") #### Manage variables ## Dependent data$y <- ifelse(data$y == "no", 0, 1) ## pdays data$p01 <- ifelse(data$pdays == 999, 0, 1) data$p01 <- as.factor(data$p01) # data$pdays[data$pdays == 999] <- 0 ## previous data$previousF <- data$previous data$previousF[data$previous >=1] <- 1 data$previousF <- as.factor(data$previousF) ## default data$default[data$default == "yes"] <- "unknown" ## housing/loan # data$hl <- paste0(data$housing, ".", data$loan) ## Remove duration ind <- !( colnames(data) %in% c("duration", "housing", "loan") ) ind <- !( colnames(data) %in% "duration" ) data <- data[, ind, drop = FALSE] #### Split data in training (70%) and test (30%) nobs <- NROW(data) ind  <- sample.int( n = nobs, size = round( nobs * 0.70 ) ) train <- data[ ind, , drop = FALSE] test  <- data[-ind, , drop = FALSE] cat( "dim(data)  = ", dim(data),  "\n" ) cat( "dim(train) = ", dim(train), "\n" ) cat( "dim(test)  = ", dim(test),  "\n" ) ################################################################################ ## Modeling settings (useful to avoid repetitions across approaches) ################################################################################ #### Variables yVar <- "y" xVar <- c( "age", "job", "marital", "education", "default", "housing", "loan",    "contact", "month", "day_of_week",    "campaign", "pdays", "previousF", "poutcome",    "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed")
#### Null model formula <- as.formula( paste0( yVar, " ~ 1" ) ) null <- glm(formula = formula, data = train, family = "binomial") #### Full model formula <- as.formula( paste0( yVar, " ~ 1 + ", paste0(xVar, collapse = " + ")) ) full <- glm(formula = formula, data = train, family = "binomial")
## ## Created: 2017.07.04 ## ## Version: 2017.07.02 ## ## Remarks:  ## ################################################################################ ################################################################################ ## Clean ################################################################################ #### rm(list=ls(all=TRUE)) ################################################################################ ## Libraries and functions ################################################################################ library(MASS) library(glmnet) library(mgcv) library(rpart) library(ROCR) source("~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/MBD2016-Functions-20160503.R") ################################################################################ ## Inputs ################################################################################ #### file.data <- "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/data/bank-additional-full.csv" ################################################################################ ## Data ################################################################################ #### Variables # Bank client data: # 1 - age (numeric) # 2 - job: type of job (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown') # 3 - marital: marital status (categorical: 'divorced', 'married', 'single', 'unknown'; note: 'divorced' means divorced or widowed) # 4 - education (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown') # 5 - default: has credit in default? (categorical: 'no', 'yes', 'unknown') # 6 - housing: has housing loan? (categorical: 'no', 'yes', 'unknown') # 7 - loan: has personal loan? (categorical: 'no', 'yes', 'unknown') # # related with the last contact of the current campaign: # 8 - contact: contact communication type (categorical: 'cellular','telephone')  # 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec') # 10 - day_of_week: last contact day of the week (categorical: 'mon', 'tue', 'wed', 'thu', 'fri') # 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model. # Other attributes: # 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact) # 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted) # 14 - previous: number of contacts performed before this campaign and for this client (numeric) # 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success') # Social and economic context attributes # 16 - emp.var.rate: employment variation rate - quarterly indicator (numeric) # 17 - cons.price.idx: consumer price index - monthly indicator (numeric)  # 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)  # 19 - euribor3m: euribor 3 month rate - daily indicator (numeric) # 20 - nr.employed: number of employees - quarterly indicator (numeric) # Output variable (desired target): # 21 - y - has the client subscribed a term deposit? (binary: 'yes','no') #### Read full dataset data <- read.table(file = file.data, header = TRUE, sep = ";",    na.strings = "NA", colClasses = NA, check.names = FALSE, comment.char = "") #### Manage variables ## Dependent data$y <- ifelse(data$y == "no", 0, 1) ## pdays data$p01 <- ifelse(data$pdays == 999, 0, 1) data$p01 <- as.factor(data$p01) # data$pdays[data$pdays == 999] <- 0 ## previous data$previousF <- data$previous data$previousF[data$previous >=1] <- 1 data$previousF <- as.factor(data$previousF) ## default data$default[data$default == "yes"] <- "unknown" ## housing/loan # data$hl <- paste0(data$housing, ".", data$loan) ## Remove duration ind <- !( colnames(data) %in% c("duration", "housing", "loan") ) ind <- !( colnames(data) %in% "duration" ) data <- data[, ind, drop = FALSE] #### Split data in training (70%) and test (30%) nobs <- NROW(data) ind  <- sample.int( n = nobs, size = round( nobs * 0.70 ) ) train <- data[ ind, , drop = FALSE] test  <- data[-ind, , drop = FALSE] cat( "dim(data)  = ", dim(data),  "\n" ) cat( "dim(train) = ", dim(train), "\n" ) cat( "dim(test)  = ", dim(test),  "\n" ) ################################################################################ ## Modeling settings (useful to avoid repetitions across approaches) ################################################################################ #### Variables yVar <- "y" xVar <- c( "age", "job", "marital", "education", "default", "housing", "loan",    "contact", "month", "day_of_week",    "campaign", "pdays", "previousF", "poutcome",    "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed") ################################################################################ ## Stepwise selection ################################################################################ #### Null model formula <- as.formula( paste0( yVar, " ~ 1" ) ) null <- glm(formula = formula, data = train, family = "binomial") #### Full model formula <- as.formula( paste0( yVar, " ~ 1 + ", paste0(xVar, collapse = " + ")) ) full <- glm(formula = formula, data = train, family = "binomial")
fx <- FALSE    ## Try TRUE and FALSE; explain k  <- 12       ## Try different choices; explain formula <- y ~ job + marital + education + default + housing + loan + contact + month + day_of_week +  pdays + poutcome + nr.employed + emp.var.rate + cons.price.idx + cons.conf.idx + previousF + #formula <- y ~ job + marital + education + default + housing + contact + month + day_of_week +   # pdays + nr.employed + emp.var.rate + cons.price.idx + cons.conf.idx + previousF +   s(age,            bs = "cr", k = k, fx = fx) +    s(campaign,       bs = "cr", k = k, fx = fx) +    s(euribor3m,      bs = "cr", k = k, fx = fx)   fit <- gam(formula = formula, family = binomial, data = train, method = "P-ML",   scale = 0) fit.gam <- fit
fit
################################################################################ ## ## File: MBD2017-Bank-20170704.R ##                 ## Purpose: Bank Marketing data analysis. ## ## Created: 2017.07.04 ## ## Version: 2017.07.02 ## ## Remarks:  ## ################################################################################ ################################################################################ ## Clean ################################################################################ #### rm(list=ls(all=TRUE)) ################################################################################ ## Libraries and functions ################################################################################ library(MASS) library(glmnet) library(mgcv) library(rpart) library(ROCR) source("~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/MBD2016-Functions-20160503.R") ################################################################################ ## Inputs ################################################################################ #### file.data <- "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/data/bank-additional-full.csv" ################################################################################ ## Data ################################################################################ #### Variables # Bank client data: # 1 - age (numeric) # 2 - job: type of job (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown') # 3 - marital: marital status (categorical: 'divorced', 'married', 'single', 'unknown'; note: 'divorced' means divorced or widowed) # 4 - education (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown') # 5 - default: has credit in default? (categorical: 'no', 'yes', 'unknown') # 6 - housing: has housing loan? (categorical: 'no', 'yes', 'unknown') # 7 - loan: has personal loan? (categorical: 'no', 'yes', 'unknown') # # related with the last contact of the current campaign: # 8 - contact: contact communication type (categorical: 'cellular','telephone')  # 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec') # 10 - day_of_week: last contact day of the week (categorical: 'mon', 'tue', 'wed', 'thu', 'fri') # 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model. # Other attributes: # 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact) # 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted) # 14 - previous: number of contacts performed before this campaign and for this client (numeric) # 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success') # Social and economic context attributes # 16 - emp.var.rate: employment variation rate - quarterly indicator (numeric) # 17 - cons.price.idx: consumer price index - monthly indicator (numeric)  # 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)  # 19 - euribor3m: euribor 3 month rate - daily indicator (numeric) # 20 - nr.employed: number of employees - quarterly indicator (numeric) # Output variable (desired target): # 21 - y - has the client subscribed a term deposit? (binary: 'yes','no') #### Read full dataset data <- read.table(file = file.data, header = TRUE, sep = ";",    na.strings = "NA", colClasses = NA, check.names = FALSE, comment.char = "") #### Manage variables ## Dependent data$y <- ifelse(data$y == "no", 0, 1) ## pdays data$p01 <- ifelse(data$pdays == 999, 0, 1) data$p01 <- as.factor(data$p01) # data$pdays[data$pdays == 999] <- 0 ## previous data$previousF <- data$previous data$previousF[data$previous >=1] <- 1 data$previousF <- as.factor(data$previousF) ## default data$default[data$default == "yes"] <- "unknown" ## housing/loan # data$hl <- paste0(data$housing, ".", data$loan) ## Remove duration ind <- !( colnames(data) %in% c("duration", "housing", "loan") ) ind <- !( colnames(data) %in% "duration" ) data <- data[, ind, drop = FALSE] #### Split data in training (70%) and test (30%) nobs <- NROW(data) ind  <- sample.int( n = nobs, size = round( nobs * 0.70 ) ) train <- data[ ind, , drop = FALSE] test  <- data[-ind, , drop = FALSE] cat( "dim(data)  = ", dim(data),  "\n" ) cat( "dim(train) = ", dim(train), "\n" ) cat( "dim(test)  = ", dim(test),  "\n" ) ################################################################################ ## Modeling settings (useful to avoid repetitions across approaches) ################################################################################ #### Variables yVar <- "y" xVar <- c( "age", "job", "marital", "education", "default", "housing", "loan",    "contact", "month", "day_of_week",    "campaign", "pdays", "previousF", "poutcome",    "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "nr.employed") ################################################################################ ## Stepwise selection ################################################################################ #### Null model formula <- as.formula( paste0( yVar, " ~ 1" ) ) null <- glm(formula = formula, data = train, family = "binomial") #### Full model formula <- as.formula( paste0( yVar, " ~ 1 + ", paste0(xVar, collapse = " + ")) ) full <- glm(formula = formula, data = train, family = "binomial") #### FORWARD #### Use k = 2 for AIC, k = log(NROW(data)) for BIC forward <- step(object = null, scope = list(lower = null, upper = full),    direction = "forward", k = 2) # #### BACKWARD # #### Use k = 2 for AIC, k = log(NROW(data)) for BIC # backward <- step(object = full, scope = list(lower = null, upper = full),  #   direction = "backward", k = 2) #  # #### BOTH # #### Use k = 2 for AIC, k = log(NROW(data)) for BIC # both <- step(object = null, scope = list(lower = null, upper = full),  #   direction = "both", k = 2) #### Look at: ## -> anova components; summary;  ## -> Comment: same final model in all cases ## -> age, campaigns, may deserve a better treatment (group it in categories?) ## -> Some clashes between pdays / previous and pdays / poutcome cast doubts on  ##    pdays ################################################################################ ## GAM  ################################################################################ #### Fit fx <- FALSE    ## Try TRUE and FALSE; explain k  <- 12       ## Try different choices; explain formula <- y ~ job + marital + education + default + housing + loan + contact + month + day_of_week +  pdays + poutcome + nr.employed + emp.var.rate + cons.price.idx + cons.conf.idx + previousF + #formula <- y ~ job + marital + education + default + housing + contact + month + day_of_week +   # pdays + nr.employed + emp.var.rate + cons.price.idx + cons.conf.idx + previousF +   s(age,            bs = "cr", k = k, fx = fx) +    s(campaign,       bs = "cr", k = k, fx = fx) +    s(euribor3m,      bs = "cr", k = k, fx = fx)   fit <- gam(formula = formula, family = binomial, data = train, method = "P-ML",   scale = 0) fit.gam <- fit #### Give a look to summary(); a look to plot # plot(x = fit, residuals = TRUE, rug = TRUE, se = TRUE, pages = 4, scale = -1) #### So tedious? # test.all <- .gam.ftest.all(fit = fit) #### Some model cleaning could be done ################################################################################ ## Ridge Regression ################################################################################ #### Ridge alpha <- 0 #### Data ## Dependent variable y <- train[, yVar] ## Independent variables (needed since 'glmnet' wants numerical matrices!) x <- .indVars(data = train, xVar = xVar, constant = FALSE) # #### Fit (more options omitted) # fit <- glmnet(x = x, y = y, family = "binomial", alpha = alpha,  #   nlambda = 100, standardize = TRUE) # fit.ridge <- fit # #### Print output (Explain columns Df and %Dev) # # print(fit) #  # #### Plot trajectories of coefficients # par(mfrow = c(1,3)) # plot(x = fit, xvar = "lambda", label = TRUE) ## lambda # plot(x = fit, xvar = "norm",   label = TRUE) ## L1-norm # plot(x = fit, xvar = "dev",    label = TRUE) ## R^2 #  # #### Print coefficients (A lot of numbers... why?) # # print(coef(fit)) #  # #### Selecting coefficients # ## 's' = lambda... but 'exact'? # # print( coef(fit, s = 0.1, exact = TRUE) ) #### Which one is the best lambda? cvfit <- cv.glmnet(x = x, y = y, alpha = alpha,             # lambda = lambda,    family = "binomial", type.measure = "deviance", nfolds = 20) par( mfrow = c(1,1) ) plot(cvfit) ## Print best lambdas cat("min(lambda) = ", cvfit$lambda.min, "1se(lambda) = ", cvfit$lambda.1se, "\n") #### Estimate best lambda <- cvfit$lambda.1se  fit <- glmnet(x = x, y = y, family = "binomial", alpha = alpha,    lambda = lambda, standardize = TRUE) #### Store lambda.best.ridge <- lambda fit.best.ridge <- fit ################################################################################ ## Lasso Regression ################################################################################ #### Lasso alpha <- 1 #### Data ## Dependent variable y <- train[, yVar] ## Independent variables (needed since 'glmnet' wants numerical matrices!) x <- .indVars(data = train, xVar = xVar, constant = FALSE) # #### Fit (more options omitted) # # lambda <- exp( seq(from = -8, to = 2, by = 0.1) ) # fit <- glmnet(x = x, y = y, family = "binomial", alpha = alpha,  #   nlambda = 100, standardize = TRUE) # fit.lasso <- fit #  # #### Plot trajectories of coefficients # par(mfrow = c(1,2)) # plot(x = fit.lasso, xvar = "lambda", label = TRUE, main = "Lasso") ## lambda Lasso # plot(x = fit.ridge, xvar = "lambda", label = TRUE, main = "Ridge") ## lambda Ridge #### Select the best lambda cvfit <- cv.glmnet(x = x, y = y, alpha = alpha, family = "binomial",    type.measure = "deviance", nfolds = 20) plot(cvfit) ## Print best lambdas cat("min(lambda) = ", cvfit$lambda.min, "1se(lambda) = ", cvfit$lambda.1se, "\n") #### Estimate best lambda <- cvfit$lambda.1se  fit <- glmnet(x = x, y = y, family = "binomial", alpha = alpha,    lambda = lambda, standardize = TRUE) #### Store lambda.best.lasso <- lambda fit.best.lasso <- fit ################################################################################ ## Recursive Partitioning: 1st try ################################################################################ #### Formula formula <- as.formula( paste0( yVar, " ~ ", paste0(xVar, collapse = " + ") ) ) #### First try using default settings ## Here a selection of the main control parameters control <- rpart.control(   minsplit = 20, ## Minimum number of observations in a node (group)   cp = 0.01,     ## Minimum cp decrease: any split not decreasing "rel error"                   ##  by a factor of cp is not attempted                  ## With "anova", the overall R-squared must increase by cp                   ##  at each step.    xval = 10)     ## Number of cross-validations to compute xerror ## Fit fit <- rpart(formula = formula, data = train, method = "class", control = control,    model = TRUE)  ## model = TRUE useful for kfold-cv #### Plot #### Try options uniform = TRUE and branch = 0.5 par(mfrow = c(1,1)) plot(x = fit, uniform = FALSE, branch = 1, compress = FALSE, margin = 0, minbranch = 0.3)  text(fit)        ## Adds text, values and labels #### Print print(fit) #### A deeper look to the main summary table ## Columns: ##  cp        = Complexity Parameter = Decrease in rel error (below)  ##  rel error = 1 - R^2 ##  xerror    = PRESS statistics (cf slide XXX) ##  xstd      = To compute the 1-se minimum rule (it gives nsplit = 4 here)  printcp(fit) ## Where to stop the tree? min(xerror) or (better) 1-se minimum rule  ## Plot cp vs rel error # plotcp(fit, minline = FALSE) #### Store last model fit.rpart1  <- fit ################################################################################ ## Recursive Partitioning: 2nd try ################################################################################ #### Second try: cp decreased from 0.01 (default) to 0.001 control <- rpart.control(   minsplit = 20, ## Minimum number of observations in a node   cp = 0.001,    ## Minimum cp decrease   xval = 10)     ## Number of cross-validations for xerror fit <- rpart(formula = formula, data = train, method = "class", control = control,   model = TRUE, x = FALSE, y = TRUE) #### Plot cp vs rel error par(mfrow = c(1,1)) plotcp(fit, minline = TRUE) #### Look to the main summary table par(mfrow = c(1,2)) rsq.rpart(fit) ## Another useful plot (Relative vs Apparent) #### Could the tree be pruned? ## How to chose the best: ## 1) min(xerror) ind <- which.min(fit$cptable[, "xerror"]) cp.best <- fit$cptable[ind, "CP"] ## 2) largest xerror s.t xerror <= min(xerror - xstd) (called 1se-rule) ind  <- fit$cptable[, "xerror"] <= min(fit$cptable[, "xerror"] + fit$cptable[, "xstd"]) ind1 <- which.max(fit$cptable[ind, "xerror"]) cp.best <- fit$cptable[ind, "CP"][ind1] #### Prune fit.rpart2 <- prune(tree = fit, cp = cp.best)   ## Prune #### Plot #### Try options uniform = TRUE and branch = 0.5 par(mfrow = c(1,1)) plot(x = fit, uniform = FALSE, branch = 1, compress = FALSE, margin = 0, minbranch = 0.3)  text(fit)        ## Adds text, values and labels #### Print print(fit) #### A deeper look to the main summary table ## Columns: ##  cp        = Complexity Parameter = Decrease in rel error (below)  ##  rel error = 1 - R^2 ##  xerror    = PRESS statistics (cf slide XXX) ##  xstd      = To compute the 1-se minimum rule (it gives nsplit = 4 here)  printcp(fit) ################################################################################ ## Final check: k-fold cv ################################################################################ #### Settings k <- 10 seed <- 100000 #### Linear Model cv.forward  <- .kfoldcv(k = k, model = forward,  seed = seed) # cv.backward <- .kfoldcv(k = k, model = backward, seed = seed) # cv.both     <- .kfoldcv(k = k, model = both,     seed = seed) #### GAM Model # cv.gam <- .kfoldcv(k = k, model = fit.gam, seed = seed) #### glmnet Based Models y <- train[, yVar] x <- .indVars(data = train, xVar = xVar, constant = FALSE) cv.ridge <- .kfoldcv.glmnet(k = k, x = x, y = y, lambda = lambda.best.ridge,    family = "binomial", alpha = 0, seed = seed) cv.lasso <- .kfoldcv.glmnet(k = k, x = x, y = y, lambda = lambda.best.lasso,    family = "binomial", alpha = 1, seed = seed) #### rpart models cv.rpart1 <- .kfoldcv(k = k, model = fit.rpart1, seed = seed) cv.rpart2 <- .kfoldcv(k = k, model = fit.rpart2, seed = seed) #### Join x1 <- c("forward", "ridge", "lasso", "rpart1", "rpart2") x2 <- rbind(cv.forward, cv.ridge, cv.lasso, cv.rpart1, cv.rpart2) kfold <- data.frame(Model = x1, x2, check.names = FALSE) cat("k-fold CV", "\n") print(kfold)
load("~/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esercitazione 7 Luglio 2017/otoluidineCS.RData")
load("otoluidineCS.RData")
load("~/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esercitazione 7 Luglio 2017/otoluidineCS.RData")
subset(workDF$doseN[workDF$doseN == 1000]        )
subset(workDF$doseN == 1000        )
subset(workDF$doseN[workDF$doseN == 1000]        )
subset(workDF$doseN, workDF$doseN == 1000)
x = subset(workDF$doseN, workDF$doseN == 1000)
x = subset(workDF$doseN, workDF$doseN == 1000, select = c(count, dish, dose, VC, doseN, Nishi, countOr))
x = subset(workDF$doseN, workDF$doseN == 1000, select = c(count, dish, dose, VC, doseN, Nishi, countOr))
x = subset(workDF, workDF$doseN == 1000, select = c(count, dish, dose, VC, doseN, Nishi, countOr))
x = subset(workDF, workDF$doseN == 1000)
summary(df)
summary(df)
summary(df)
df = subset(workDF, workDF$doseN == 1000)
str(df)
summary(df)
var(df$count)
var(df$count)
var(df$count) * 9 / 10
modPoisson = glm(count ~ 1, data = df, family = "poisson")
coefficients(modPoisson)
lambdadot = mean(df$count)
barplot(ygrid, names.arg = paste(xgrid))
xgrid = 0:7
ygrid = dpois(xgrid, lambdadot)
barplot(ygrid, names.arg = paste(xgrid))
barplot(ygrid, names.arg = paste(xgrid))
require(rstan)
mydata = list()
mydata$yobs = df$count
mydata$n = length(mydata$yobs)
names(mydata)
mydata = list() mydata$yobs = df$count mydata$N = length(mydata$yobs) names(mydata)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
mydata = list()
mydata$yobs = df$count
mydata$N = length(mydata$yobs)
names(mydata)
modPG =  '
data{     int<lower=1> N;     int<lower=0> Yobs[N] }
transformed data{}
parameters{     real < lower = 0 > lambda; }
transformed parameters{}
model{     for(osser in 1:N)     {         Yobs[osser] ~    poisson(lambda);     }     lambda ~ uniform(0,35) }
'
modello =  ' data{     int<lower=1> N;     int<lower=0> Yobs[N] } transformed data{} parameters{     real < lower = 0 > lambda; } transformed parameters{} model{     for(osser in 1:N)     {         Yobs[osser] ~    poisson(lambda);     }     lambda ~ uniform(0,35) } '
inizializza = function() {     list(lambda = 7+runif(1,-6,6)) }
load("~/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esercitazione 7 Luglio 2017/otoluidineCS.RData") # Prendo le osservazioni con doseN = 1000 df = subset(workDF, workDF$doseN == 1000) # Calcolo media, varianza e varianza ML lambdadot = mean(df$count) var(df$count) var(df$count) * 9 / 10 modPoisson = glm(count ~ 1, data = df, family = "poisson") coefficients(modPoisson) xgrid = 0:7 ygrid = dpois(xgrid, lambdadot) barplot(ygrid, names.arg = paste(xgrid)) require(rstan) rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) mydata = list() mydata$yobs = df$count mydata$N = length(mydata$yobs) names(mydata) modello =  ' data{     int<lower=1> N;     int<lower=0> Yobs[N] } transformed data{} parameters{     real < lower = 0 > lambda; } transformed parameters{} model{     for(osser in 1:N)     {         Yobs[osser] ~    poisson(lambda);     }     lambda ~ uniform(0,35) } ' inizializza = function() {     list(lambda = 7+runif(1,-6,6)) } iniTime = date() fit = stan(            model_code = modello,            data = mydata,            iter = 15000,            chains = 3,            warmup = 5000,            thin = 5,            init = inizializza()            ) endTime <- date();c(iniTime = iniTime, endTime = endTime) pairs(fit) summary(fit) plot(fit) traceplot(fit) getwd()
load("~/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esercitazione 7 Luglio 2017/otoluidineCS.RData") # Prendo le osservazioni con doseN = 1000 df = subset(workDF, workDF$doseN == 1000) # Calcolo media, varianza e varianza ML lambdadot = mean(df$count) var(df$count) var(df$count) * 9 / 10 modPoisson = glm(count ~ 1, data = df, family = "poisson") coefficients(modPoisson) xgrid = 0:7 ygrid = dpois(xgrid, lambdadot) barplot(ygrid, names.arg = paste(xgrid)) require(rstan) rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) mydata = list() mydata$yobs = df$count mydata$N = length(mydata$yobs) names(mydata) modello =  ' data{     int<lower=1> N;     int<lower=0> Yobs[N]; } transformed data{} parameters{     real < lower = 0 > lambda; } transformed parameters{} model{     for(osser in 1:N)     {         Yobs[osser] ~    poisson(lambda);     }     lambda ~ uniform(0,35) } ' inizializza = function() {     list(lambda = 7+runif(1,-6,6)) } iniTime = date() fit = stan(            model_code = modello,            data = mydata,            iter = 15000,            chains = 3,            warmup = 5000,            thin = 5,            init = inizializza()            ) endTime <- date();c(iniTime = iniTime, endTime = endTime) pairs(fit) summary(fit) plot(fit) traceplot(fit) getwd()
load("~/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esercitazione 7 Luglio 2017/otoluidineCS.RData") # Prendo le osservazioni con doseN = 1000 df = subset(workDF, workDF$doseN == 1000) # Calcolo media, varianza e varianza ML lambdadot = mean(df$count) var(df$count) var(df$count) * 9 / 10 modPoisson = glm(count ~ 1, data = df, family = "poisson") coefficients(modPoisson) xgrid = 0:7 ygrid = dpois(xgrid, lambdadot) barplot(ygrid, names.arg = paste(xgrid)) require(rstan) rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) mydata = list() mydata$yobs = df$count mydata$N = length(mydata$yobs) names(mydata) modello =  ' data{     int<lower=1> N;     int<lower=0> Yobs[N]; } transformed data{} parameters{     real < lower = 0 > lambda; } transformed parameters{} model{     for(osser in 1:N)     {         Yobs[osser] ~    poisson(lambda);     }     lambda ~ uniform(0,35); } ' inizializza = function() {     list(lambda = 7+runif(1,-6,6)) } iniTime = date() fit = stan(            model_code = modello,            data = mydata,            iter = 15000,            chains = 3,            warmup = 5000,            thin = 5,            init = inizializza()            ) endTime <- date();c(iniTime = iniTime, endTime = endTime) pairs(fit) summary(fit) plot(fit) traceplot(fit) getwd()
load("~/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esercitazione 7 Luglio 2017/otoluidineCS.RData") # Prendo le osservazioni con doseN = 1000 df = subset(workDF, workDF$doseN == 1000) # Calcolo media, varianza e varianza ML lambdadot = mean(df$count) var(df$count) var(df$count) * 9 / 10 modPoisson = glm(count ~ 1, data = df, family = "poisson") coefficients(modPoisson) xgrid = 0:7 ygrid = dpois(xgrid, lambdadot) barplot(ygrid, names.arg = paste(xgrid)) require(rstan) rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) mydata = list() mydata$Yobs = df$count mydata$N = length(mydata$yobs) names(mydata) modello =  ' data{     int<lower=1> N;     int<lower=0> Yobs[N]; } transformed data{} parameters{     real < lower = 0 > lambda; } transformed parameters{} model{     for(osser in 1:N)     {         Yobs[osser] ~    poisson(lambda);     }     lambda ~ uniform(0,35); } ' inizializza = function() {     list(lambda = 7+runif(1,-6,6)) } iniTime = date() fit = stan(            model_code = modello,            data = mydata,            iter = 15000,            chains = 3,            warmup = 5000,            thin = 5,            init = inizializza()            ) endTime <- date();c(iniTime = iniTime, endTime = endTime) pairs(fit) summary(fit) plot(fit) traceplot(fit) getwd()
load("~/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esercitazione 7 Luglio 2017/otoluidineCS.RData") # Prendo le osservazioni con doseN = 1000 df = subset(workDF, workDF$doseN == 1000) # Calcolo media, varianza e varianza ML lambdadot = mean(df$count) var(df$count) var(df$count) * 9 / 10 modPoisson = glm(count ~ 1, data = df, family = "poisson") coefficients(modPoisson) xgrid = 0:7 ygrid = dpois(xgrid, lambdadot) barplot(ygrid, names.arg = paste(xgrid)) require(rstan) rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) mydata = list() mydata$Yobs = df$count mydata$N = length(mydata$Yobs) names(mydata) modello =  ' data{     int<lower=1> N;     int<lower=0> Yobs[N]; } transformed data{} parameters{     real < lower = 0 > lambda; } transformed parameters{} model{     for(osser in 1:N)     {         Yobs[osser] ~    poisson(lambda);     }     lambda ~ uniform(0,35); } ' inizializza = function() {     list(lambda = 7+runif(1,-6,6)) } iniTime = date() fit = stan(            model_code = modello,            data = mydata,            iter = 15000,            chains = 3,            warmup = 5000,            thin = 5,            init = inizializza()            ) endTime <- date();c(iniTime = iniTime, endTime = endTime) pairs(fit) summary(fit) plot(fit) traceplot(fit) getwd()
inizia
inizializza()
load("~/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esercitazione 7 Luglio 2017/otoluidineCS.RData") # Prendo le osservazioni con doseN = 1000 df = subset(workDF, workDF$doseN == 1000) # Calcolo media, varianza e varianza ML lambdadot = mean(df$count) var(df$count) var(df$count) * 9 / 10 modPoisson = glm(count ~ 1, data = df, family = "poisson") coefficients(modPoisson) xgrid = 0:7 ygrid = dpois(xgrid, lambdadot) barplot(ygrid, names.arg = paste(xgrid)) require(rstan) rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) mydata = list() mydata$Yobs = df$count mydata$N = length(mydata$Yobs) names(mydata) modello =  ' data{     int<lower=1> N;     int<lower=0> Yobs[N]; } transformed data{} parameters{     real < lower = 0 > lambda; } transformed parameters{} model{     for(osser in 1:N)     {         Yobs[osser] ~    poisson(lambda);     }     lambda ~ uniform(0,35); } ' inizializza = function() {     list(lambda = 7+runif(1,-6,6)) } iniTime = date() fit = stan(            model_code = modello,            data = mydata,            iter = 15000,            chains = 3,            warmup = 5000,            thin = 5,            init = inizializza()            ) endTime <- date(); c(iniTime = iniTime, endTime = endTime) pairs(fit) summary(fit) plot(fit) traceplot(fit) getwd()
fit = stan(            model_code = modello,            data = mydata,            iter = 15000,            chains = 3,            warmup = 5000,            thin = 5,            init = inizializza()            )
require(ggmcmc)
library(ISLR)
install.packages('ISLR')
library(ISLR)
names(DEfault)
names(Default)
Default[1:4,]
head(Default)
summary(Default)
Default$ID = seq.int(nrow(Default))
    data = Default, xlab="Studente", ylab="Movimenti CC")
    ylab="Movimenti CC")
boxplot(balanece = student,     data = Default,     xlab = "Studente",     ylab="Movimenti CC")
# Grafici boxplot(balance ~ student,     data = Default,     xlab = "Studente", ylab = "Movimenti CC")
ylab = "Movimenti CC")
boxplot(income ~ student,     data = Default,     xlab = "Studente", ylab = "Movimenti CC")
ylab = "Movimenti CC")
# Income e Student boxplot(income ~ default,     data = Default,     xlab = "Studente", ylab = "Movimenti CC")
plot(Default$income,Default$balance)
cor.test(Default$income,Default$balance)
x = table(Default$default, Default$student)
table(Default$default, Default$student)
mosaicplot(x, main="Default Data",col=T)
Default$y = ifelse(Default$default == "Yes", c(1), c(0))
table(Default$y, Default$default)
summary(Default$y)
set.seed(223) sequenz = sample(nrow(Default)) sequenz[1:10] sequenz = data.frame(sequenz) Default = merge(sequenz, Default, by.x = "sequenz", by.y = "ID", sort = FALSE) head(Default) DefaultTr = Default[1:8000] DefaultTe = Default[8001:10000] summary(DefaultTr$y) summary(DefaultTe$y)
DefaultTr = Default[1:8000]
DefaultTr = Default[1:8000,]
DefaultTe = Default[8001:10000,]
summary(DefaultTr$y)
summary(DefaultTe$y)
mlogit3 = glm(default ~ balanceMethodsList + income + factor(student), data = DefaultTR, family = "binomial")
mlogit3 = glm(default ~ balanceMethodsList + income + factor(student), data = DefaultTr, family = "binomial")
mlogit3 = glm(default ~ balance + income + factor(student), data = DefaultTr, family = "binomial")
confint(mlogit3)
anova(mlogit3, test="Chisq")
stud = ifelse(Default$student == "Yes", c(1), c(0))
table(stud, default$student)
table(stud, Default$student)
exp(coef(mlogit3))[1] + coef(mlogit3)[2] * mean(Default$balance) + coef(mlogit3)[3] * mean(Default$income) + coef(mlogit3)[4] * mean(stud))
exp(coef(mlogit3)[1] + coef(mlogit3)[2] * mean(Default$balance) + coef(mlogit3)[3] * mean(Default$income) + coef(mlogit3)[4] * mean(stud))
odd = exp(coef(mlogit3)[1] + coef(mlogit3)[2] * mean(Default$balance) + coef(mlogit3)[3] * mean(Default$income) + coef(mlogit3)[4] * mean(stud))
prob default = odd / (1+odd)
prob_default = odd / (1+odd)
prob_default = (odd / (1+odd))*100
odd_nd = exp(coef(mlogit3)[1] + coef(mlogit3)[2] * (mean(Default$balance)+1000) + coef(mlogit3)[3] * mean(Default$income) + coef(mlogit3)[4] * mean(stud))
odd_nd = exp(coef(mlogit3)[1] + coef(mlogit3)[2] * (mean(Default$balance)+1000) + coef(mlogit3)[3] * mean(Default$income) + coef(mlogit3)[4] * mean(stud))
prob_non_default = (odd_nd / (1 + odd_nd)) * 100
predict = predict(mlogit3, type="response")
table(DefaultTr$default, predict > 0.5)
predictTest = predict(mlogit3, DefaultTe, type = "response")
table(DefaultTe$default, predictTest > 0.5)
require(ROCR)
summary(mlogit3)
library(ISLR)
#Descrizione del data set: variabili, dimensione, statistiche descrittive
#Weekly ha 4 colonne con info su reddito di 10000 soggeti num movimento carta di credito, 
#se sono studenti e se sono andati in Weekly usando la carta di credito.
#a chi meglio dare la carta o no. Reg logistica fa risolvere problemi di classificazione,
#come albero decisionale ma inmodo diverso.
#svm sempre tra metodi di classificazione, se un record appartiene ad ua classe o ad un'altra.
names(Weekly)
View(Weekly) #dataset in forma tabellare
dim(Weekly)
str(Weekly) #struttura, quali sono le variabili e di che natura sono, è variabile dicotomica a 
#a due livelli (yes, no)balance, income
summary(Weekly) #
Weekly[1:4,]
Weekly$ID <- seq.int(nrow(Weekly)) #creo variabile ID e l'attacco subito come colonna
#al dataset, infatti nella nuova struttura del dataset compare l'ID.
#Analisi grafiche: analise di tipo bivariato, considerando coppie di variabili,
#per vedere relazioni. Due variabili discrete e due continuue e quindi faccio 
#boxplot
#un dataset sono tutte variabili continue e conviene fare pairs
boxplot(balance ~ student, data = Weekly, main = "Weekly Data",         col = (c("blue", "orange")),         xlab = "Studente", ylab = "Movimenti carta di credito")
require(ISLR)
library(ISLR)
names(Weekly)
library(ISLR)
names(Weekly)
View(Weekly) #dataset in forma tabellare
library(ISLR) names(Weekly) View(Weekly) #dataset in forma tabellare dim(Weekly) str(Weekly) #struttura, quali sono le variabili e di che natura sono, è variabile dicotomica a  #a due livelli (yes, no)balance, income summary(Weekly) # Weekly[1:4,] Weekly$ID <- seq.int(nrow(Weekly))
pairs(Weekly)
plot(Weekly)
plot(Weekly)
plot(Weekly)
pairs(Weekly)
cor.test(Weekly$Year, Weekly$Volume)
mod <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = "binomial")
summary(mod)
stud = ifelse(Default$student == "Yes", c(1), c(0))
table(stud, Default$student)
up = ifelse(Weekly$Direction == "Up", c(1), c(0))
table(up, Weekly$Direction)
trainset = (Weekly$Year <= 2008)
testset = Weekly[!trainset,]
glm.fit.d <- glm(Direction ~ Lag2, data = Weekly, subset = trainset, family = "binomial")
glm.probs.d <- predict(glm.fit.d, type = "response", newdata = testset)
glm.preds.d <- ifelse(glm.probs.d > .5, "Up", "Down")
cm.d <- table(testset$Direction, glm.preds.d)
cm.d
library(ISLR) # A1 names(Weekly) View(Weekly) dim(Weekly) str(Weekly) summary(Weekly) head(Weekly) pairs(Weekly) cor.test(Weekly$Year, Weekly$Volume) # Si osserva la relazione fra Volume e Year # A2 mod <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = "binomial") summary(mod) # A3 up = ifelse(Weekly$Direction == "Up", c(1), c(0)) table(up, Weekly$Direction) odds = exp(coef(mod)[1] +     coef(mod)[2] * mean(Weekly$Lag1) +     coef(mod)[3] * mean(Weekly$Lag2) +     coef(mod)[4] * mean(Weekly$Lag3) +     coef(mod)[5] * mean(Weekly$Lag4) +     coef(mod)[6] * mean(Weekly$Lag5) +     coef(mod)[7] * (mean(Weekly$Volume) + 1) * mean(up)  ) prob = (odds / (1 + odds)) * 100 prob # A4 pred1 = predict(mod, type = "response") table(Weekly$Volume, pred1 > 0.5) # A5 trainset = (Weekly$Year <= 2008) testset = Weekly[!trainset,] glm.fit.d <- glm(Direction ~ Lag2, data = Weekly, subset = trainset, family = "binomial") glm.probs.d <- predict(glm.fit.d, type = "response", newdata = testset) glm.preds.d <- ifelse(glm.probs.d > .5, "Up", "Down") cm.d <- table(testset$Direction, glm.preds.d) cm.d
pred1 = predict(mod, type = "response") table(Weekly$Direction, pred1 > 0.5)
pred1 = predict(mod, type = "response") table(Weekly$Direction, pred1 > 0.5)
up = ifelse(Weekly$Direction == "Up", c(1), c(0)) table(up, Weekly$Direction)
prob
pred1 = predict(mod, type = "response") pred2 <- ifelse(pred1 > .5, "Up", "Down") table(Weekly$Direction, pred2)
#B1 remove(list = ls()) data(Auto) Auto$mpg01 <- with(ifelse(mpg > median(mpg), "1", "0"), data = Auto) #B2 require(ggplot2) attach(Auto) # Boxplots par(mfrow = c(2, 3)) for (i in names(Auto)) {     # excluding the own mpgs variables and others categorical variables     if (grepl(i, pattern = "^mpg|cylinders|origin|name")) { next }     boxplot(eval(parse(text = i)) ~ mpg01, ylab = i, col = c("red", "blue")) } # for the categorical variables i do barplots colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan") par(mfrow = c(1, 2)) for (i in c("cylinders", "origin")) {     aux <- table(eval(parse(text = i)), mpg01)     cols <- colors[1:nrow(aux)]     barplot(aux, xlab = "mpg01", ylab = i, beside = T, legend = rownames(aux), col = cols) } pairs(Auto[, -9]) #B3 set.seed(654) rows <- sample(x = nrow(Auto), size = .80 * nrow(Auto)) trainset <- Auto[rows,] testset <- Auto[-rows,] #B4 lr.fit <- glm(as.factor(mpg01) ~ displacement + horsepower + weight + acceleration + year + cylinders + origin, data = trainset, family = "binomial") lr.probs <- predict(lr.fit, testset, type = "response") lr.pred <- ifelse(lr.probs > 0.5, "1", "0") table(testset$mpg01, lr.pred)
pairs(Auto)
pairs(Auto[,-9])
pairs(Auto[,0:8])
#B1 remove(list = ls()) data(Auto) Auto$mpg01 <- with(ifelse(mpg > median(mpg), "1", "0"), data = Auto) #B2 require(ggplot2) attach(Auto) # Boxplots par(mfrow = c(2, 3)) for (i in names(Auto)) {     # excluding the own mpgs variables and others categorical variables     if (grepl(i, pattern = "^mpg|cylinders|origin|name")) { next }     boxplot(eval(parse(text = i)) ~ mpg01, ylab = i, col = c("red", "blue")) } # for the categorical variables i do barplots colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan") par(mfrow = c(1, 2)) for (i in c("cylinders", "origin")) {     aux <- table(eval(parse(text = i)), mpg01)     cols <- colors[1:nrow(aux)]     barplot(aux, xlab = "mpg01", ylab = i, beside = T, legend = rownames(aux), col = cols) } pairs(Auto[,0:8]) #B3 set.seed(654) rows <- sample(x = nrow(Auto), size = .80 * nrow(Auto)) trainset <- Auto[rows,] testset <- Auto[-rows,] #B4 lr.fit <- glm(as.factor(mpg01) ~ displacement + horsepower + weight + acceleration + year + cylinders + origin, data = trainset, family = "binomial") lr.probs <- predict(lr.fit, testset, type = "response") lr.pred <- ifelse(lr.probs > 0.5, "1", "0") table(testset$mpg01, lr.pred)
for (i in c("cylinders", "origin")) {     aux <- table(eval(parse(text = i)), mpg01)     cols <- colors[1:nrow(aux)]     plot(aux, xlab = "mpg01", ylab = i, beside = T, legend = rownames(aux), col = cols) }
colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan") par(mfrow = c(1, 2)) for (i in c("cylinders", "origin")) {     aux <- table(eval(parse(text = i)), mpg01)     cols <- colors[1:nrow(aux)]     plot(aux, xlab = "mpg01", ylab = i, beside = T, legend = rownames(aux), col = cols) }
colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan") par(mfrow = c(1, 2)) for (i in c("cylinders", "origin")) {     aux <- table(eval(parse(text = i)), mpg01)     cols <- colors[1:nrow(aux)]     barplot(aux, xlab = "mpg01", ylab = i, beside = T, legend = rownames(aux), col = cols) }
data_trip<-read.csv("trip_data_4.csv",sep=',', header=1, nrows = 20000)
data_trip<-read.csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/MDS-I/03 Business Dashboard in R/Data/trip_data_4.csv",sep=',', header=1, nrows = 20000)
library(shiny) runApp()
library(psych)
library(dplyr)
library(ggmap)
data_fares<-read.csv("/Data/trip_fare_4.csv",sep=',', header=1, nrows = 10000)
setwd(" ~ / Visual Studio 2017 / Projects / MABIDA2017 / Gigli / MDS - I / 03 Business Dashboard in R / ")
data_fares <- read.csv(" ~ / Visual Studio 2017 / Projects / MABIDA2017 / Gigli / MDS - I / 03 Business Dashboard in R / /Data/trip_fare_4.csv", sep = ',', header = 1, nrows = 10000)
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/MDS-I/03 Business Dashboard in R/")
data_fares<-read.csv("/Data/trip_fare_4.csv",sep=',', header=1, nrows = 10000)
data_fares<-read.csv("Data/trip_fare_4.csv",sep=',', header=1, nrows = 10000)
data_trip<-read.csv("Data/trip_data_4.csv",sep=',', header=1, nrows = 20000)
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/MDS-I/03 Business Dashboard in R/")
library(psych)
library(dplyr)
library(ggmap)
#read fare_data_4
data_fares<-read.csv("Data/trip_fare_4.csv",sep=',', header=1, nrows = 10000)
head(data_fares)
data_fares$medallion<-NULL
data_fares$vendor_id<-NULL
data_fares$payment_type<-NULL
#read trip_data_4
data_trip<-read.csv("Data/trip_data_4.csv",sep=',', header=1, nrows = 20000)
summary(data_trip)
#remove columns I'm not going to use in the following
data_trip$medallion<-NULL
data_trip$vendor_id<-NULL
data_trip$store_and_fwd_flag<-NULL
data_trip$rate_code<-NULL
data_trip$dropoff_datetime<-NULL
#create a column for pickup_hour
data_trip$pickup_hour<-as.POSIXlt(data_trip$pickup_datetime)$hour
#create a column for week 
data_trip$week<-as.numeric(format(as.Date(data_trip$pickup_datetime),"%U"))
#create a column for day
data_trip$day<-mapply(FUN = function(x) substring(x, 1, 10), data_trip$pickup_datetime) #substr(x, start, stop)
#exclude trip with time less than 60 seconds
data_trip<-data_trip[(data_trip$trip_time_in_secs)>60,]
#exclude trip with distance less than 0.1 miles
data_trip<-data_trip[(data_trip$trip_distance)>0.1,]
#dummy for detecting trip starting and ending at JFK airport
data_trip$jfk<-as.numeric((data_trip$pickup_latitude<40.665&data_trip$pickup_latitude>40.635)&
                            (data_trip$pickup_longitude<(-73.75)&data_trip$pickup_longitude>(-73.82))&
                            (data_trip$dropoff_latitude<40.665&data_trip$dropoff_latitude>40.635)&
                            (data_trip$dropoff_longitude<(-73.75)&data_trip$dropoff_longitude>(-73.82)))
#dummy for detecting trip starting at JFK airport
data_trip$jfk_pickup<-as.numeric((data_trip$pickup_latitude<40.665&data_trip$pickup_latitude>40.635)&
                                   (data_trip$pickup_longitude<(-73.75)&data_trip$pickup_longitude>(-73.82)))
#dummy for detecting trip ending at JFK airport
data_trip$jfk_dropoff<-as.numeric((data_trip$dropoff_latitude<40.665&data_trip$dropoff_latitude>40.635)&
                                    (data_trip$dropoff_longitude<(-73.75)&data_trip$dropoff_longitude>(-73.82)))
#uncomment this if you want to exclude pickups and dropoffs in NY JFK 
#data_trip<-data_trip[data_trip$jfk_pickup==0,]
#data_trip<-data_trip[data_trip$jfk_dropoff==0,]
#create a column for counting trips
data_trip$count<-1
#create key run_id for data_trip table and drop reduntant colums
data_trip$run_id<-paste(data_trip$hack_license,data_trip$pickup_datetime,sep="|")
data_trip$pickup_datetime<-NULL
data_trip$hack_license<-NULL
#create key run_id for data_fares table and drop reduntant colums
data_fares$run_id<-paste(data_fares$hack_license,data_fares$pickup_datetime,sep="|")
data_fares$pickup_datetime<-NULL
data_fares$hack_license<-NULL
#join data_trip and data_fares by run_id
data_all<-merge(x=data_trip,y=data_fares, by.x="run_id", by.y="run_id", all.x=TRUE)
#####################################################################################################
#####################################################################################################
# If you were a taxi owner, how would you maximize your 
# earnings in a day? Compute some analytics to get insights
#####################################################################################################
#####################################################################################################
#work on a selection of the NYC area if the following command is uncommented
data_all<-data_all[(data_all$pickup_latitude>(40.62)& data_all$pickup_latitude<40.9 &
                      data_all$pickup_longitude>(-74.1) & data_all$pickup_longitude<(-73.75) &
                      data_all$dropoff_latitude>(40.62)& data_all$dropoff_latitude<40.9 &
                      data_all$dropoff_longitude>(-74.1) & data_all$dropoff_longitude<(-73.75)),]
#remove HH-mm-ss from run_id key
data_all$run_id<-mapply(FUN = function(x) substring(x, 1, 43), data_all$run_id)
head(data_all)
summary(data_all)
#exclude trip having both pickup & dropoff inside jfk airport if next line is not commented
data_all_exjfk<-data_all[data_all$jfk==0,]
#Build a data frame "df1" where each row contains the sum p/day p/hack_license of
# trip_time_in_secs 
# trip_distance 
# count 
# fare_amount 
# surcharge 
# mta_tax 
# tip_amount 
# tolls_amount 
# total_amount
df1<-aggregate(data_all_exjfk[,c("trip_time_in_secs",
                                 "trip_distance",
                                 "count",
                                 "fare_amount",
                                 "tip_amount",
                                 "total_amount")],
               list(data_all_exjfk$run_id), FUN=sum)
#Build a data frame "df2" where each row contains the median pickup and dropoff
#coordinate p/day p/hack_license
# pickup_latitude 
# pickup_longitude 
# dropoff_latitude
# dropoff_longitude
df2<-aggregate(data_all_exjfk[,c("pickup_latitude",
                                 "pickup_longitude",
                                 "dropoff_latitude",
                                 "dropoff_longitude")],
               list(data_all_exjfk$run_id), FUN=median)
data_trip<-NULL
data_fares<-NULL
data_all_exjfk<-NULL
#merge df1 and df2
mydf<-merge(x=df1,y=df2, by.x="Group.1", by.y="Group.1", all.x=TRUE)
mydf<-mydf[complete.cases(mydf),]#[sample(nrow(mydf), 100000), ]
names(mydf)[1]<-"driver_day_id"
#create a column for fare+tip amount
mydf$fare_tip_amount<-mydf$fare_amount+mydf$tip_amount
#create a column for speed p/day p/driver
mydf$speed<-mydf$trip_distance/mydf$trip_time_in_secs
mydf$distance_ptrip<-mydf$trip_distance/mydf$count
mydf$time_ptrip<-mydf$trip_time_in_secs/mydf$count
# mydf$log_fare_tip_amount<-log(mydf$fare_tip_amount)
# mydf$log_trip_time_in_secs<-log(mydf$trip_time_in_secs)
# mydf$log_trip_distance<-log(mydf$trip_distance)
# mydf<-mydf[complete.cases(mydf),]
#view correlations among some relevant variables
pairs.panels(mydf[,c("fare_tip_amount",
                     "trip_time_in_secs",
                     "trip_distance",
                     "count",
                     "speed",
                     "distance_ptrip",
                     "time_ptrip")
                  ])
#fit tree models
#build train and test set
train<-sample(1:nrow(mydf),nrow(mydf)*0.7)
mydf_train<-mydf[train,]
#install.packages("tree")
library(tree)
treefit1 = tree(fare_tip_amount ~ pickup_latitude+pickup_longitude,mydf_train)
treefit2 = tree(fare_tip_amount ~ dropoff_latitude+dropoff_longitude,mydf_train)
treefit3 = tree(fare_tip_amount ~ pickup_latitude+pickup_longitude+dropoff_latitude+
                  dropoff_longitude+trip_time_in_secs+trip_distance,
                mydf_train)
treefit4 = tree(fare_tip_amount ~ trip_time_in_secs+trip_distance,
                mydf_train)
attach(mtcars)
par(mfrow=c(2,2))
plot(treefit1)
text(treefit1,cex=0.75)
fare.deciles = quantile((mydf_train$fare_tip_amount),0:10/10, na.rm = TRUE)
cut.fare = cut((mydf_train$fare_tip_amount),fare.deciles,include.lowest=TRUE)
plot(mydf_train$pickup_longitude,mydf_train$pickup_latitude,col=grey(10:2/11)[cut.fare],pch=20,xlab="Longitude",ylab="Latitude",ylim=c(40.64,40.825),xlim=c(-74.02,-73.78))
partition.tree(treefit1,ordvars=c("pickup_longitude","pickup_latitude"),add=TRUE)
plot(treefit2)
text(treefit2,cex=0.9)
fare.deciles = quantile((mydf_train$fare_tip_amount),0:10/10, na.rm = TRUE)
cut.fare = cut((mydf_train$fare_tip_amount),fare.deciles,include.lowest=TRUE)
plot(mydf_train$dropoff_longitude,mydf_train$dropoff_latitude,col = grey(10:2/11)[cut.fare],pch=20,xlab="Longitude",ylab="Latitude",ylim=c(40.64,40.825),xlim=c(-74.02,-73.78))
partition.tree(treefit2,ordvars=c("dropoff_longitude","dropoff_latitude"),add=TRUE)
attach(mtcars)
par(mfrow=c(2,2))
plot(treefit1)
text(treefit1,cex=0.9)
plot(treefit2)
text(treefit2,cex=0.9)
plot(treefit3)
text(treefit3,cex=0.9)
plot(treefit4)
text(treefit4,cex=0.9)
yhat<-predict(treefit1,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
sqrt(mean((yhat-test)^2))
yhat<-predict(treefit2,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
sqrt(mean((yhat-test)^2))
yhat<-predict(treefit3,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
sqrt(mean((yhat-test)^2))
yhat<-predict(treefit4,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
sqrt(mean((yhat-test)^2))
library(randomForest)
rf_fit <- randomForest(fare_amount ~ pickup_latitude+pickup_longitude+dropoff_latitude+dropoff_longitude+trip_time_in_secs+trip_distance,mydf_train,ntree=100)
print(rf_fit) # view results 
par(mfrow=c(1,1))
importance(rf_fit)
varImpPlot(rf_fit)
yhat<-predict(rf_fit,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
sqrt(mean((yhat-test)^2))
#run a regression
model1 = lm(fare_tip_amount ~ trip_time_in_secs+trip_distance,mydf_train)
summary(model1)
library(car)
vif(model1) #variance inflation factor: if  >10 multicollinearity is high
#check outliers
outlierTest(model1)
mydf_train2<-mydf_train[-c(463310,116341,355644,116359,355636,355643,296075,355631,116345,116351),]
model2 = lm(fare_tip_amount ~ trip_time_in_secs+trip_distance,mydf_train2)
summary(model2)
vif(model2) #ok
outlierTest(model2)
yhat<-predict(model2,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
plot(yhat,test)
abline(0,1)
sqrt(mean((yhat-test)^2))
#check residuals
par(mfrow=c(2,2))
hist(model2$residuals,50)
qqnorm(model2$residuals)
qqline(model2$residuals)
#the following plot is interesting it seems that trip having short distance
#need to be investigated
plot( mydf_train2$trip_distance,model2$residuals) 
plot( mydf_train2$trip_time_in_secs,model2$residuals)
install.packages('psych')
install.packages('dplyr')
install.packages('ggmap')
install.packages('tree')
install.packages('randomForest')
install.packages('car')
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/MDS-I/03 Business Dashboard in R/")
library(psych)
library(dplyr)
library(ggmap)
#read fare_data_4
data_fares<-read.csv("Data/trip_fare_4.csv",sep=',', header=1, nrows = 10000)
head(data_fares)
data_fares$medallion<-NULL
data_fares$vendor_id<-NULL
data_fares$payment_type<-NULL
#read trip_data_4
data_trip<-read.csv("Data/trip_data_4.csv",sep=',', header=1, nrows = 20000)
summary(data_trip)
#remove columns I'm not going to use in the following
data_trip$medallion<-NULL
data_trip$vendor_id<-NULL
data_trip$store_and_fwd_flag<-NULL
data_trip$rate_code<-NULL
data_trip$dropoff_datetime<-NULL
#create a column for pickup_hour
data_trip$pickup_hour<-as.POSIXlt(data_trip$pickup_datetime)$hour
#create a column for week 
data_trip$week<-as.numeric(format(as.Date(data_trip$pickup_datetime),"%U"))
#create a column for day
data_trip$day<-mapply(FUN = function(x) substring(x, 1, 10), data_trip$pickup_datetime) #substr(x, start, stop)
#exclude trip with time less than 60 seconds
data_trip<-data_trip[(data_trip$trip_time_in_secs)>60,]
#exclude trip with distance less than 0.1 miles
data_trip<-data_trip[(data_trip$trip_distance)>0.1,]
#dummy for detecting trip starting and ending at JFK airport
data_trip$jfk<-as.numeric((data_trip$pickup_latitude<40.665&data_trip$pickup_latitude>40.635)&
                            (data_trip$pickup_longitude<(-73.75)&data_trip$pickup_longitude>(-73.82))&
                            (data_trip$dropoff_latitude<40.665&data_trip$dropoff_latitude>40.635)&
                            (data_trip$dropoff_longitude<(-73.75)&data_trip$dropoff_longitude>(-73.82)))
#dummy for detecting trip starting at JFK airport
data_trip$jfk_pickup<-as.numeric((data_trip$pickup_latitude<40.665&data_trip$pickup_latitude>40.635)&
                                   (data_trip$pickup_longitude<(-73.75)&data_trip$pickup_longitude>(-73.82)))
#dummy for detecting trip ending at JFK airport
data_trip$jfk_dropoff<-as.numeric((data_trip$dropoff_latitude<40.665&data_trip$dropoff_latitude>40.635)&
                                    (data_trip$dropoff_longitude<(-73.75)&data_trip$dropoff_longitude>(-73.82)))
#uncomment this if you want to exclude pickups and dropoffs in NY JFK 
#data_trip<-data_trip[data_trip$jfk_pickup==0,]
#data_trip<-data_trip[data_trip$jfk_dropoff==0,]
#create a column for counting trips
data_trip$count<-1
#create key run_id for data_trip table and drop reduntant colums
data_trip$run_id<-paste(data_trip$hack_license,data_trip$pickup_datetime,sep="|")
data_trip$pickup_datetime<-NULL
data_trip$hack_license<-NULL
#create key run_id for data_fares table and drop reduntant colums
data_fares$run_id<-paste(data_fares$hack_license,data_fares$pickup_datetime,sep="|")
data_fares$pickup_datetime<-NULL
data_fares$hack_license<-NULL
#join data_trip and data_fares by run_id
data_all<-merge(x=data_trip,y=data_fares, by.x="run_id", by.y="run_id", all.x=TRUE)
#####################################################################################################
#####################################################################################################
# If you were a taxi owner, how would you maximize your 
# earnings in a day? Compute some analytics to get insights
#####################################################################################################
#####################################################################################################
#work on a selection of the NYC area if the following command is uncommented
data_all<-data_all[(data_all$pickup_latitude>(40.62)& data_all$pickup_latitude<40.9 &
                      data_all$pickup_longitude>(-74.1) & data_all$pickup_longitude<(-73.75) &
                      data_all$dropoff_latitude>(40.62)& data_all$dropoff_latitude<40.9 &
                      data_all$dropoff_longitude>(-74.1) & data_all$dropoff_longitude<(-73.75)),]
#remove HH-mm-ss from run_id key
data_all$run_id<-mapply(FUN = function(x) substring(x, 1, 43), data_all$run_id)
head(data_all)
summary(data_all)
#exclude trip having both pickup & dropoff inside jfk airport if next line is not commented
data_all_exjfk<-data_all[data_all$jfk==0,]
#Build a data frame "df1" where each row contains the sum p/day p/hack_license of
# trip_time_in_secs 
# trip_distance 
# count 
# fare_amount 
# surcharge 
# mta_tax 
# tip_amount 
# tolls_amount 
# total_amount
df1<-aggregate(data_all_exjfk[,c("trip_time_in_secs",
                                 "trip_distance",
                                 "count",
                                 "fare_amount",
                                 "tip_amount",
                                 "total_amount")],
               list(data_all_exjfk$run_id), FUN=sum)
#Build a data frame "df2" where each row contains the median pickup and dropoff
#coordinate p/day p/hack_license
# pickup_latitude 
# pickup_longitude 
# dropoff_latitude
# dropoff_longitude
df2<-aggregate(data_all_exjfk[,c("pickup_latitude",
                                 "pickup_longitude",
                                 "dropoff_latitude",
                                 "dropoff_longitude")],
               list(data_all_exjfk$run_id), FUN=median)
data_trip<-NULL
data_fares<-NULL
data_all_exjfk<-NULL
#merge df1 and df2
mydf<-merge(x=df1,y=df2, by.x="Group.1", by.y="Group.1", all.x=TRUE)
mydf<-mydf[complete.cases(mydf),]#[sample(nrow(mydf), 100000), ]
names(mydf)[1]<-"driver_day_id"
#create a column for fare+tip amount
mydf$fare_tip_amount<-mydf$fare_amount+mydf$tip_amount
#create a column for speed p/day p/driver
mydf$speed<-mydf$trip_distance/mydf$trip_time_in_secs
mydf$distance_ptrip<-mydf$trip_distance/mydf$count
mydf$time_ptrip<-mydf$trip_time_in_secs/mydf$count
# mydf$log_fare_tip_amount<-log(mydf$fare_tip_amount)
# mydf$log_trip_time_in_secs<-log(mydf$trip_time_in_secs)
# mydf$log_trip_distance<-log(mydf$trip_distance)
# mydf<-mydf[complete.cases(mydf),]
#view correlations among some relevant variables
pairs.panels(mydf[,c("fare_tip_amount",
                     "trip_time_in_secs",
                     "trip_distance",
                     "count",
                     "speed",
                     "distance_ptrip",
                     "time_ptrip")
                  ])
#fit tree models
#build train and test set
train<-sample(1:nrow(mydf),nrow(mydf)*0.7)
mydf_train<-mydf[train,]
#install.packages("tree")
library(tree)
treefit1 = tree(fare_tip_amount ~ pickup_latitude+pickup_longitude,mydf_train)
treefit2 = tree(fare_tip_amount ~ dropoff_latitude+dropoff_longitude,mydf_train)
treefit3 = tree(fare_tip_amount ~ pickup_latitude+pickup_longitude+dropoff_latitude+
                  dropoff_longitude+trip_time_in_secs+trip_distance,
                mydf_train)
treefit4 = tree(fare_tip_amount ~ trip_time_in_secs+trip_distance,
                mydf_train)
attach(mtcars)
par(mfrow=c(2,2))
plot(treefit1)
text(treefit1,cex=0.75)
fare.deciles = quantile((mydf_train$fare_tip_amount),0:10/10, na.rm = TRUE)
cut.fare = cut((mydf_train$fare_tip_amount),fare.deciles,include.lowest=TRUE)
plot(mydf_train$pickup_longitude,mydf_train$pickup_latitude,col=grey(10:2/11)[cut.fare],pch=20,xlab="Longitude",ylab="Latitude",ylim=c(40.64,40.825),xlim=c(-74.02,-73.78))
partition.tree(treefit1,ordvars=c("pickup_longitude","pickup_latitude"),add=TRUE)
plot(treefit2)
text(treefit2,cex=0.9)
fare.deciles = quantile((mydf_train$fare_tip_amount),0:10/10, na.rm = TRUE)
cut.fare = cut((mydf_train$fare_tip_amount),fare.deciles,include.lowest=TRUE)
plot(mydf_train$dropoff_longitude,mydf_train$dropoff_latitude,col = grey(10:2/11)[cut.fare],pch=20,xlab="Longitude",ylab="Latitude",ylim=c(40.64,40.825),xlim=c(-74.02,-73.78))
partition.tree(treefit2,ordvars=c("dropoff_longitude","dropoff_latitude"),add=TRUE)
attach(mtcars)
par(mfrow=c(2,2))
plot(treefit1)
text(treefit1,cex=0.9)
plot(treefit2)
text(treefit2,cex=0.9)
plot(treefit3)
text(treefit3,cex=0.9)
plot(treefit4)
text(treefit4,cex=0.9)
yhat<-predict(treefit1,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
sqrt(mean((yhat-test)^2))
yhat<-predict(treefit2,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
sqrt(mean((yhat-test)^2))
yhat<-predict(treefit3,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
sqrt(mean((yhat-test)^2))
yhat<-predict(treefit4,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
sqrt(mean((yhat-test)^2))
library(randomForest)
rf_fit <- randomForest(fare_amount ~ pickup_latitude+pickup_longitude+dropoff_latitude+dropoff_longitude+trip_time_in_secs+trip_distance,mydf_train,ntree=100)
print(rf_fit) # view results 
par(mfrow=c(1,1))
importance(rf_fit)
varImpPlot(rf_fit)
yhat<-predict(rf_fit,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
sqrt(mean((yhat-test)^2))
#run a regression
model1 = lm(fare_tip_amount ~ trip_time_in_secs+trip_distance,mydf_train)
summary(model1)
library(car)
vif(model1) #variance inflation factor: if  >10 multicollinearity is high
#check outliers
outlierTest(model1)
mydf_train2<-mydf_train[-c(463310,116341,355644,116359,355636,355643,296075,355631,116345,116351),]
model2 = lm(fare_tip_amount ~ trip_time_in_secs+trip_distance,mydf_train2)
summary(model2)
vif(model2) #ok
outlierTest(model2)
yhat<-predict(model2,mydf[-train,])
test<-mydf[-train,"fare_tip_amount"]
plot(yhat,test)
abline(0,1)
sqrt(mean((yhat-test)^2))
#check residuals
par(mfrow=c(2,2))
hist(model2$residuals,50)
qqnorm(model2$residuals)
qqline(model2$residuals)
#the following plot is interesting it seems that trip having short distance
#need to be investigated
plot( mydf_train2$trip_distance,model2$residuals) 
plot( mydf_train2$trip_time_in_secs,model2$residuals)
library(ISLR) # A1 names(Weekly) View(Weekly) dim(Weekly) str(Weekly) summary(Weekly) head(Weekly) pairs(Weekly) cor.test(Weekly$Year, Weekly$Volume) # Si osserva la relazione fra Volume e Year # A2 mod <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = "binomial") summary(mod) # A3 up = ifelse(Weekly$Direction == "Up", c(1), c(0)) table(up, Weekly$Direction) odds = exp(coef(mod)[1] +     coef(mod)[2] * mean(Weekly$Lag1) +     coef(mod)[3] * mean(Weekly$Lag2) +     coef(mod)[4] * mean(Weekly$Lag3) +     coef(mod)[5] * mean(Weekly$Lag4) +     coef(mod)[6] * mean(Weekly$Lag5) +     coef(mod)[7] * (mean(Weekly$Volume) + 1) * mean(up)  ) prob = (odds / (1 + odds)) * 100 prob # A4 pred1 = predict(mod, type = "response") pred2 <- ifelse(pred1 > .5, "Up", "Down") table(Weekly$Direction, pred2) # There are a predominance of Up prediction. The model predicts well the Up direction, but it predict poorly the Down direction. # A5 trainset = (Weekly$Year <= 2008) testset = Weekly[!trainset,] glm.fit.d <- glm(Direction ~ Lag2, data = Weekly, subset = trainset, family = "binomial") glm.probs.d <- predict(glm.fit.d, type = "response", newdata = testset) glm.preds.d <- ifelse(glm.probs.d > .5, "Up", "Down") cm.d <- table(testset$Direction, glm.preds.d) cm.d ############ # Secondo esercizio ############ #B1 remove(list = ls()) data(Auto) Auto$mpg01 <- with(ifelse(mpg > median(mpg), "1", "0"), data = Auto) #B2 require(ggplot2) attach(Auto) # Boxplots par(mfrow = c(2, 3)) for (i in names(Auto)) {     # excluding the own mpgs variables and others categorical variables     if (grepl(i, pattern = "^mpg|cylinders|origin|name")) { next }     boxplot(eval(parse(text = i)) ~ mpg01, ylab = i, col = c("red", "blue")) } pairs(Auto[,0:8]) #B3 set.seed(654) rows <- sample(x = nrow(Auto), size = .80 * nrow(Auto)) trainset <- Auto[rows,] testset <- Auto[-rows,] #B4 lr.fit <- glm(as.factor(mpg01) ~ displacement + horsepower + weight + acceleration + year + cylinders + origin, data = trainset, family = "binomial") lr.probs <- predict(lr.fit, testset, type = "response") lr.pred <- ifelse(lr.probs > 0.5, "1", "0") table(testset$mpg01, lr.pred)
prob
up = ifelse(Weekly$Direction == "Up", c(1), c(0)) table(up, Weekly$Direction) odds = exp(coef(mod)[1] +     coef(mod)[2] * mean(Weekly$Lag1) +     coef(mod)[3] * mean(Weekly$Lag2) +     coef(mod)[4] * mean(Weekly$Lag3) +     coef(mod)[5] * mean(Weekly$Lag4) +     coef(mod)[6] * mean(Weekly$Lag5) +     coef(mod)[7] * (mean(Weekly$Volume) + 1) * mean(up)  ) prob = (odds / (1 + odds)) * 100 prob
mod <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = "binomial") summary(mod)
up = ifelse(Weekly$Direction == "Up", c(1), c(0)) table(up, Weekly$Direction) odds = exp(coef(mod)[1] +     coef(mod)[2] * mean(Weekly$Lag1) +     coef(mod)[3] * mean(Weekly$Lag2) +     coef(mod)[4] * mean(Weekly$Lag3) +     coef(mod)[5] * mean(Weekly$Lag4) +     coef(mod)[6] * mean(Weekly$Lag5) +     coef(mod)[7] * (mean(Weekly$Volume) + 1) * mean(up)  ) prob = (odds / (1 + odds)) * 100 prob
odds
library(ISLR) # A1 names(Weekly) View(Weekly) dim(Weekly) str(Weekly) summary(Weekly) head(Weekly) pairs(Weekly) cor.test(Weekly$Year, Weekly$Volume) # Si osserva la relazione fra Volume e Year # A2 mod <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = "binomial") summary(mod)
install.packages('ISLR')
library(ISLR) # A1 names(Weekly) View(Weekly) dim(Weekly) str(Weekly) summary(Weekly) head(Weekly) pairs(Weekly) cor.test(Weekly$Year, Weekly$Volume) # Si osserva la relazione fra Volume e Year # A2 mod <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = "binomial") summary(mod)
a <- coef(m1) odds <- exp(a[1] + a[2] * mean(Weekly$Lag1) + a[3] * mean(Weekly$Lag2) + a[4] * mean(Weekly$Lag3) + a[5] * mean(Weekly$Lag4) + a[6] * mean(Weekly$Lag5) + a[7] * mean(Weekly$Volume)) prob <- odds / (1 + odds) newodds <- exp(a[1] + a[2] * mean(Weekly$Lag1) + a[3] * mean(Weekly$Lag2) + a[4] * mean(Weekly$Lag3) + a[5] * mean(Weekly$Lag4) + a[6] * mean(Weekly$Lag5) + a[7] * ((mean(Weekly$Volume) + 1))) newprob <- newodds / (1 + newodds) paste(format((newodds - odds), digits = 4), c("Variazione negativa del odds dovuto ad un aumento di un bilione della variabile Volume")) paste(format(prob * 100, digits = 4), c(" % Probabilità 1")) paste(format((newprob * 100), digits = 4), c(" % Probabilità 2")) paste(format((newprob - prob) * 100, digits = 4), c(" % Diminuzione della probabilità per effetto di un aumento di un bilione della variabile Volume"))
m1 <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial(link = "logit"))
summary(m1)
`` `
#### il coefficiente di Lag2 è statisticamente significativo (non tantissimo però...). Nel modello di Regessione logistica non si possono fare commenti di chissà quale natura (come nel modello lineare). L'unica cosa che si può dire è sul segno del coefficente. In questo caso per Lag2 il segno del coefficente è positivo e ciò significa che se Lag2 aumenta allora si avrà un effetto positivo (aumenta) sulla probabilità dell'evento Direction-Up.
## A3) Coefficiente Volume, ODDS, Variazione Probabilità
` ``{ r }
a <- coef(m1)
odds <- exp(a[1] + a[2] * mean(Weekly$Lag1) + a[3] * mean(Weekly$Lag2) + a[4] * mean(Weekly$Lag3) + a[5] * mean(Weekly$Lag4) + a[6] * mean(Weekly$Lag5) + a[7] * mean(Weekly$Volume))
prob <- odds / (1 + odds)
newodds <- exp(a[1] + a[2] * mean(Weekly$Lag1) + a[3] * mean(Weekly$Lag2) + a[4] * mean(Weekly$Lag3) + a[5] * mean(Weekly$Lag4) + a[6] * mean(Weekly$Lag5) + a[7] * ((mean(Weekly$Volume) + 1)))
newprob <- newodds / (1 + newodds)
paste(format((newodds - odds), digits = 4), c("Variazione negativa del odds dovuto ad un aumento di un bilione della variabile Volume"))
paste(format(prob * 100, digits = 4), c(" % Probabilità 1"))
paste(format((newprob * 100), digits = 4), c(" % Probabilità 2"))
paste(format((newprob - prob) * 100, digits = 4), c(" % Diminuzione della probabilità per effetto di un aumento di un bilione della variabile Volume"))
# A4
m1 <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial(link = "logit")) summary(m1) a <- coef(m1) odds <- exp(a[1] + a[2] * mean(Weekly$Lag1) + a[3] * mean(Weekly$Lag2) + a[4] * mean(Weekly$Lag3) + a[5] * mean(Weekly$Lag4) + a[6] * mean(Weekly$Lag5) + a[7] * mean(Weekly$Volume)) prob <- odds / (1 + odds) newodds <- exp(a[1] + a[2] * mean(Weekly$Lag1) + a[3] * mean(Weekly$Lag2) + a[4] * mean(Weekly$Lag3) + a[5] * mean(Weekly$Lag4) + a[6] * mean(Weekly$Lag5) + a[7] * ((mean(Weekly$Volume) + 1))) newprob <- newodds / (1 + newodds) paste(format((newodds - odds), digits = 4), c("Variazione negativa del odds dovuto ad un aumento di un bilione della variabile Volume")) paste(format(prob * 100, digits = 4), c(" % Probabilità 1")) paste(format((newprob * 100), digits = 4), c(" % Probabilità 2")) paste(format((newprob - prob) * 100, digits = 4), c(" % Diminuzione della probabilità per effetto di un aumento di un bilione della variabile Volume"))
m1 <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial(link = "logit")) summary(m1) a <- coef(m1) odds <- exp(a[1] + a[2] * mean(Weekly$Lag1) + a[3] * mean(Weekly$Lag2) + a[4] * mean(Weekly$Lag3) + a[5] * mean(Weekly$Lag4) + a[6] * mean(Weekly$Lag5) + a[7] * mean(Weekly$Volume)) prob <- odds / (1 + odds) newodds <- exp(a[1] + a[2] * mean(Weekly$Lag1) + a[3] * mean(Weekly$Lag2) + a[4] * mean(Weekly$Lag3) + a[5] * mean(Weekly$Lag4) + a[6] * mean(Weekly$Lag5) + a[7] * ((mean(Weekly$Volume) + 1))) newprob <- newodds / (1 + newodds) paste(format((newodds - odds), digits = 4), c("Variazione negativa del odds dovuto ad un aumento di un bilione della variabile Volume")) paste(format(prob * 100, digits = 4), c(" % Probabilità 1")) paste(format((newprob * 100), digits = 4), c(" % Probabilità 2")) paste(format((newprob - prob) * 100, digits = 4), c(" % Diminuzione della probabilità per effetto di un aumento di un bilione della variabile Volume"))
up = ifelse(Weekly$Direction == "Up", c(1), c(0))
table(up, Weekly$Direction)
up = ifelse(Weekly$Direction == "Up", c(1), c(0)) table(up, Weekly$Direction) oddsa = exp(coef(mod)[1] +     coef(mod)[2] * mean(Weekly$Lag1) +     coef(mod)[3] * mean(Weekly$Lag2) +     coef(mod)[4] * mean(Weekly$Lag3) +     coef(mod)[5] * mean(Weekly$Lag4) +     coef(mod)[6] * mean(Weekly$Lag5) +     coef(mod)[7] * mean(Weekly$Volume) * mean(up) ) proba = (oddsa / (1 + oddsa)) * 100 oddsb = exp(coef(mod)[1] +     coef(mod)[2] * mean(Weekly$Lag1) +     coef(mod)[3] * mean(Weekly$Lag2) +     coef(mod)[4] * mean(Weekly$Lag3) +     coef(mod)[5] * mean(Weekly$Lag4) +     coef(mod)[6] * mean(Weekly$Lag5) +     coef(mod)[7] * (mean(Weekly$Volume) + 1) * mean(up) ) probb = (oddsb / (1 + oddsb)) * 100 proba - probb
proba- probb
proba - prob
proba - probb
probb - proba
pred1 = predict(mod, type = "response") pred2 <- ifelse(pred1 > .5, "Up", "Down") table(Weekly$Direction, pred2)
trainset = (Weekly$Year <= 2008) testset = Weekly[!trainset,] glm.fit.d <- glm(Direction ~ Lag2, data = Weekly, subset = trainset, family = "binomial") glm.probs.d <- predict(glm.fit.d, type = "response", newdata = testset) glm.preds.d <- ifelse(glm.probs.d > .5, "Up", "Down") cm.d <- table(testset$Direction, glm.preds.d) cm.d
39/65
(430+48)/(54+557)
library(ISLR) # A1 names(Weekly) View(Weekly) dim(Weekly) str(Weekly) summary(Weekly) head(Weekly) pairs(Weekly) cor.test(Weekly$Year, Weekly$Volume) # Si osserva la relazione fra Volume e Year # A2 mod <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = "binomial") summary(mod) # A3 up = ifelse(Weekly$Direction == "Up", c(1), c(0)) table(up, Weekly$Direction) oddsa = exp(coef(mod)[1] +     coef(mod)[2] * mean(Weekly$Lag1) +     coef(mod)[3] * mean(Weekly$Lag2) +     coef(mod)[4] * mean(Weekly$Lag3) +     coef(mod)[5] * mean(Weekly$Lag4) +     coef(mod)[6] * mean(Weekly$Lag5) +     coef(mod)[7] * mean(Weekly$Volume) * mean(up) ) proba = (oddsa / (1 + oddsa)) * 100 oddsb = exp(coef(mod)[1] +     coef(mod)[2] * mean(Weekly$Lag1) +     coef(mod)[3] * mean(Weekly$Lag2) +     coef(mod)[4] * mean(Weekly$Lag3) +     coef(mod)[5] * mean(Weekly$Lag4) +     coef(mod)[6] * mean(Weekly$Lag5) +     coef(mod)[7] * (mean(Weekly$Volume) + 1) * mean(up) ) probb = (oddsb / (1 + oddsb)) * 100 proba - probb # A4 pred1 = predict(mod, type = "response") pred2 <- ifelse(pred1 > .5, "Up", "Down") table(Weekly$Direction, pred2) # There are a predominance of Up prediction. The model predicts well the Up direction, but it predict poorly the Down direction. # A5 trainset = (Weekly$Year <= 2008) testset = Weekly[!trainset,] glm.fit.d <- glm(Direction ~ Lag2, data = Weekly, subset = trainset, family = "binomial") glm.probs.d <- predict(glm.fit.d, type = "response", newdata = testset) glm.preds.d <- ifelse(glm.probs.d > .5, "Up", "Down") cm.d <- table(testset$Direction, glm.preds.d) cm.d
glm.probs
glm.probs.b
glm.probs.d
summary(glm.probs.d)
summary(glm.preds.d)
summary(glm.fit.d)
#B1 remove(list = ls()) data(Auto) Auto$mpg01 <- with(ifelse(mpg > median(mpg), "1", "0"), data = Auto) #B2 require(ggplot2) attach(Auto) # Boxplots par(mfrow = c(2, 3)) for (i in names(Auto)) {     # excluding the own mpgs variables and others categorical variables     if (grepl(i, pattern = "^mpg|cylinders|origin|name")) { next }     boxplot(eval(parse(text = i)) ~ mpg01, ylab = i, col = c("red", "blue")) } pairs(Auto[,0:8])
#B1 remove(list = ls()) data(Auto) Auto$mpg01 <- with(ifelse(mpg > median(mpg), "1", "0"), data = Auto) #B2 require(ggplot2) attach(Auto) # Boxplots par(mfrow = c(2, 3)) for (i in names(Auto)) {     # excluding the own mpgs variables and others categorical variables     if (grepl(i, pattern = "^mpg|cylinders|origin|name")) { next }     boxplot(eval(parse(text = i)) ~ mpg01, ylab = i, col = c("red", "blue")) } pairs(Auto[,0:8]) #B3 set.seed(654) rows <- sample(x = nrow(Auto), size = .80 * nrow(Auto)) trainset <- Auto[rows,] testset <- Auto[-rows,] #B4 lr.fit <- glm(as.factor(mpg01) ~ displacement + horsepower + weight + acceleration + year + cylinders + origin, data = trainset, family = "binomial") lr.probs <- predict(lr.fit, testset, type = "response") lr.pred <- ifelse(lr.probs > 0.5, "1", "0") table(testset$mpg01, lr.pred)
#install.packages(c("bnlearn", "RBGL", "gRain", "igraph")) library(bnlearn) library(gRain) library(igraph) # 1 load("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esame Parte 2/insuranceDF.RData") nrow(insuranceDF) # 10000 osservazioni, 27 variabili tutte di tipo Factor names(insuranceDF) str(insuranceDF) # Non ci sono dati mancanti summary(insuranceDF) # 2.1  mod1 <- hc(insuranceDF, score = "bde") plot(mod1) # 2.2 library(dplyr) # La rete ha 54 archi orientati (arcs <- mod1$arcs) (directed.arcs(mod1)) for (node in nodes(mod1)) {     print(paste0("Nodo ", node))     if (length(which(mod1$arcs[, 1] == node)) > 0)         print(mod1$arcs[which(mod1$arcs[, 1] == node), 2])     else         print("senza figli")     } # Un unico nodo radice, nessun nodo isolato for (node2 in root.nodes(mod1)) {     print(paste0("Nodo radice ", node2))     if (length(which(mod1$arcs[, 1] == node2)) == 0)         print(paste0("Nodo isolato ", node2))     else         print("Nodo non isolato")     } # 3 bl1 <- cbind(names(insuranceDF[, -2]), rep("Age", length(names(insuranceDF[, -2])))) bl2 <- cbind(rep("PropCost", length(names(insuranceDF[, -20]))), names(insuranceDF[, -20])) bl3 <- cbind(rep("DrivHist", length(names(insuranceDF[, -27]))), names(insuranceDF[, -27])) bl <- rbind(bl1, bl2, bl3) mod.2 <- hc(insuranceDF, score = "bde", blacklist = bl) plot(mod.2) # La rete contiene 56 archi orientati directed.arcs(mod2) # 4.1 wl <- rbind(c("SeniorTrain", "ThisCarDam"), c("MedCost", "RuggedAuto"), c("SocioEcon", "GoodStudent"),           c("Age", "RiskAversion"), c("Age", "ThisCarDam"), c("RuggedAuto", "VehicleYear")) mod.3 <- hc(insuranceDF, score = "bic", whitelist = wl) plot(mod.3) # La rete contiene 45 archi orientati directed.arcs(mod.3) # 4.2 modello3 <- graph_from_data_frame(arcs(mod.3), directed = TRUE, vertices = NULL) plot(modello3) idbn <- tkplot(modello3) tk_set_coords(idbn, 2 * tk_coords(idbn)) coordinate <- tk_coords(idbn, norm = FALSE) tk_close(idbn) # 5.0 bnlearn::compare(mod.2, mod.3, arcs = FALSE) bnlearn::compare(mod.2, mod.3, arcs = TRUE) # Le 2 reti hanno 38 archi uguali # 6 fit = bn.fit(mod.3, insuranceDF) mod.3$arcs[which(mod.3$arcs[, 2] == "Theft")] #Conditional probability table for the Theft variable (theftProb <- fit$Theft) bn.fit.barchart(theftProb) bn.fit.dotplot(theftProb) theftProbab <- as.data.frame(fit$Theft$prob) str(theftProbab) # probabilità condizionata di Theft, condizionata all'avverarsi di ThisCarCost=="TenThou",ThisCarDam=="Moderate" freq <- filter(theftProbab, ThisCarCost == "TenThou", ThisCarDam == "Moderate") #bassisima probabilità che venga rubata una macchina del costo del ordine della decina di migliaia di dollari  #con danni di entità moderata freq # 7.1 library(pcalg) mod.2 mod.2.graphnel <- as.graphNEL(mod.2) # PropCost non è indipendente da RiskAversion condizionatamente a MakeModel,DrivQuality,Theft pcalg::dsep("PropCost", "RiskAversion", c("MakeModel", "DrivQuality", "Theft"), mod.2.graphnel) # 7.2 #source("https://bioconductor.org/biocLite.R") #biocLite("BiocGenerics") mod.2.fit <- bn.fit(mod.2, insuranceDF) ? simulate sim <- bnlearn::simulate(mod.2, nsim = 10000) ? simulate # 8 cptable(vpar, levels = NULL, values = NULL, normalize = TRUE, smooth = 0)
#install.packages(c("bnlearn", "RBGL", "gRain", "igraph")) library(bnlearn) library(gRain) library(igraph) # 1 load("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esame Parte 2/insuranceDF.RData") nrow(insuranceDF) # 10000 osservazioni, 27 variabili tutte di tipo Factor names(insuranceDF) str(insuranceDF) # Non ci sono dati mancanti summary(insuranceDF) # 2.1  mod1 <- hc(insuranceDF, score = "bde") plot(mod1) # 2.2 library(dplyr) # La rete ha 54 archi orientati (arcs <- mod1$arcs) (directed.arcs(mod1)) for (node in nodes(mod1)) {     print(paste0("Nodo ", node))     if (length(which(mod1$arcs[, 1] == node)) > 0)         print(mod1$arcs[which(mod1$arcs[, 1] == node), 2])     else         print("senza figli")     } # Un unico nodo radice, nessun nodo isolato for (node2 in root.nodes(mod1)) {     print(paste0("Nodo radice ", node2))     if (length(which(mod1$arcs[, 1] == node2)) == 0)         print(paste0("Nodo isolato ", node2))     else         print("Nodo non isolato")     } # 3 bl1 <- cbind(names(insuranceDF[, -2]), rep("Age", length(names(insuranceDF[, -2])))) bl2 <- cbind(rep("PropCost", length(names(insuranceDF[, -20]))), names(insuranceDF[, -20])) bl3 <- cbind(rep("DrivHist", length(names(insuranceDF[, -27]))), names(insuranceDF[, -27])) bl <- rbind(bl1, bl2, bl3) mod.2 <- hc(insuranceDF, score = "bde", blacklist = bl) plot(mod.2) # La rete contiene 56 archi orientati directed.arcs(mod2) # 4.1 wl <- rbind(c("SeniorTrain", "ThisCarDam"), c("MedCost", "RuggedAuto"), c("SocioEcon", "GoodStudent"),           c("Age", "RiskAversion"), c("Age", "ThisCarDam"), c("RuggedAuto", "VehicleYear")) mod.3 <- hc(insuranceDF, score = "bic", whitelist = wl) plot(mod.3) # La rete contiene 45 archi orientati directed.arcs(mod.3) # 4.2 modello3 <- graph_from_data_frame(arcs(mod.3), directed = TRUE, vertices = NULL) plot(modello3) idbn <- tkplot(modello3) tk_set_coords(idbn, 2 * tk_coords(idbn)) coordinate <- tk_coords(idbn, norm = FALSE) tk_close(idbn) # 5.0 bnlearn::compare(mod.2, mod.3, arcs = FALSE) bnlearn::compare(mod.2, mod.3, arcs = TRUE) # Le 2 reti hanno 38 archi uguali # 6 fit = bn.fit(mod.3, insuranceDF) mod.3$arcs[which(mod.3$arcs[, 2] == "Theft")] #Conditional probability table for the Theft variable (theftProb <- fit$Theft) bn.fit.barchart(theftProb) bn.fit.dotplot(theftProb) theftProbab <- as.data.frame(fit$Theft$prob) str(theftProbab) # probabilità condizionata di Theft, condizionata all'avverarsi di ThisCarCost=="TenThou",ThisCarDam=="Moderate" freq <- filter(theftProbab, ThisCarCost == "TenThou", ThisCarDam == "Moderate") #bassisima probabilità che venga rubata una macchina del costo del ordine della decina di migliaia di dollari  #con danni di entità moderata freq # 7.1 library(pcalg) mod.2 mod.2.graphnel <- as.graphNEL(mod.2) # PropCost non è indipendente da RiskAversion condizionatamente a MakeModel,DrivQuality,Theft pcalg::dsep("PropCost", "RiskAversion", c("MakeModel", "DrivQuality", "Theft"), mod.2.graphnel) # 7.2 #source("https://bioconductor.org/biocLite.R") #biocLite("BiocGenerics") mod.2.fit <- bn.fit(mod.2, insuranceDF) ? simulate sim <- bnlearn::simulate(mod.2, nsim = 10000) ? simulate # 8 cptable(vpar, levels = NULL, values = NULL, normalize = TRUE, smooth = 0)
install.packages('pcalg')
freq
library(pcalg)
library("pcalg", lib.loc="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
source("https://bioconductor.org/biocLite.R") biocLite("RBGL")
library(pcalg)
mod.2
mod.2.graphnel <- as.graphNEL(mod.2)
# PropCost non è indipendente da RiskAversion condizionatamente a MakeModel,DrivQuality,Theft
pcalg::dsep("PropCost", "RiskAversion", c("MakeModel", "DrivQuality", "Theft"), mod.2.graphnel)
####### INSTALLARE PRIMA DI ESEGUIRE #install.packages(c("bnlearn", "RBGL", "gRain", "igraph")) #source("https://bioconductor.org/biocLite.R") #biocLite("RBGL") #biocLite("graph") #biocLite("BiocGenerics") library(bnlearn) library(gRain) library(igraph) # 1 load("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esame Parte 2/insuranceDF.RData") nrow(insuranceDF) # 10000 osservazioni, 27 variabili tutte di tipo Factor names(insuranceDF) str(insuranceDF) # Non ci sono dati mancanti summary(insuranceDF) # 2.1  mod1 <- hc(insuranceDF, score = "bde") summary(mod1) plot(mod1)
# 2.2 library(dplyr) # La rete ha 54 archi orientati (arcs <- mod1$arcs) (directed.arcs(mod1)) for (node in nodes(mod1)) {     print(paste0("Nodo ", node))     if (length(which(mod1$arcs[, 1] == node)) > 0)         print(mod1$arcs[which(mod1$arcs[, 1] == node), 2])     else         print("senza figli")     } # Un unico nodo radice, nessun nodo isolato for (node2 in root.nodes(mod1)) {     print(paste0("Nodo radice ", node2))     if (length(which(mod1$arcs[, 1] == node2)) == 0)         print(paste0("Nodo isolato ", node2))     else         print("Nodo non isolato")     }
####### INSTALLARE PRIMA DI ESEGUIRE #install.packages(c("bnlearn", "RBGL", "gRain", "igraph")) #source("https://bioconductor.org/biocLite.R") #biocLite("RBGL") #biocLite("graph") #biocLite("BiocGenerics") library(bnlearn) library(gRain) library(igraph) # 1 load("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esame Parte 2/insuranceDF.RData") nrow(insuranceDF) # 10000 osservazioni, 27 variabili tutte di tipo Factor names(insuranceDF) str(insuranceDF) # Non ci sono dati mancanti summary(insuranceDF) # 2.1  mod1 <- hcbl1 <- cbind(names(insuranceDF[, -2]), rep("Age", length(names(insuranceDF[, -2])))) bl2 <- cbind(rep("PropCost", length(names(insuranceDF[, -20]))), names(insuranceDF[, -20])) bl3 <- cbind(rep("DrivHist", length(names(insuranceDF[, -27]))), names(insuranceDF[, -27])) bl <- rbind(bl1, bl2, bl3)(insuranceDF, score = "bde") summary(mod1) plot(mod1)
bl <- rbind(bl1, bl2, bl3)
bl1 <- cbind(names(insuranceDF[, -2]), rep("Age", length(names(insuranceDF[, -2]))))
bl2 <- cbind(rep("PropCost", length(names(insuranceDF[, -20]))), names(insuranceDF[, -20]))
bl3 <- cbind(rep("DrivHist", length(names(insuranceDF[, -27]))), names(insuranceDF[, -27]))
bl <- rbind(bl1, bl2, bl3)
mod.2 <- hc(insuranceDF, score = "bde", blacklist = bl)
plot(mod.2)
# La rete contiene 56 archi orientati
directed.arcs(mod2)
directed.arcs(mod.2)
graphviz.plot(mod.2)
install.packages('Rgraphviz')
library(Rgraphviz)
source("https://bioconductor.org/biocLite.R")
biocLite("RBGL")
biocLite("graph")
biocLite("Rgraphviz")
biocLite("BiocGenerics")
####### INSTALLARE PRIMA DI ESEGUIRE #install.packages(c("bnlearn", "RBGL", "gRain", "igraph")) source("https://bioconductor.org/biocLite.R") biocLite("RBGL") biocLite("graph") biocLite("Rgraphviz") biocLite("BiocGenerics") library(bnlearn) library(gRain) library(igraph) library(Rgraphviz) # 1 load("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esame Parte 2/insuranceDF.RData") nrow(insuranceDF) # 10000 osservazioni, 27 variabili tutte di tipo Factor names(insuranceDF) str(insuranceDF) # Non ci sono dati mancanti summary(insuranceDF) # 2.1  mod1 <- hc(insuranceDF, score = "bde") summary(mod1) plot(mod1) # 2.2 library(dplyr) # La rete ha 54 archi orientati (arcs <- mod1$arcs) (directed.arcs(mod1)) for (node in nodes(mod1)) {     print(paste0("Nodo ", node))     if (length(which(mod1$arcs[, 1] == node)) > 0)         print(mod1$arcs[which(mod1$arcs[, 1] == node), 2])     else         print("senza figli")     } # Un unico nodo radice, nessun nodo isolato for (node2 in root.nodes(mod1)) {     print(paste0("Nodo radice ", node2))     if (length(which(mod1$arcs[, 1] == node2)) == 0)         print(paste0("Nodo isolato ", node2))     else         print("Nodo non isolato")     } # 3 bl1 <- cbind(names(insuranceDF[, -2]), rep("Age", length(names(insuranceDF[, -2])))) bl2 <- cbind(rep("PropCost", length(names(insuranceDF[, -20]))), names(insuranceDF[, -20])) bl3 <- cbind(rep("DrivHist", length(names(insuranceDF[, -27]))), names(insuranceDF[, -27])) bl <- rbind(bl1, bl2, bl3) mod.2 <- hc(insuranceDF, score = "bde", blacklist = bl) graphviz.plot(mod.2) # La rete contiene 56 archi orientati directed.arcs(mod.2) # 4.1 wl <- rbind(c("SeniorTrain", "ThisCarDam"), c("MedCost", "RuggedAuto"), c("SocioEcon", "GoodStudent"),           c("Age", "RiskAversion"), c("Age", "ThisCarDam"), c("RuggedAuto", "VehicleYear")) mod.3 <- hc(insuranceDF, score = "bic", whitelist = wl) plot(mod.3) # La rete contiene 45 archi orientati directed.arcs(mod.3) # 4.2 modello3 <- graph_from_data_frame(arcs(mod.3), directed = TRUE, vertices = NULL) plot(modello3) idbn <- tkplot(modello3) tk_set_coords(idbn, 2 * tk_coords(idbn)) coordinate <- tk_coords(idbn, norm = FALSE) tk_close(idbn) # 5.0 bnlearn::compare(mod.2, mod.3, arcs = FALSE) bnlearn::compare(mod.2, mod.3, arcs = TRUE) # Le 2 reti hanno 38 archi uguali # 6 fit = bn.fit(mod.3, insuranceDF) mod.3$arcs[which(mod.3$arcs[, 2] == "Theft")] #Conditional probability table for the Theft variable (theftProb <- fit$Theft) bn.fit.barchart(theftProb) bn.fit.dotplot(theftProb) theftProbab <- as.data.frame(fit$Theft$prob) str(theftProbab) # probabilità condizionata di Theft, condizionata all'avverarsi di ThisCarCost=="TenThou",ThisCarDam=="Moderate" freq <- filter(theftProbab, ThisCarCost == "TenThou", ThisCarDam == "Moderate") #bassisima probabilità che venga rubata una macchina del costo del ordine della decina di migliaia di dollari  #con danni di entità moderata freq # 7.1 library(pcalg) mod.2 mod.2.graphnel <- as.graphNEL(mod.2) # PropCost non è indipendente da RiskAversion condizionatamente a MakeModel,DrivQuality,Theft pcalg::dsep("PropCost", "RiskAversion", c("MakeModel", "DrivQuality", "Theft"), mod.2.graphnel)
library(bnlearn) library(gRain) library(igraph) library(Rgraphviz) # 1 load("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esame Parte 2/insuranceDF.RData") nrow(insuranceDF) # 10000 osservazioni, 27 variabili tutte di tipo Factor names(insuranceDF) str(insuranceDF) # Non ci sono dati mancanti summary(insuranceDF) # 2.1  mod1 <- hc(insuranceDF, score = "bde") summary(mod1) plot(mod1) # 2.2 library(dplyr) # La rete ha 54 archi orientati (arcs <- mod1$arcs) (directed.arcs(mod1)) for (node in nodes(mod1)) {     print(paste0("Nodo ", node))     if (length(which(mod1$arcs[, 1] == node)) > 0)         print(mod1$arcs[which(mod1$arcs[, 1] == node), 2])     else         print("senza figli")     } # Un unico nodo radice, nessun nodo isolato for (node2 in root.nodes(mod1)) {     print(paste0("Nodo radice ", node2))     if (length(which(mod1$arcs[, 1] == node2)) == 0)         print(paste0("Nodo isolato ", node2))     else         print("Nodo non isolato")     } # 3 bl1 <- cbind(names(insuranceDF[, -2]), rep("Age", length(names(insuranceDF[, -2])))) bl2 <- cbind(rep("PropCost", length(names(insuranceDF[, -20]))), names(insuranceDF[, -20])) bl3 <- cbind(rep("DrivHist", length(names(insuranceDF[, -27]))), names(insuranceDF[, -27])) bl <- rbind(bl1, bl2, bl3) mod.2 <- hc(insuranceDF, score = "bde", blacklist = bl) graphviz.plot(mod.2) # La rete contiene 56 archi orientati directed.arcs(mod.2) # 4.1 wl <- rbind(c("SeniorTrain", "ThisCarDam"), c("MedCost", "RuggedAuto"), c("SocioEcon", "GoodStudent"),           c("Age", "RiskAversion"), c("Age", "ThisCarDam"), c("RuggedAuto", "VehicleYear")) mod.3 <- hc(insuranceDF, score = "bic", whitelist = wl) plot(mod.3) # La rete contiene 45 archi orientati directed.arcs(mod.3) # 4.2 modello3 <- graph_from_data_frame(arcs(mod.3), directed = TRUE, vertices = NULL) plot(modello3) idbn <- tkplot(modello3) tk_set_coords(idbn, 2 * tk_coords(idbn)) coordinate <- tk_coords(idbn, norm = FALSE) tk_close(idbn) # 5.0 bnlearn::compare(mod.2, mod.3, arcs = FALSE) bnlearn::compare(mod.2, mod.3, arcs = TRUE) # Le 2 reti hanno 38 archi uguali # 6 fit = bn.fit(mod.3, insuranceDF) mod.3$arcs[which(mod.3$arcs[, 2] == "Theft")] #Conditional probability table for the Theft variable (theftProb <- fit$Theft) bn.fit.barchart(theftProb) bn.fit.dotplot(theftProb) theftProbab <- as.data.frame(fit$Theft$prob) str(theftProbab) # probabilità condizionata di Theft, condizionata all'avverarsi di ThisCarCost=="TenThou",ThisCarDam=="Moderate" freq <- filter(theftProbab, ThisCarCost == "TenThou", ThisCarDam == "Moderate") #bassisima probabilità che venga rubata una macchina del costo del ordine della decina di migliaia di dollari  #con danni di entità moderata freq # 7.1 library(pcalg) mod.2 mod.2.graphnel <- as.graphNEL(mod.2) # PropCost non è indipendente da RiskAversion condizionatamente a MakeModel,DrivQuality,Theft pcalg::dsep("PropCost", "RiskAversion", c("MakeModel", "DrivQuality", "Theft"), mod.2.graphnel) # 7.2 mod.2.fit <- bn.fit(mod.2, insuranceDF) ? simulate sim <- bnlearn::simulate(mod.2, nsim = 10000) ? simulate
graphviz.plot(mod1)
graphviz.plot(modello3)
modello3 <- graph_from_data_frame(arcs(mod.3), directed = TRUE, vertices = NULL)
graphviz.plot(modello3)
plot(modello3)
# 7.2
mod.2.fit <- bn.fit(mod.2, insuranceDF)
? simulate
? simulate
?simulate
?simulate
sim <- bnlearn::simulate(mod.2, nsim = 10000)
library(bnlearn) library(gRain) library(igraph) library(Rgraphviz)
library(graph)
# 1 load("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esame Parte 2/insuranceDF.RData") nrow(insuranceDF) # 10000 osservazioni, 27 variabili tutte di tipo Factor names(insuranceDF) str(insuranceDF) # Non ci sono dati mancanti summary(insuranceDF) # 2.1  mod1 <- hc(insuranceDF, score = "bde") summary(mod1) plot(mod1)
library(dplyr) # La rete ha 54 archi orientati (arcs <- mod1$arcs) (directed.arcs(mod1)) for (node in nodes(mod1)) {     print(paste0("Nodo ", node))     if (length(which(mod1$arcs[, 1] == node)) > 0)         print(mod1$arcs[which(mod1$arcs[, 1] == node), 2])     else         print("senza figli")     } # Un unico nodo radice, nessun nodo isolato for (node2 in root.nodes(mod1)) {     print(paste0("Nodo radice ", node2))     if (length(which(mod1$arcs[, 1] == node2)) == 0)         print(paste0("Nodo isolato ", node2))     else         print("Nodo non isolato")     }
nrow(directed.arcs(mod1))
load("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esame Parte 2/insuranceDF.RData") nrow(insuranceDF) # 10000 osservazioni, 27 variabili tutte di tipo Factor names(insuranceDF) str(insuranceDF) # Non ci sono dati mancanti summary(insuranceDF) # 2.1  mod1 <- hc(insuranceDF, score = "bde") summary(mod1) plot(mod1)
# 2.2 library(dplyr) # La rete ha 54 archi orientati (arcs <- mod1$arcs) nrow(directed.arcs(mod1)) (directed.arcs(mod1)) for (node in nodes(mod1)) {     print(paste0("Nodo ", node))     if (length(which(mod1$arcs[, 1] == node)) > 0)         print(mod1$arcs[which(mod1$arcs[, 1] == node), 2])     else         print("senza figli")     } # Un unico nodo radice, nessun nodo isolato for (node2 in root.nodes(mod1)) {     print(paste0("Nodo radice ", node2))     if (length(which(mod1$arcs[, 1] == node2)) == 0)         print(paste0("Nodo isolato ", node2))     else         print("Nodo non isolato")     }
# 3 bl1 <- cbind(names(insuranceDF[, -2]), rep("Age", length(names(insuranceDF[, -2])))) bl2 <- cbind(rep("PropCost", length(names(insuranceDF[, -20]))), names(insuranceDF[, -20])) bl3 <- cbind(rep("DrivHist", length(names(insuranceDF[, -27]))), names(insuranceDF[, -27])) bl <- rbind(bl1, bl2, bl3) mod.2 <- hc(insuranceDF, score = "bde", blacklist = bl) plot(mod.2) # La rete contiene 56 archi orientati nrow(directed.arcs(mod.2)) directed.arcs(mod.2)
# 4.1 wl <- rbind(c("SeniorTrain", "ThisCarDam"), c("MedCost", "RuggedAuto"), c("SocioEcon", "GoodStudent"),           c("Age", "RiskAversion"), c("Age", "ThisCarDam"), c("RuggedAuto", "VehicleYear")) mod.3 <- hc(insuranceDF, score = "bic", whitelist = wl) plot(mod.3) # La rete contiene 45 archi orientati nrow(directed.arcs(mod.3)) directed.arcs(mod.3)
# 4.2 modello3 <- graph_from_data_frame(arcs(mod.3), directed = TRUE, vertices = NULL) plot(modello3) idbn <- tkplot(modello3) tk_set_coords(idbn, 2 * tk_coords(idbn)) coordinate <- tk_coords(idbn, norm = FALSE) tk_close(idbn)
# 5.0 bnlearn::compare(mod.2, mod.3, arcs = FALSE) bnlearn::compare(mod.2, mod.3, arcs = TRUE) # Le 2 reti hanno 38 archi uguali
# 6 fit = bn.fit(mod.3, insuranceDF) mod.3$arcs[which(mod.3$arcs[, 2] == "Theft")] #Conditional probability table for the Theft variable (theftProb <- fit$Theft) bn.fit.barchart(theftProb) bn.fit.dotplot(theftProb) theftProbab <- as.data.frame(fit$Theft$prob) str(theftProbab) # probabilità condizionata di Theft, condizionata all'avverarsi di ThisCarCost=="TenThou",ThisCarDam=="Moderate" freq <- filter(theftProbab, ThisCarCost == "TenThou", ThisCarDam == "Moderate") #bassisima probabilità che venga rubata una macchina del costo del ordine della decina di migliaia di dollari  #con danni di entità moderata freq
library(pcalg) mod.2.graphnel <- as.graphNEL(mod.2) # PropCost non è indipendente da RiskAversion condizionatamente a MakeModel,DrivQuality,Theft pcalg::dsep("PropCost", "RiskAversion", c("MakeModel", "DrivQuality", "Theft"), mod.2.graphnel)
source("https://bioconductor.org/biocLite.R")
biocLite("graph")
library(pcalg)
biocLite("RBGL")
library(pcalg)                          
require(pcalg)
mod.2.graphnel <- as.graphNEL(mod.2)
# PropCost non è indipendente da RiskAversion condizionatamente a MakeModel,DrivQuality,Theft
pcalg::dsep("PropCost", "RiskAversion", c("MakeModel", "DrivQuality", "Theft"), mod.2.graphnel)
mod.2.fit <- bn.fit(mod.2, insuranceDF)
mod2gr <- as.grain(mod.2.fit)
sim <- simulate(mod2gr, nsim = 10000)
str(sim)
modsim <- hc(sim, score = "bde")
d <- graphviz.plot(modsim);
graphviz.plot(mod1)
graphviz.plot(mod.2)
modsim <- hc(sim, score = "bde")
d <- graphviz.plot(modsim);
plot(d, main = "Simulation of Bayesian Network", attrs = list(node = list(fillcolor = "yellow", fontsize = 70),             edge = list(color = "black"),             graph = list(rankdir = "UD")))
for (var in names(sim)) {     print(sim %>%           group_by_(var) %>%           summarise(n = n()) %>%           mutate(freq = n / sum(n))) }
# 8
mod2grain <- as.grain(mod2fit)
MPCarValue <- querygrain(mod2grain, nodes = c("CarValue"))
MPCarValue[[1]]["FiftyThou"]
ev <- list(Age = "Adult", mod2grain <- as.grain(mod.2.fit) MPCarValue <- querygrain(mod2grain, nodes = c("CarValue"))
MPCarValue <- querygrain(mod2grain, nodes = c("CarValue"))
MPCarValue[[1]]["FiftyThou"]
mod.2.fit <- bn.fit(mod.2, insuranceDF)
mod2gr <- as.grain(mod.2.fit)
mod2grain <- as.grain(mod.2.fit)
MPCarValue <- querygrain(mod2grain, nodes = c("CarValue"))
library(gRain)
MPCarValue <- querygrain(mod2grain, nodes = c("CarValue"))
mod2grain <- as.grain(mod.2.fit)
MPCarValue <- querygrain(mod2grain, nodes = c("CarValue"))
MPCarValue[[1]]["FiftyThou"]
ev <- list(Age = "Adult",                  RiskAversion = "Psychopath",                  AntiTheft = "True",                  MakeModel = "SportsCar",                  PropCost = "HundredThou")
CarValueCond <- querygrain(mod2grain, nodes = c("CarValue"), evidence = ev)
CarValueCond[[1]]["FiftyThou"]
mod1 <- hc(insuranceDF, score = "bde") summary(mod1) graphviz.plot(mod1)
mod1 <- hc(insuranceDF, score = "bde") summary(mod1) graphviz.plot(mod1)
mod1
####### INSTALLARE PRIMA DI ESEGUIRE #install.packages(c("bnlearn", "RBGL", "gRain", "igraph")) #source("https://bioconductor.org/biocLite.R") #biocLite("RBGL") #biocLite("graph") #biocLite("Rgraphviz") #biocLite("BiocGenerics") library(bnlearn) library(gRain) library(igraph) library(Rgraphviz) library(tidyverse) library(pcalg)                           # 1 load("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Stefanini/Esame Parte 2/insuranceDF.RData") nrow(insuranceDF) # 10000 osservazioni, 27 variabili tutte di tipo Factor names(insuranceDF) str(insuranceDF) # Non ci sono dati mancanti summary(insuranceDF) # 2.1  mod1 <- hc(insuranceDF, score = "bde") summary(mod1) graphviz.plot(mod1) # 2.2 library(dplyr) # La rete ha 54 archi orientati (arcs <- mod1$arcs) nrow(directed.arcs(mod1)) (directed.arcs(mod1)) for (node in nodes(mod1)) {     print(paste0("Nodo ", node))     if (length(which(mod1$arcs[, 1] == node)) > 0)         print(mod1$arcs[which(mod1$arcs[, 1] == node), 2])     else         print("senza figli")     } # Un unico nodo radice, nessun nodo isolato for (node2 in root.nodes(mod1)) {     print(paste0("Nodo radice ", node2))     if (length(which(mod1$arcs[, 1] == node2)) == 0)         print(paste0("Nodo isolato ", node2))     else         print("Nodo non isolato")     } # 3 bl1 <- cbind(names(insuranceDF[, -2]), rep("Age", length(names(insuranceDF[, -2])))) bl2 <- cbind(rep("PropCost", length(names(insuranceDF[, -20]))), names(insuranceDF[, -20])) bl3 <- cbind(rep("DrivHist", length(names(insuranceDF[, -27]))), names(insuranceDF[, -27])) bl <- rbind(bl1, bl2, bl3) mod.2 <- hc(insuranceDF, score = "bde", blacklist = bl) graphviz.plot(mod.2) # La rete contiene 56 archi orientati nrow(directed.arcs(mod.2)) directed.arcs(mod.2) # 4.1 wl <- rbind(c("SeniorTrain", "ThisCarDam"), c("MedCost", "RuggedAuto"), c("SocioEcon", "GoodStudent"),           c("Age", "RiskAversion"), c("Age", "ThisCarDam"), c("RuggedAuto", "VehicleYear")) mod.3 <- hc(insuranceDF, score = "bic", whitelist = wl) plot(mod.3) # La rete contiene 45 archi orientati nrow(directed.arcs(mod.3)) directed.arcs(mod.3) # 4.2 modello3 <- graph_from_data_frame(arcs(mod.3), directed = TRUE, vertices = NULL) plot(modello3) idbn <- tkplot(modello3) tk_set_coords(idbn, 2 * tk_coords(idbn)) coordinate <- tk_coords(idbn, norm = FALSE) tk_close(idbn) # 5.0 bnlearn::compare(mod.2, mod.3, arcs = FALSE) bnlearn::compare(mod.2, mod.3, arcs = TRUE) # Le 2 reti hanno 38 archi uguali # 6 fit = bn.fit(mod.3, insuranceDF) mod.3$arcs[which(mod.3$arcs[, 2] == "Theft")] #Conditional probability table for the Theft variable (theftProb <- fit$Theft) bn.fit.barchart(theftProb) bn.fit.dotplot(theftProb) theftProbab <- as.data.frame(fit$Theft$prob) str(theftProbab) # probabilità condizionata di Theft, condizionata all'avverarsi di ThisCarCost=="TenThou",ThisCarDam=="Moderate" freq <- filter(theftProbab, ThisCarCost == "TenThou", ThisCarDam == "Moderate") #bassisima probabilità che venga rubata una macchina del costo del ordine della decina di migliaia di dollari  #con danni di entità moderata freq # 7.1 require(pcalg) mod.2.graphnel <- as.graphNEL(mod.2) # PropCost non è indipendente da RiskAversion condizionatamente a MakeModel,DrivQuality,Theft pcalg::dsep("PropCost", "RiskAversion", c("MakeModel", "DrivQuality", "Theft"), mod.2.graphnel) # 7.2 mod.2.fit <- bn.fit(mod.2, insuranceDF) mod2gr <- as.grain(mod.2.fit) sim <- simulate(mod2gr, nsim = 10000) str(sim) modsim <- hc(sim, score = "bde") d <- graphviz.plot(modsim); plot(d, main = "Simulation of Bayesian Network", attrs = list(node = list(fillcolor = "yellow", fontsize = 70),             edge = list(color = "black"),             graph = list(rankdir = "UD"))) for (var in names(sim)) {     print(sim %>%           group_by_(var) %>%           summarise(n = n()) %>%           mutate(freq = n / sum(n))) } # 8 mod2grain <- as.grain(mod.2.fit) MPCarValue <- querygrain(mod2grain, nodes = c("CarValue")) MPCarValue[[1]]["FiftyThou"] ev <- list(Age = "Adult",                  RiskAversion = "Psychopath",                  AntiTheft = "True",                  MakeModel = "SportsCar",                  PropCost = "HundredThou") CarValueCond <- querygrain(mod2grain, nodes = c("CarValue"), evidence = ev) CarValueCond[[1]]["FiftyThou"] MPCarValue[[1]]["FiftyThou"] - CarValueCond[[1]]["FiftyThou"]
mod2grain <- as.grain(mod.2.fit) MPCarValue <- querygrain(mod2grain, nodes = c("CarValue")) MPCarValue[[1]]["FiftyThou"] ev <- list(Age = "Adult",                  RiskAversion = "Psychopath",                  AntiTheft = "True",                  MakeModel = "SportsCar",                  PropCost = "HundredThou") CarValueCond <- querygrain(mod2grain, nodes = c("CarValue"), evidence = ev) CarValueCond[[1]]["FiftyThou"] MPCarValue[[1]]["FiftyThou"] - CarValueCond[[1]]["FiftyThou"]
### Parte 1 fb.tr <- "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/Facebook-Training.txt" fb.te <- "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/Facebook-Test.txt" fb.train <- read.table(file = fb.tr, header = TRUE, sep = "\t", na.strings = "NA") fb.test <- read.table(file = fb.te, header = TRUE, sep = "\t", na.strings = "NA") ### Parte 2 sp.tr <- "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/Spam-Training.txt" sp.te <- "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/Spam-Test.txt" sp.train <- read.table(file = sp.tr, header = TRUE, sep = "\t", na.strings = "NA") sp.test <- read.table(file = sp.te, header = TRUE, sep = "\t", na.strings = "NA")
ls
ls()
### Parte 1 fb.train <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/Facebook-Training.txt", header = TRUE, sep = "\t", na.strings = "NA") fb.test <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/Facebook-Test.txt", header = TRUE, sep = "\t", na.strings = "NA")
ls()
rm()
ls()
fb.train <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/Facebook-Training.txt", header = TRUE, sep = "\t", na.strings = "NA") fb.test <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/Facebook-Test.txt", header = TRUE, sep = "\t", na.strings = "NA")
### Parte 1
fb.train <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/Facebook-Training.txt", header = TRUE, sep = "\t", na.strings = "NA")
fb.test <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/Facebook-Test.txt", header = TRUE, sep = "\t", na.strings = "NA")
fb.train <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/data/Facebook-Training.txt", header = TRUE, sep = "\t", na.strings = "NA")
fb.test <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/data/Facebook-Test.txt", header = TRUE, sep = "\t", na.strings = "NA")
coln
colnames(fb.)
colnames(fb.test)
#### Variables yVar <- "PsC24" xVar <- colnames(fb.train)
fb.test <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/data/Facebook-Test.txt", header = TRUE, sep = "\t", na.strings = "NA")
yVar <- "PsC24"
xVar <- colnames(fb.test)
#### Null model
formula <- as.formula(paste0(yVar, " ~ 1", paste0(xVar, collapse = " + ")))
null <- lm(formula = formula, data = fb.train)
formula <- as.formula(paste0(yVar, " ~ 1"))
null <- lm(formula = formula, data = fb.train)
#### Full model
formula <- as.formula(paste0(yVar, " ~ 1 + ", paste0(xVar, collapse = " * ")))
fb.train <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/data/Facebook-Training.txt", header = TRUE, sep = "\t", na.strings = "NA")
fb.test <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/data/Facebook-Test.txt", header = TRUE, sep = "\t", na.strings = "NA")
################################################################################
## Modeling settings (useful to avoid repetitions across approaches)
################################################################################
#### Variables
yVar <- "PsC24"
xVar <- colnames(fb.test)
################################################################################
## Stepwise selection
################################################################################
#### Null model
formula <- as.formula(paste0(yVar, " ~ 1"))
null <- lm(formula = formula, data = fb.train)
#### Full model
formula <- as.formula(paste0(yVar, " ~ 1 + ", paste0(xVar, collapse = " + ")))
full <- lm(formula = formula, data = fb.train)
#### Use k = 2 for AIC, k = log(NROW(data)) for BIC
forward <- step(object = null, scope = list(lower = null, upper = full),   direction = "forward", k = 2)
#### Use k = 2 for AIC, k = log(NROW(data)) for BIC
backward <- step(object = full, scope = list(lower = null, upper = full),   direction = "backward", k = 2)
#### BOTH
#### Use k = 2 for AIC, k = log(NROW(data)) for BIC
both <- step(object = null, scope = list(lower = null, upper = full),   direction = "both", k = 2)
library(MASS)
library(glmnet)
library(mgcv)
library(rpart)
library(ROCR)
### Parte 1
fb.train <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/data/Facebook-Training.txt", header = TRUE, sep = "\t", na.strings = "NA")
fb.test <- read.table(file = "~/Visual Studio 2017/Projects/MABIDA2017/Cipollini/Esame/data/Facebook-Test.txt", header = TRUE, sep = "\t", na.strings = "NA")
################################################################################
## Modeling settings (useful to avoid repetitions across approaches)
################################################################################
#### Variables
yVar <- "PsC24"
xVar <- colnames(fb.test)
################################################################################
## Stepwise selection
################################################################################
#### Null model
formula <- as.formula(paste0(yVar, " ~ 1"))
null <- lm(formula = formula, data = fb.train)
#### Full model
formula <- as.formula(paste0(yVar, " ~ 1 + ", paste0(xVar, collapse = " + ")))
full <- lm(formula = formula, data = fb.train)
#### FORWARD
#### Use k = 2 for AIC, k = log(NROW(data)) for BIC
forward <- step(object = null, scope = list(lower = null, upper = full),   direction = "forward", k = 2)
#### BACKWARD
#### Use k = 2 for AIC, k = log(NROW(data)) for BIC
backward <- step(object = full, scope = list(lower = null, upper = full),   direction = "backward", k = 2)
#### BOTH
#### Use k = 2 for AIC, k = log(NROW(data)) for BIC
both <- step(object = null, scope = list(lower = null, upper = full),   direction = "both", k = 2)
################################################################################
