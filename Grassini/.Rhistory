setwd("C:/Users/GiulioVannini/OneDrive - Il Gabbiano s.r.l/MasterMABIDA/Metodi Supervisionati")
credit<-read.table("credit_logit.csv", sep=",", header=T)
View(credit)
names(credit)
dim(credit)
mlogit=glm(y~acc1+acc2+duration+amount+moral+intuse, data=credit,family=binomial)
summary(mlogit)
coef(mlogit)
summary(mlogit)
coef(mlogit)
mlogit$coefficients[2]
summary(mlogit)$coef
attach(credit)
S=predict(mlogit,type="response")
summary(s)
summary(S)
attach(credit)
roc.curve=function(s,print=FALSE){
Ps=(S>s)*1
FP=sum((Ps==1)*(y==0))/sum(y==0)
TP=sum((Ps==1)*(y==1))/sum(y==1)
if(print==TRUE){
print(table(Observed=y,Predicted=Ps))
}
vect=c(FP,TP)
names(vect)=c("FPR","TPR")
return(vect)
}
threshold = 0.5
y=credit$y
roc.curve(threshold,print=TRUE)
threshold = 0.5
y=credit$y
roc.curve(threshold,print=TRUE)
threshold = 0.3
roc.curve(threshold,print=TRUE)
ROC.curve=Vectorize(roc.curve)
I=(((S>threshold)&(y==0))|((S<=threshold)&(y==1)))
plot(S,y,col=c("red","blue")[I+1],pch=19,cex=.7,,xlab="",ylab="")
abline(v=threshold,col="gray")
M.ROC=ROC.curve(seq(0,1,by=.01))
plot(M.ROC[1,],M.ROC[2,],col="grey",lwd=2,type="l", main="ROC curve",  xlab="False Positive Rate" , ylab="True positive rate",)
library(pROC)
install.packages("pROC")
plot(roc(y,S),legacy.axes = TRUE)
library(pROC)
plot(roc(y,S),legacy.axes = TRUE)
plot.roc(y, S, main="ROC Curve", percent=TRUE,legacy.axes = TRUE,
ci=TRUE, of="thresholds", # compute AUC (of threshold)
thresholds="best", # select the (best) threshold
print.thres="best") # also highlight this threshold on the plot
rm(list=ls(all=TRUE))
library(rpart)
source("C:/Users/GiulioVannini/OneDrive - Il Gabbiano s.r.l/MasterMABIDA/Cipollini/MBD2016-Functions-20160503.R")
file.data <- "C:/Users/GiulioVannini/OneDrive - Il Gabbiano s.r.l/MasterMABIDA/Cipollini/energydata_complete.csv"
data <- read.table(file = file.data, header = TRUE, sep = ",",
na.strings = "NA", colClasses = NA, check.names = FALSE, comment.char = "")
colnames(data)[colnames(data) == "date"] <- "time"
time <- strptime(x = data$time, format = "%Y-%m-%d %H:%M:%S", tz = "")
month <- format(x = time, format = "%m")
tmp <- NROW( levels(cut(x = time, breaks = "week")) )
week <- cut(x = time, breaks = "week", labels = 1 : tmp )
weekday <- weekdays(x = time, abbreviate = TRUE)
hour <- format(x = time, format = "%H")
lightsF <- data$lights
lightsF[lightsF >= 40] <- 40
lightsF <- factor(lightsF)
data <- data.frame(data, month = month, week = week, weekday = weekday, hour = hour,
Appliances.log = log(data$Appliances), lightsF = lightsF,
check.names = FALSE)
nobs <- NROW(data)
in1 <- round( nobs * 0.70 )
train <- data[ 1 : in1, , drop = FALSE]
test  <- data[ (in1 + 1) : nobs, , drop = FALSE]
cat( "dim(data)  = ", dim(data), "\n" )
cat( "dim(train) = ", dim(train), "\n" )
cat( "dim(test)  = ", dim(test), "\n" )
formula <- Appliances.log ~ weekday + hour + lightsF +
T2 + T3 + T8 + T9 +
RH_1 + RH_2 + RH_3 + RH_4 + RH_5 + RH_6 + RH_8 + RH_9 +
T_out + RH_out + Windspeed
fit <- lm(formula = formula, data = train)
fit.lm <- fit
formula <- Appliances.log ~ weekday + hour + lightsF +
T1 + T2 + T3 + T4 + T5 + T6 + T7 + T8 + T9 +
RH_1 + RH_2 + RH_3 + RH_4 + RH_5 + RH_6 + RH_7 + RH_8 + RH_9 +
T_out + RH_out + Windspeed + Visibility + Tdewpoint
control <- rpart.control(
minsplit = 20, ## Minimum number of observations in a node (group)
cp = 0.01,     ## Minimum cp decrease: any split not decreasing "rel error"
##  by a factor of cp is not attempted
## With "anova", the overall R-squared must increase by cp
##  at each step.
xval = 10)     ## Number of cross-validations to compute xerror
fit <- rpart(formula = formula, data = train, method = "anova", control = control,
model = TRUE)  ## model = TRUE useful for kfold-cv
par(mfrow = c(1,1))
plot(x = fit, uniform = FALSE, branch = 1, compress = FALSE, margin = 0, minbranch = 0.3)
text(fit)        ## Adds text, values and labels
print(fit)
printcp(fit)
plotcp(fit, minline = FALSE)
model <- fit
yVar <- rownames(attr(terms(model), "factors"))[1]
y <- train[, yVar]
layout(matrix(1:2, nc = 1))
plot(model, uniform = FALSE, margin = 0.1, branch = 1, compress = TRUE)
text(model)
rnames <- rownames(model$frame)
lev    <- rnames[sort(unique(model$where))]
where  <- factor(rnames[model$where], levels = lev)
boxplot(y ~ where, varwidth = TRUE,
ylim = range(y) * c(0.8, 1.2),
pars = list(axes = FALSE), ylab = yVar)
abline(h = mean(y), lty = 3)
axis(2)
n <- tapply(y, where, length)
text(1:length(n), max(y) * 1.2, paste("n = ", n))
rm(list=ls(all=TRUE))
library(MASS)
library(glmnet)
library(mgcv)
source("C:/Users/GiulioVannini/OneDrive - Il Gabbiano s.r.l/MasterMABIDA/Cipollini/MBD2016-Functions-20160503.R")
.ftest <-
function(fit0, fit1)
.ftest <-
function(fit0, fit1)
{
## FUNCTION:
#### ANOVA
tab <- anova(fit0, fit1)
#### Statistic
D0 <- tab[1, 2]
D1 <- tab[2, 2]
rdf0 <- tab[1, 1]
rdf1 <- tab[2, 1]
fstat <- (D0 - D1) / (D1 / rdf1)
df1 <- rdf0 - rdf1
df2 <- rdf1
pvalue <- 1 - pf(q = fstat, df1 = df1, df2 = df2)
#### Answer
c(D0 = D0, D1 = D1, rdf0 = rdf0, rdf1 = rdf1,
fstat = fstat, df1 = df1, df2 = df2, pvalue = pvalue)
}
.ftest.all <-
function(fit)
.ftest.all <-
function(fit)
{
## FUNCTION:
#### terms
formula <- fit$formula
vars <- rownames(attr(x = terms(formula), which = "factors"))
vY <- vars[ 1]
vX <- vars[-1]
data <- fit$model
#### Find spline terms
x1  <- strsplit(x = vX, split = "[(]|,")
ind <- which( mapply(FUN = "[", x1, 1) == "s" & mapply(FUN = "NROW", x1) > 1 )
vXT <- mapply(FUN = "[", x1[ind], 2)
#### Initialize
ans <- vector(length = NROW(ind), mode = "list")
j <- 0
#### Trace
cat("Linearity tests:\n")
#### Cycle
for ( i in ind )
{
#### Prog
j <- j + 1
#### Trace
cat(vXT[j], ", ")
#### Compose formula
vX0 <- vX
vX0[i] <- vXT[j]
formula0 <- paste0( vY, " ~ ", paste0( vX0 , collapse = " + "))
formula0 <- as.formula(formula0)
#### Estimate
fit0 <- gam(formula = formula0, family = fit$family, data = fit$model, scale = 0)
#### Test
ans[[j]] <- .ftest(fit0 = fit0, fit1 = fit)
}
#### Answer
data.frame(Var = vXT, do.call(what = rbind, args = ans),
check.names = FALSE)
}
file.data <- "C:/Users/GiulioVannini/OneDrive - Il Gabbiano s.r.l/MasterMABIDA/Cipollini/energydata_complete.csv"
data <- read.table(file = file.data, header = TRUE, sep = ",",
na.strings = "NA", colClasses = NA, check.names = FALSE, comment.char = "")
colnames(data)[colnames(data) == "date"] <- "time"
time <- strptime(x = data$time, format = "%Y-%m-%d %H:%M:%S", tz = "")
month <- format(x = time, format = "%m")
tmp <- NROW( levels(cut(x = time, breaks = "week")) )
week <- cut(x = time, breaks = "week", labels = 1 : tmp )
weekday <- weekdays(x = time, abbreviate = TRUE)
hour <- format(x = time, format = "%H")
lightsF <- data$lights
lightsF[lightsF >= 40] <- 40
lightsF <- factor(lightsF)
data <- data.frame(data, month = month, week = week, weekday = weekday, hour = hour,
Appliances.log = log(data$Appliances), lightsF = lightsF,
check.names = FALSE)
nobs <- NROW(data)
in1 <- round( nobs * 0.30 )
train <- data[ 1 : in1, , drop = FALSE]
test  <- data[ (in1 + 1) : nobs, , drop = FALSE]
cat( "dim(data)  = ", dim(data), "\n" )
cat( "dim(train) = ", dim(train), "\n" )
cat( "dim(test)  = ", dim(test), "\n" )
yVar <- "Appliances.log"
xVar <- c( "T1", "RH_1", "T2", "RH_2", "T3", "RH_3", "T4", "RH_4", "T5", "RH_5",
"T6", "RH_6", "T7", "RH_7", "T8", "RH_8", "T9", "RH_9",
"T_out", "Press_mm_hg", "RH_out", "Windspeed", "Visibility", "Tdewpoint",
"weekday", "hour", "lightsF")
formula <- Appliances.log ~ weekday + hour + lightsF +
T2 + T3 + T8 + T9 +
RH_1 + RH_2 + RH_3 + RH_4 + RH_5 + RH_6 + RH_8 + RH_9 +
T_out + RH_out + Windspeed
fit <- lm(formula = formula, data = train)
fit.lm <- fit
fx <- FALSE    ## Try TRUE and FALSE; explain
k  <- 12       ## Try different choices; explain
formula <- Appliances.log ~ weekday + hour + lightsF +
s(T2,        bs = "cr", k = k, fx = fx) +
s(T3,        bs = "cr", k = k, fx = fx) +
s(T8,        bs = "cr", k = k, fx = fx) +
s(T9,        bs = "cr", k = k, fx = fx) +
s(RH_1,      bs = "cr", k = k, fx = fx) +
s(RH_2,      bs = "cr", k = k, fx = fx) +
s(RH_3,      bs = "cr", k = k, fx = fx) +
s(RH_4,      bs = "cr", k = k, fx = fx) +
s(RH_5,      bs = "cr", k = k, fx = fx) +
s(RH_6,      bs = "cr", k = k, fx = fx) +
s(RH_8,      bs = "cr", k = k, fx = fx) +
s(RH_9,      bs = "cr", k = k, fx = fx) +
s(T_out,     bs = "cr", k = k, fx = fx) +
s(RH_out,    bs = "cr", k = k, fx = fx) +
s(Windspeed, bs = "cr", k = k, fx = fx)
fit <- gam(formula = formula, family = gaussian(), data = train, method = "GCV.Cp",
scale = 0)
fit.gam <- fit
formula0 <- Appliances.log ~ weekday + hour + lightsF +
s(T2,        bs = "cr", k = k, fx = fx) +
s(T3,        bs = "cr", k = k, fx = fx) +
s(T8,        bs = "cr", k = k, fx = fx) +
s(T9,        bs = "cr", k = k, fx = fx) +
s(RH_1,      bs = "cr", k = k, fx = fx) +
s(RH_2,      bs = "cr", k = k, fx = fx) +
s(RH_3,      bs = "cr", k = k, fx = fx) +
s(RH_4,      bs = "cr", k = k, fx = fx) +
s(RH_5,      bs = "cr", k = k, fx = fx) +
s(RH_6,      bs = "cr", k = k, fx = fx) +
s(RH_8,      bs = "cr", k = k, fx = fx) + RH_9 +
s(T_out,     bs = "cr", k = k, fx = fx) +
s(RH_out,    bs = "cr", k = k, fx = fx) +
s(Windspeed, bs = "cr", k = k, fx = fx)
fit0 <- gam(formula = formula0, family = gaussian(), data = train,
method = "GCV.Cp", scale = 0)
test0 <- .ftest(fit0 = fit0, fit1 = fit)
test.all <- .ftest.all(fit = fit)
k <- 10
seed <- 100000
cv.lm  <- .kfoldcv(k = k, model = fit.lm,  seed = seed)
cv.gam <- .kfoldcv(k = k, model = fit.gam, seed = seed)
x1 <- c("lm", "gam")
x2 <- rbind(cv.lm, cv.gam)
kfold <- data.frame(Model = x1, x2, check.names = FALSE)
cat("k-fold CV", "\n")
print(kfold)
test.all
test
test.all
test0 <- .ftest(fit0 = fit0, fit1 = fit)
test.all <- .ftest.all(fit = fit)
test.all
rm(list=ls(all=TRUE))
library(rpart)
source("C:/Users/GiulioVannini/OneDrive - Il Gabbiano s.r.l/MasterMABIDA/Cipollini/MBD2016-Functions-20160503.R")
file.data <- "C:/Users/GiulioVannini/OneDrive - Il Gabbiano s.r.l/MasterMABIDA/Cipollini/energydata_complete.csv"
data <- read.table(file = file.data, header = TRUE, sep = ",",
na.strings = "NA", colClasses = NA, check.names = FALSE, comment.char = "")
formula <- Appliances.log ~ weekday + hour + lightsF +
T1 + T2 + T3 + T4 + T5 + T6 + T7 + T8 + T9 +
RH_1 + RH_2 + RH_3 + RH_4 + RH_5 + RH_6 + RH_7 + RH_8 + RH_9 +
T_out + RH_out + Windspeed + Visibility + Tdewpoint
control <- rpart.control(
minsplit = 20, ## Minimum number of observations in a node (group)
cp = 0.01,     ## Minimum cp decrease: any split not decreasing "rel error"
##  by a factor of cp is not attempted
## With "anova", the overall R-squared must increase by cp
##  at each step.
xval = 10)     ## Number of cross-validations to compute xerror
fit <- rpart(formula = formula, data = train, method = "anova", control = control,
model = TRUE)  ## model = TRUE useful for kfold-cv
par(mfrow = c(1,1))
plot(x = fit, uniform = FALSE, branch = 1, compress = FALSE, margin = 0, minbranch = 0.3)
text(fit)        ## Adds text, values and labels
print(fit)
printcp(fit)
plotcp(fit, minline = FALSE)
model <- fit
rm(list=ls(all=TRUE))
library(rpart)
source("C:/Users/GiulioVannini/OneDrive - Il Gabbiano s.r.l/MasterMABIDA/Cipollini/MBD2016-Functions-20160503.R")
file.data <- "C:/Users/GiulioVannini/OneDrive - Il Gabbiano s.r.l/MasterMABIDA/Cipollini/energydata_complete.csv"
data <- read.table(file = file.data, header = TRUE, sep = ",",
na.strings = "NA", colClasses = NA, check.names = FALSE, comment.char = "")
colnames(data)[colnames(data) == "date"] <- "time"
time <- strptime(x = data$time, format = "%Y-%m-%d %H:%M:%S", tz = "")
month <- format(x = time, format = "%m")
tmp <- NROW( levels(cut(x = time, breaks = "week")) )
week <- cut(x = time, breaks = "week", labels = 1 : tmp )
weekday <- weekdays(x = time, abbreviate = TRUE)
hour <- format(x = time, format = "%H")
lightsF <- data$lights
lightsF[lightsF >= 40] <- 40
lightsF <- factor(lightsF)
data <- data.frame(data, month = month, week = week, weekday = weekday, hour = hour,
Appliances.log = log(data$Appliances), lightsF = lightsF,
check.names = FALSE)
nobs <- NROW(data)
in1 <- round( nobs * 0.70 )
train <- data[ 1 : in1, , drop = FALSE]
test  <- data[ (in1 + 1) : nobs, , drop = FALSE]
cat( "dim(data)  = ", dim(data), "\n" )
cat( "dim(train) = ", dim(train), "\n" )
cat( "dim(test)  = ", dim(test), "\n" )
formula <- Appliances.log ~ weekday + hour + lightsF +
T2 + T3 + T8 + T9 +
RH_1 + RH_2 + RH_3 + RH_4 + RH_5 + RH_6 + RH_8 + RH_9 +
T_out + RH_out + Windspeed
fit <- lm(formula = formula, data = train)
fit.lm <- fit
formula <- Appliances.log ~ weekday + hour + lightsF +
T1 + T2 + T3 + T4 + T5 + T6 + T7 + T8 + T9 +
RH_1 + RH_2 + RH_3 + RH_4 + RH_5 + RH_6 + RH_7 + RH_8 + RH_9 +
T_out + RH_out + Windspeed + Visibility + Tdewpoint
control <- rpart.control(
minsplit = 20, ## Minimum number of observations in a node (group)
cp = 0.01,     ## Minimum cp decrease: any split not decreasing "rel error"
##  by a factor of cp is not attempted
## With "anova", the overall R-squared must increase by cp
##  at each step.
xval = 10)     ## Number of cross-validations to compute xerror
fit <- rpart(formula = formula, data = train, method = "anova", control = control,
model = TRUE)  ## model = TRUE useful for kfold-cv
par(mfrow = c(1,1))
plot(x = fit, uniform = FALSE, branch = 1, compress = FALSE, margin = 0, minbranch = 0.3)
text(fit)        ## Adds text, values and labels
print(fit)
printcp(fit)
plotcp(fit, minline = FALSE)
model <- fit
yVar <- rownames(attr(terms(model), "factors"))[1]
y <- train[, yVar]
layout(matrix(1:2, nc = 1))
plot(model, uniform = FALSE, margin = 0.1, branch = 1, compress = TRUE)
text(model)
rnames <- rownames(model$frame)
lev    <- rnames[sort(unique(model$where))]
where  <- factor(rnames[model$where], levels = lev)
boxplot(y ~ where, varwidth = TRUE,
ylim = range(y) * c(0.8, 1.2),
pars = list(axes = FALSE), ylab = yVar)
abline(h = mean(y), lty = 3)
axis(2)
n <- tapply(y, where, length)
text(1:length(n), max(y) * 1.2, paste("n = ", n))
fit.rpart1  <- fit
control <- rpart.control(
minsplit = 20, ## Minimum number of observations in a node
cp = 0.001,    ## Minimum cp decrease
xval = 10)     ## Number of cross-validations for xerror
fit <- rpart(formula = formula, data = train, method = "anova", control = control,
model = TRUE, x = FALSE, y = TRUE)
plotcp(fit, minline = TRUE)
par(mfrow = c(1,2))
rsq.rpart(fit) ## Another useful plot (Relative vs Apparent)
ind <- which.min(fit$cptable[, "xerror"])
ind <- fit$cptable[, "xerror"] > min(fit$cptable[, "xerror"] + fit$cptable[, "xstd"])
ind <- which.min(fit$cptable[ind, "xerror"])
fit.rpart2  <- prune(tree = fit, cp = fit$cptable[ind, "CP"])   ## Prune
k <- 20
par(mfrow = c(1,1))
plot(x = fit, uniform = FALSE, branch = 1, compress = FALSE, margin = 0, minbranch = 0.3)
text(fit)        ## Adds text, values and labels
print(fit)
fit <- rpart(formula = formula, data = train, method = "anova", control = control,
model = TRUE)  ## model = TRUE useful for kfold-cv
par(mfrow = c(1,1))
plot(x = fit, uniform = FALSE, branch = 1, compress = FALSE, margin = 0, minbranch = 0.3)
text(fit)        ## Adds text, values and labels
print(fit)
printcp(fit)
plotcp(fit, minline = FALSE)
model <- fit
yVar <- rownames(attr(terms(model), "factors"))[1]
y <- train[, yVar]
layout(matrix(1:2, nc = 1))
plot(model, uniform = FALSE, margin = 0.1, branch = 1, compress = TRUE)
text(model)
rnames <- rownames(model$frame)
lev    <- rnames[sort(unique(model$where))]
where  <- factor(rnames[model$where], levels = lev)
boxplot(y ~ where, varwidth = TRUE,
ylim = range(y) * c(0.8, 1.2),
pars = list(axes = FALSE), ylab = yVar)
abline(h = mean(y), lty = 3)
axis(2)
n <- tapply(y, where, length)
text(1:length(n), max(y) * 1.2, paste("n = ", n))
fit.rpart1  <- fit
control <- rpart.control(
minsplit = 20, ## Minimum number of observations in a node
cp = 0.001,    ## Minimum cp decrease
xval = 10)     ## Number of cross-validations for xerror
fit <- rpart(formula = formula, data = train, method = "anova", control = control,
model = TRUE, x = FALSE, y = TRUE)
plotcp(fit, minline = TRUE)
par(mfrow = c(1,2))
rsq.rpart(fit) ## Another useful plot (Relative vs Apparent)
control <- rpart.control(
minsplit = 20, ## Minimum number of observations in a node
cp = 0.001,    ## Minimum cp decrease
xval = 10)     ## Number of cross-validations for xerror
fit <- rpart(formula = formula, data = train, method = "anova", control = control,
model = TRUE, x = FALSE, y = TRUE)
plotcp(fit, minline = TRUE)
par(mfrow = c(1,2))
rsq.rpart(fit) ## Another useful plot (Relative vs Apparent)
ind <- which.min(fit$cptable[, "xerror"])
ind <- fit$cptable[, "xerror"] > min(fit$cptable[, "xerror"] + fit$cptable[, "xstd"])
ind <- which.min(fit$cptable[ind, "xerror"])
fit.rpart2  <- prune(tree = fit, cp = fit$cptable[ind, "CP"])   ## Prune
k <- 20
seed <- 100000
cv.lm     <- .kfoldcv(k = k, model = fit.lm, seed = seed)
cv.rpart1 <- .kfoldcv(k = k, model = fit.rpart1, seed = seed)
cv.rpart2 <- .kfoldcv(k = k, model = fit.rpart2, seed = seed)
x1 <- c("lm", "rpart1", "rpart2")
x2 <- rbind(cv.lm, cv.rpart1, cv.rpart2)
fit.rpart2  <- prune(tree = fit, cp = fit$cptable[ind, "CP"])   ## Prune
k <- 20
seed <- 100000
cv.lm     <- .kfoldcv(k = k, model = fit.lm, seed = seed)
cv.rpart1 <- .kfoldcv(k = k, model = fit.rpart1, seed = seed)
cv.rpart2 <- .kfoldcv(k = k, model = fit.rpart2, seed = seed)
x1 <- c("lm", "rpart1", "rpart2")
x2 <- rbind(cv.lm, cv.rpart1, cv.rpart2)
kfold <- data.frame(Model = x1, x2, check.names = FALSE)
cat("k-fold CV", "\n")
print(kfold)
fit.rpart2  <- prune(tree = fit, cp = fit$cptable[ind, "CP"])   ## Prune
fit.rpart2
k <- 20
seed <- 100000
cv.lm     <- .kfoldcv(k = k, model = fit.lm, seed = seed)
cv.rpart1 <- .kfoldcv(k = k, model = fit.rpart1, seed = seed)
cv.rpart2 <- .kfoldcv(k = k, model = fit.rpart2, seed = seed)
x1 <- c("lm", "rpart1", "rpart2")
x2 <- rbind(cv.lm, cv.rpart1, cv.rpart2)
kfold <- data.frame(Model = x1, x2, check.names = FALSE)
cat("k-fold CV", "\n")
print(kfold)
str(df, list=101)
setwd("C:\Users\GiulioVannini\Documents\Visual Studio 2017\Projects\MABIDA2017\Grassini")
#install.packages("C50")
library(C50)
install.packages("C50", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
#install.packages("C50")
library(C50)
setwd("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Grassini")
crx <- read.table( file="expanded_AGARICUS_LEPIOTA.txt", header=TRUE, sep="," )
head(crx)
summary(crx)
set.seed(22)
crx <- crx[sample(nrow(crx)),] # Mischia i record, prima di estratte training e test set
crx$veil.type=NULL # ha un solo valore
X <- crx[,-1]
y <- crx[,1]
# Creazione training set e test set
trainX <- X[1:8000,]
trainy <- y[1:8000]
testX <- X[8001:8416,]
testy <- y[8001:8416]
# Realizzo il modello
model <- C5.0(trainX, trainy)
summary(model)
plot(model)
# Con variabile dipendente categorica/discreta
#install.packages("C50")
library(C50)
setwd("C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Grassini")
crx <- read.table( file="expanded_AGARICUS_LEPIOTA.txt", header=TRUE, sep="," )
head(crx)
summary(crx)
set.seed(22)
crx <- crx[sample(nrow(crx)),] # Mischia i record, prima di estratte training e test set
crx$veil.type=NULL # ha un solo valore
X <- crx[,-1]
y <- crx[,1]
# Creazione training set e test set
trainX <- X[1:8000,]
trainy <- y[1:8000]
testX <- X[8001:8416,]
testy <- y[8001:8416]
# Realizzo il modello
model <- C5.0(trainX, trainy)
summary(model)
plot(model)
# Aggiungo una matrice di costi per penalizzare l'erronea classificazione dei funghi
#levels(crx[,1])
cost_matrix<-matrix(c(0,10,1,0),2,2,byrow=TRUE)
cost_matrix
rownames(cost_matrix)<-levels(crx[,1])
colnames(cost_matrix)<-levels(crx[,1])
cost_matrix
model.1<-C5.0(trainX,trainy,costs=cost_matrix)
summary(model.1)
plot(model.1)
predizione<-predict(model.1,testX,type='class') ## classe assegnata dalla regola
summary(predizione)
table(testy,predizione)
probabilita<-predict(model,testX,type='prob')  ## probabilità
str(probabilita)
head(probabilita)
probabilita<-predict(model.1,testX,type='prob')  ## probabilità
str(probabilita)
head(probabilita)
probabilita<-predict(model,testX,type='prob')  ## probabilità
probabilita<-predict(model,testX,type='prob')  ## probabilità
str(probabilita)
head(probabilita)
install.packages("rpart.plot", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
dove_commestibile<-which(probabilita[,1]>.5)
dove_nonCommestibile<-which(probabilita[,2]>.5)
####################################################################################
# Con variabile dipendente continua
#install.packages("rpart")
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
#### File (adjust path)
file.data <- "C:/Users/GiulioVannini/Documents/Visual Studio 2017/Projects/MABIDA2017/Grassini/Income-training.csv"
remove <- c("", "Numero.di.FamiglieProvincia", "Numero.di.FamiglieRegione")
#### Read data
data <- read.table(file = file.data, header = TRUE, sep = ",",                     na.strings = "NA", colClasses = NA, check.names = FALSE, comment.char = "")
ind <- !( colnames(data) %in% remove )
data <- data[, ind, drop = FALSE]
# data$gini.index <- log(data$gini.index / (1 - data$gini.index))
## Levels on log scale
head(data)
## VAR on log scale
data <- data[sample(nrow(data)),] # Mischia i record, prima di estratte training e test set
#summary(data)
# Creazione training set e test set
datatr <- data[1:5500,]
datate <- data[5501:6071,]
# Le variabili che entrano nel nostro modello
formula <- gini.index ~ Regione + Popolazione.TotaleProvincia + Numero.medio.di.componenti.per.famigliaProvincia +             contribuenti
## Here a selection of the main control parameters
# Le variabili che entrano nel nostro modello
formula <- gini.index ~ Regione + Popolazione.TotaleProvincia + Numero.medio.di.componenti.per.famigliaProvincia +             contribuenti
## Here a selection of the main control parameters
control <- rpart.control(   minsplit = 20, ## Minimum number of observations in a node   cp = 0.0025,     ## Minimum cp decrease: any split not decreasing "rel error"                   ##  by a factor of cp is not attempted                  ## With "anova", the overall R-squared must increase by cp                   ##  at each step.    xval = 10)     ## Number of cross-validations for xerror
## Fit
fit <- rpart(formula = formula, data = datatr, method = "anova", control = control)
plot(x = fit, uniform = FALSE, branch = 0.1, compress = FALSE, margin = 0, minbranch = 0.3) 
text(fit)        ## Adds text, values and labels
plot(x = fit, uniform = FALSE, branch = 0.1, compress = FALSE, margin = 0, minbranch = 0.3) 
text(fit)        ## Adds text, values and labels
print(fit)
printcp(fit)
plotcp(fit, minline = FALSE)
## min(xerror)
ind <- which.min(fit$cptable[, "xerror"])
# Potatura
fit.prune <- prune(tree = fit, cp = fit$cptable[ind, "CP"]) 
print(fit.prune)
# Valutiamo il modello sul data test
dat<-datate[,c("Regione","Popolazione.TotaleProvincia","Numero.medio.di.componenti.per.famigliaProvincia",             "contribuenti")]
predizione<-predict(fit.prune,dat)
SE<-sqrt((predizione-datate[,"gini.index"])^2)
mean(SE)
summary(SE)
plot(SE)
summary(predizione)
predizione[1:20]
datate[,"gini.index"][1:20]
plot(fit.prune)
text(fit.prune)
library(readxl)    # free data from excel hades
library(dplyr)     # sane data manipulation
library(tidyr)     # sane data munging
library(viridis)   # sane colors
install.packages('dplyr')
library(readxl)    # free data from excel hades
library(dplyr)     # sane data manipulation
library(tidyr)     # sane data munging
library(viridis)   # sane colors
library(ggplot2)   # needs no introduction
library(ggfortify) # super-helpful for plotting non-"standard" stats objects
install.packages('ggfortify')
library(ggfortify) # super-helpful for plotting non-"standard" stats objects
my_seed = 0
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/")
#The dataset contains both information on marketing newsletters/e-mail campaigns (e-mail offers sent) and 
#transaction level data from customers (which offer customers responded to and what they bought).
#get data
url <- "http://blog.yhathq.com/static/misc/data/WineKMC.xlsx"
d.file <- basename(url)
if (!file.exists(d.file)) download.file(url, d.file)
#Read Campaign Data
offers <- read_excel(d.file, sheet = 1)
install.packages("readOffice", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
unloadNamespace("readxl")
library("readxl", lib.loc="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
library(readxl)    # free data from excel hades
library(dplyr)     # sane data manipulation
library(tidyr)     # sane data munging
library(viridis)   # sane colors
library(ggplot2)   # needs no introduction
library(ggfortify) # super-helpful for plotting non-"standard" stats objects
my_seed = 0
set.seed(my_seed)
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/")
#The dataset contains both information on marketing newsletters/e-mail campaigns (e-mail offers sent) and 
#transaction level data from customers (which offer customers responded to and what they bought).
#get data
url <- "http://blog.yhathq.com/static/misc/data/WineKMC.xlsx"
d.file <- basename(url)
if (!file.exists(d.file)) download.file(url, d.file)
#Read Campaign Data
offers <- read_excel(d.file, sheet = 1)
library("readOffice", lib.loc="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
library(readOffice)    # free data from excel hades
offers <- readOffice(d.file, sheet = 1)
readxl
unloadNamespace("readOffice")
library(readxl)    # free data from excel hades
library(dplyr)     # sane data manipulation
library(tidyr)     # sane data munging
library(viridis)   # sane colors
library(ggplot2)   # needs no introduction
library(ggfortify) # super-helpful for plotting non-"standard" stats objects
my_seed = 0
set.seed(my_seed)
install.packages("ggfortify", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
library("ggfortify", lib.loc="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
#get data
url <- "http://blog.yhathq.com/static/misc/data/WineKMC.xlsx"
d.file <- basename(url)
if (!file.exists(d.file)) download.file(url, d.file)
#Read Campaign Data
offers <- read_excel(d.file, sheet = 1)
offers <- read_excel("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMC.xlsx", sheet = 1)
library(readxl)    # free data from excel hades
library(dplyr)     # sane data manipulation
library(tidyr)     # sane data munging
library(viridis)   # sane colors
library(ggplot2)   # needs no introduction
library(ggfortify) # super-helpful for plotting non-"standard" stats objects
my_seed = 0
set.seed(my_seed)
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/")
#The dataset contains both information on marketing newsletters/e-mail campaigns (e-mail offers sent) and 
#transaction level data from customers (which offer customers responded to and what they bought).
#get data
url <- "http://blog.yhathq.com/static/misc/data/WineKMC.xlsx"
d.file <- basename(url)
if (!file.exists(d.file)) download.file(url, d.file)
#Read Campaign Data
offers <- read_excel(d.file, sheet = 1)
offers <- read_excel("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMC.xlsx", sheet = 1)
offers <- read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCOffers.xlsx", sheet = 1)
install.packages("csv", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
offers <- read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCOffers.xlsx", sheet = 1)
offers <- readcsv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCOffers.xlsx", sheet = 1)
offers <- readcsv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCOffers.csv", sheet = 1)
library("csv", lib.loc="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
library(readcsv)    # free data from excel hades
library(csv)    # free data from excel hades
offers <- readcsv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCOffers.csv", sheet = 1)
offers <- read.csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCOffers.csv", sheet = 1)
offers <- read.csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCOffers.csv")
library(dplyr)     # sane data manipulation
library(tidyr)     # sane data munging
library(viridis)   # sane colors
library(ggplot2)   # needs no introduction
library(ggfortify) # super-helpful for plotting non-"standard" stats objects
my_seed = 0
set.seed(my_seed)
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/")
#The dataset contains both information on marketing newsletters/e-mail campaigns (e-mail offers sent) and 
#transaction level data from customers (which offer customers responded to and what they bought).
#get data
#url <- "http://blog.yhathq.com/static/misc/data/WineKMC.xlsx"
#d.file <- basename(url)
#if (!file.exists(d.file)) download.file(url, d.file)
#Read Campaign Data
offers <- read.csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCOffers.csv")
colnames(offers) <- c("offer_id", "campaign", "varietal", "min_qty", "discount", "origin", "past_peak")
offers <- read.csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCOffers.csv", header = F, sep = ";")
colnames(offers) <- c("offer_id", "campaign", "varietal", "min_qty", "discount", "origin", "past_peak")
#Show Campaign Data
head(offers)
#Read Transactional Data
transactions <- read_excel(fil, sheet = 2)
transactions <- read.csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCTransactions.csv", header = F, sep = ";")
colnames(transactions) <- c("customer_name", "offer_id")
transactions$n <- 1
# Show Campaign Data
head(transactions)
# join the offers and transactions table
left_join(offers, transactions, by = "offer_id") %>%   # get the number of times each customer responded to a given offer count(customer_name, offer_id, wt = n) %>% # change it from long to wide spread(offer_id, nn) %>%  # and fill in the NAs that get generated as a result mutate_each(funs(ifelse(is.na(.), 0, .))) -> dat
head(dat)
# How does clustering apply to our customers? We're trying to learn more about how our customers 
# behave, we can use their behavior (whether or not they purchased something based on an offer) 
# as a way to group similar minded customers together. We can then study those groups to look 
# for patterns and trends which can help us formulate future offers.
nr_cluster <- 4
fit <- kmeans(dat[,-1], nr_cluster, iter.max=1000)
table(fit$cluster)
table(fit$cluster)
barplot(table(fit$cluster), col="maroon")
autoplot(fit, data=dat[,-1], frame=TRUE)
autoplot(fit, data=dat[,-1], frame=TRUE)
barplot(table(fit$cluster), col="maroon")
autoplot(fit, data=dat[,-1], frame=TRUE)
dat[, -1]
autoplot(fit, data=dat[,-1], frame=TRUE)
library(ggfortify) # super-helpful for plotting non-"standard" stats objects
autoplot(fit, data=dat[,-1], frame=TRUE)
library(dplyr)     # sane data manipulation
library(tidyr)     # sane data munging
library(viridis)   # sane colors
library(ggplot2)   # needs no introduction
library(ggfortify) # super-helpful for plotting non-"standard" stats objects
my_seed = 0
set.seed(my_seed)
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/")
#The dataset contains both information on marketing newsletters/e-mail campaigns (e-mail offers sent) and 
#transaction level data from customers (which offer customers responded to and what they bought).
#get data
#url <- "http://blog.yhathq.com/static/misc/data/WineKMC.xlsx"
#d.file <- basename(url)
#if (!file.exists(d.file)) download.file(url, d.file)
#Read Campaign Data
offers <- read.csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCOffers.csv", header = F, sep = ";")
colnames(offers) <- c("offer_id", "campaign", "varietal", "min_qty", "discount", "origin", "past_peak")
#Show Campaign Data
head(offers)
#Read Transactional Data
transactions <- read.csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/WineKMCTransactions.csv", header = F, sep = ";")
colnames(transactions) <- c("customer_name", "offer_id")
transactions$n <- 1
# Show Campaign Data
head(transactions)
# The first thing we need is a way to compare customers. 
# To do this, we're going to create a matrix that contains each customer 
# and a 0/1 indicator for whether or not they responded to a given offer.
# dplyr PIPE operator %>%
# join the offers and transactions table
left_join(offers, transactions, by = "offer_id") %>%   # get the number of times each customer responded to a given offer count(customer_name, offer_id, wt = n) %>% # change it from long to wide spread(offer_id, nn) %>%  # and fill in the NAs that get generated as a result mutate_each(funs(ifelse(is.na(.), 0, .))) -> dat
head(dat)
# How does clustering apply to our customers? We're trying to learn more about how our customers 
# behave, we can use their behavior (whether or not they purchased something based on an offer) 
# as a way to group similar minded customers together. We can then study those groups to look 
# for patterns and trends which can help us formulate future offers.
nr_cluster <- 4
fit <- kmeans(dat[,-1], nr_cluster, iter.max=1000)
table(fit$cluster)
barplot(table(fit$cluster), col="maroon")
autoplot(fit, data=dat[,-1], frame=TRUE)
# A really cool trick that the probably didn't teach you in school is Principal Component Analysis. 
# There are lots of uses for it, but today we're going to use it to transform our multi-dimensional 
# dataset into a 2 dimensional dataset. Why? Because it becomes much easier to plot!
pca <- prcomp(dat[,-1])
print(pca)
plot(pca, type = "l")
autoplot(fit, frame=TRUE)
autoplot(fit, data=dat, frame=TRUE)
autoplot(fit, data=dat[,], frame=TRUE)
autoplot(fit, data=dat[], frame=TRUE)
autoplot(fit, data = dat[, -1], frame = TRUE)
pca <- prcomp(dat[,-1])
print(pca)
plot(pca, type = "l")
pca_dat <- mutate(fortify(pca), fitted.cluster = fit$cluster) #fortify convert pca variable to data.frame
ggplot(pca_dat) +   geom_point(aes(x=PC1, y=PC2, fill = factor(fitted.cluster)), size=3, col="#7f7f7f", shape = 21) +   scale_fill_viridis(name="Cluster", discrete=TRUE) + theme_bw(base_family="Helvetica")
#If you want to get fancy, you can also plot the centers of the clusters as well. 
#These are stored in the KMeans instance using the cluster_centers_ variable. #
#Make sure that you also transform the cluster centers into the 2-D projection.
cluster_centers = predict(pca, newdata=fit$centers)
cluster_centers = fortify(cluster_centers[,1:2])
cluster_centers$fitted.cluster = seq(1:nr_cluster)
pca_dat <- left_join(pca_dat[,c("PC1", "PC2", "fitted.cluster")], cluster_centers, by="fitted.cluster")
ggplot(pca_dat) +   geom_point(aes(x=PC1.x, y=PC2.x, fill=factor(fitted.cluster)), size=3, col="#7f7f7f", shape=21) +   geom_point(aes(x=PC1.y, y=PC2.y, fill=factor(fitted.cluster)), size = 8, col="#e55252", show.legend = FALSE) +   scale_fill_viridis(name="Cluster", discrete=TRUE) + theme_bw(base_family="Helvetica")
pca <- prcomp(dat[,-1])
pca <- prcomp(dat[,-1])
print(pca)
plot(pca, type = "l")
pca_dat <- mutate(fortify(pca), fitted.cluster = fit$cluster) #fortify convert pca variable to data.frame
ggplot(pca_dat) +   geom_point(aes(x=PC1, y=PC2, fill = factor(fitted.cluster)), size=3, col="#7f7f7f", shape = 21) +   scale_fill_viridis(name="Cluster", discrete=TRUE) + theme_bw(base_family="Helvetica")
transactions %>%    left_join(data_frame(customer_name=dat$customer_name,                         cluster=fit$cluster)) %>%    left_join(offers) -> customer_clusters
customer_clusters %>%    mutate(is_1=(cluster==1)) %>%    count(is_1, varietal) -> varietal_1
varietal_1
head(customer_clusters)
customer_clusters %>%    mutate(is_1=(cluster==1)) %>%    count(is_1, varietal) -> varietal_1
varietal_1
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/")
dir("Data")
require(lattice)
require(grDevices)
require(vcd)
load(file = "Data/InterestPreferenceSurvey.Rda")
str(csb)
load(file = "Data/InterestPreferenceSurvey.Rda")
str(csb)
# This is (mostly) an "optional-response type" survey
# 1 = “yes” is significant
# 0 is just absence not really a “no”
# Respondents checking Role_SA have much more in common than those not checking Role_SA
require(flexclust)
install.packages("flexclust", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
library("flexclust", lib.loc="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
require(lattice)
require(grDevices)
require(vcd)
load(file = "Data/InterestPreferenceSurvey.Rda")
str(csb)
# This is (mostly) an "optional-response type" survey
# 1 = “yes” is significant
# 0 is just absence not really a “no”
# Respondents checking Role_SA have much more in common than those not checking Role_SA
require(flexclust)
## set up flexclust control object
fc_cont <- new("flexclustControl")
fc_cont@tolerance <- 0.1
## this doesn't seem to work as expected
fc_cont@iter.max <- 30
## seems to be effective convergence
## fc_cont@verbose <- 1
## set TRUE if to see each step
my_seed <- 0
my_family <- "ejaccard"
num_clust <- 4
my_seed <- my_seed + 1
set.seed(my_seed)
cl <- kcca(csb,             k = num_clust,             save.data = TRUE,             control = fc_cont,            family = kccaFamily(my_family))
summary(cl)
nsamp = as.character(dim(csb)[1]/1000)
#preparo il titolo del grafico
main_txt <- paste("kcca ",                    cl@family@name,                    " - ",                   num_clust,                    " clusters (",                   nsamp,                    "k sample,                    seed = ",                    my_seed,                   ")",                    sep = "")
# Neighborhood Graph on 1st principle components
csb.pca <- prcomp(csb)  # Performs a principal components analysis on the given data matrix 
                        # Returns the results as an object of class prcomp.
# used in the plot
pop_av_dist <- with(cl@clusinfo, sum(size*av_dist)/sum(size))
plot(cl,       data = as.matrix(csb),       project = csb.pca,      main = main_txt,      sub = paste("\nAv Dist = ",                   format(pop_av_dist, digits = 3),                  ", k = ",                   cl@k,                   sep = "") )
# Activity Profiles for each segment
print(barchart(cl,                 main = main_txt,                 strip.prefix = "#",                scales = list(cex = 0.6)))
# HOW TO SELECT K
# 1. Choice of k, must have mostly ~ stable solutions, and
# 2. Cluster profiles must be interpretable: what is the story you can  
# tell about each cluster? Will the marketers relate to it?
fc_cont@iter.max <- 200
my_seed <- 9
my_family <- "ejaccard"
num_clust <- 6
set.seed(my_seed)
cl <- kcca(csb,             k = num_clust,             save.data = TRUE,             control = fc_cont,            family = kccaFamily(my_family))
summary(cl)
# Neighborhood Graph 
csb.pca <- prcomp(csb)
plot(cl,       data = as.matrix(csb),       project = csb.pca,      main = main_txt,      sub = paste("\nAv Dist = ",                   format(pop_av_dist, digits = 5),                  ", k = ",                   cl@k,                   sep = "") )
# Activity Profiles for each segment
print(barchart(cl,                 main = main_txt,                 strip.prefix = "#",                scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
require(flexclust)
## set up flexclust control object
fc_cont <- new("flexclustControl")
fc_cont@tolerance <- 0.1
## this doesn't seem to work as expected
fc_cont@iter.max <- 30
## seems to be effective convergence
## fc_cont@verbose <- 1
## set TRUE if to see each step
my_seed <- 0
my_family <- "ejaccard"
num_clust <- 4
my_seed <- my_seed + 1
set.seed(my_seed)
cl <- kcca(csb,             k = num_clust,             save.data = TRUE,             control = fc_cont,            family = kccaFamily(my_family))
summary(cl)
# Neighborhood Graph on 1st principle components
csb.pca <- prcomp(csb)  # Performs a principal components analysis on the given data matrix 
                        # Returns the results as an object of class prcomp.
# used in the plot
pop_av_dist <- with(cl@clusinfo, sum(size*av_dist)/sum(size))
plot(cl,       data = as.matrix(csb),       project = csb.pca,      main = main_txt,      sub = paste("\nAv Dist = ",                   format(pop_av_dist, digits = 3),                  ", k = ",                   cl@k,                   sep = "") )
# Activity Profiles for each segment
print(barchart(cl,                 main = main_txt,                 strip.prefix = "#",                scales = list(cex = 0.6)))
# HOW TO SELECT K
# 1. Choice of k, must have mostly ~ stable solutions, and
# 2. Cluster profiles must be interpretable: what is the story you can  
# tell about each cluster? Will the marketers relate to it?
fc_cont@iter.max <- 200
my_seed <- 9
my_family <- "ejaccard"
num_clust <- 6
set.seed(my_seed)
cl <- kcca(csb,             k = num_clust,             save.data = TRUE,             control = fc_cont,            family = kccaFamily(my_family))
summary(cl)
# Neighborhood Graph 
csb.pca <- prcomp(csb)
plot(cl,       data = as.matrix(csb),       project = csb.pca,      main = main_txt,      sub = paste("\nAv Dist = ",                   format(pop_av_dist, digits = 5),                  ", k = ",                   cl@k,                   sep = "") )
# Activity Profiles for each segment
print(barchart(cl,                 main = main_txt,                 strip.prefix = "#",                scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
               scales = list(cex = 0.6)))
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/")
library(flexclust)
data("volunteers") 
vol_ch <-  volunteers[-(1:2)] 
vol.mat <-  as.matrix(vol_ch)
fc_cont<-  new("flexclustControl")
## holds "hyperparameters"
fc_cont@tolerance <- 0.1
fc_cont@iter.max <- 30
fc_cont@verbose <- 0
## verbose > 0 will show iterations
fc_family <-"ejaccard"
## Jaccard distance w/ centroid means
fc_seed <-  577
## Why we use this seed will become clear below
num_clusters <-  3
## Simple example – only three clusters 
set.seed(fc_seed)
vol.cl <- kcca(vol.mat,                 k = num_clusters,                 save.data = TRUE,                 control = fc_cont,                 family = kccaFamily(fc_family))
summary(vol.cl)
vol.pca <-prcomp(vol.mat) ## plot on first two principal components 
nsamp = as.character(dim(vol.mat)[1]/1000)
main_txt <- paste("kcca ",                    vol.cl@family@name,                    " - ",                   num_clusters,                    " clusters (",                   nsamp,                    "k sample, seed = ",                    fc_seed,                   ")",                    sep = "")
plot(vol.cl, data = vol.mat, project = vol.pca, main = main_txt)
barchart(vol.cl,strip.prefix = "#", shade = TRUE,layout= c(vol.cl@k  , 1), main  = "titolo")
plot(vol.cl, data = vol.mat, project = vol.pca, main = main_txt)
plot(vol.cl, data = vol.mat, project = vol.pca, main = main_txt)
barchart(vol.cl,strip.prefix = "#", shade = TRUE,layout= c(vol.cl@k  , 1), main  = "titolo")
# 1. Different starting seeds will number ~ equal clusters differently. The numbering problem.
# 2. Different starting seeds will result in quite different clusters. The stability problem.
# 3. There is no automatic way to pick optimum k. The “best” k problem.
source("Data/ds4ci_code.R")
#####################################################
##          NUMBERING PROBLEM SOLVED
#####################################################
#the function plot_clusters in ds4ci_code.R solvethe cluster's numbering problem
par(mfrow=c(1,3)) 
#plot(vol.cl, data = vol.mat, project = vol.pca, main = main_txt)
plot_clusters(vol.mat,num_clusters,fc_seed)
plot_clusters(vol.mat,num_clusters,fc_seed+1,FALSE) #not ordered
plot_clusters(vol.mat,num_clusters,fc_seed+1,TRUE)  #ordered
par(mfrow=c(1,1))
vol.cl <- plot_clusters(vol.mat,num_clusters,100,TRUE)
barchart(vol.cl,strip.prefix = "#", shade = TRUE,layout= c(vol.cl@k  , 1), main           = "titolo")
vol.cl <- plot_clusters(vol.mat,num_clusters,300,TRUE)
barchart(vol.cl,strip.prefix = "#", shade = TRUE,layout= c(vol.cl@k  , 1), main           = "titolo")
vol.cl <- plot_clusters(vol.mat,num_clusters,500,TRUE)
#the function plot_clusters in ds4ci_code.R solvethe cluster's numbering problem
par(mfrow=c(1,3)) 
#plot(vol.cl, data = vol.mat, project = vol.pca, main = main_txt)
plot_clusters(vol.mat,num_clusters,fc_seed)
plot_clusters(vol.mat,num_clusters,fc_seed+1,FALSE) #not ordered
plot_clusters(vol.mat,num_clusters,fc_seed+1,TRUE)  #ordered
par(mfrow=c(1,1))
vol.cl <- plot_clusters(vol.mat,num_clusters,100,TRUE)
barchart(vol.cl,strip.prefix = "#", shade = TRUE,layout= c(vol.cl@k  , 1), main           = "titolo")
vol.cl <- plot_clusters(vol.mat,num_clusters,300,TRUE)
barchart(vol.cl,strip.prefix = "#", shade = TRUE,layout= c(vol.cl@k  , 1), main           = "titolo")
vol.cl <- plot_clusters(vol.mat,num_clusters,500,TRUE)
barchart(vol.cl,strip.prefix = "#", shade = TRUE,layout= c(vol.cl@k  , 1), main           = "titolo")
#####################################################
##          STABIITY PROBLEM SOLVED
#####################################################
# fc_clust is a user function you can find in Data/ds4ci_code.R
rclust<-fc_rclust(vol.mat,                    num_clusters,                   fc_cont,                   nrep= 100,                   fc_family,                    verbose =  FALSE,                    FUN=   kcca,                    seed= 1234,                    plotme= TRUE)
par(mfrow=c(1,1))
vol.cl <- plot_clusters(vol.mat,num_clusters,rclust$best[1,4][[1]],TRUE)
barchart(vol.cl,          strip.prefix = "#",           shade = TRUE,          layout= c(vol.cl@k  , 1),          main = "...")
#####################################################
##          BEST K PROBLEM SOLVED
#####################################################
# it takes a while...
for (i in 3:10 ) {   rclust<-fc_rclust(vol.mat,                      i,                     fc_cont,                     nrep= 100,                     fc_family,                      verbose =  FALSE,                      FUN=   kcca,                      seed= 1234,                      plotme= TRUE) }
# k = 8 seems the best
rclust<-fc_rclust(vol.mat,                    8,                   fc_cont,                   nrep= 100,                   fc_family,                    verbose =  FALSE,                    FUN=   kcca,                    seed= 1234,                    plotme= TRUE)
par(mfrow=c(1,1))
vol.cl <- plot_clusters(vol.mat,9,rclust$best[1,4][[1]],TRUE)
barchart(vol.cl,strip.prefix = "#", shade = TRUE,layout= c(vol.cl@k  , 1), main           = "titolo")
# Cercate di costruire una storia sui cluster emersi
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data")
library(psych)
library(dplyr)
library(ggmap)
install.packages("ggmap", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
library("ggmap", lib.loc="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
library(ggmap)
multi.fun <- function(x) {   cbind(freq = table(x),          percentage = prop.table(table(x)))   }
#read fare_data_4
data_fares <- read.csv("trip_fare_4.csv",                        sep = ',',                         header = 1,                         nrows = 10000)
head(data_fares)
#remove columns I'n not going to use in the following
data_fares$medallion<-NULL
data_fares$vendor_id<-NULL
summary(data_fares)
#read trip_data_4
data_trip<-read.csv("trip_data_4.csv",sep=',', header=1, nrows = 30000)
head(data_trip)
#remove columns I'n not going to use in the following
data_trip$medallion<-NULL
data_trip$vendor_id<-NULL
data_trip$store_and_fwd_flag<-NULL
data_trip$rate_code<-NULL
summary(data_trip)
#CLEAN DATA
#exclude trip with time less than 60 seconds
data_trip<-data_trip[(data_trip$trip_time_in_secs)>60,]
#exclude trip with distance less than 0.3 miles
data_trip<-data_trip[(data_trip$trip_distance)>0.3,]
#work on a selection of the NYC area if the following command is uncommented
data_trip<-data_trip[(data_trip$pickup_latitude>(40.62)& data_trip$pickup_latitude<40.9 &                         data_trip$pickup_longitude>(-74.1) & data_trip$pickup_longitude<(-73.75) &                         data_trip$dropoff_latitude>(40.62)& data_trip$dropoff_latitude<40.9 &                         data_trip$dropoff_longitude>(-74.1) & data_trip$dropoff_longitude<(-73.75)),]
#create a column for pickup_hour
data_trip$pickup_hour<-as.POSIXlt(data_trip$pickup_datetime)$hour
#create a column for dropoff_hour
data_trip$dropoff_hour<-as.POSIXlt(data_trip$dropoff_datetime)$hour
#create a column for counting
data_trip$ones<-1
summary(data_trip)
###############################################################
#a.     What is the distribution of number of passengers per trip?
###############################################################
summary(data_trip$passenger_count)
hist(data_trip$passenger_count)
hist(data_trip$passenger_count,      10,      main="Distribution of Number of Passengers per Trip",      xlab="Number of Passengers p/Trip")
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
hist(data_trip$passenger_count,10, add = TRUE,col="yellow")
#other statistics on customer habits
summary(data_trip$trip_time_in_secs/60)
hist(data_trip$trip_time_in_secs/60,10,xlim=c(0,100),      main="Distribution of Trip Time",      xlab="Trip Time in minutes")
#rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
#hist(data_trip$trip_time_in_secs/60,10, add = TRUE,col="yellow")
summary(data_trip$trip_distance)
hist(data_trip$trip_distance,10,xlim=c(0,40),      main="Distribution of Trip Distance",      xlab="Trip Distance")
#rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
#hist(data_trip$trip_distance,10, add = TRUE,col="yellow")
###############################################################
#b.     What is the distribution of payment_type?
###############################################################
summary(data_fares$payment_type)
mop<-multi.fun(data_fares$payment_type)
mop
barplot(sort(mop[,2], decreasing = TRUE),ylim=c(0,0.8), xaxt = 'n')
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
barplot(sort(mop[,2], decreasing = TRUE),add = TRUE,col="yellow",         main="Distribution of Payement Type",         ylab="Frequency")
###############################################################
#c.     What is the distribution of fare amount?
###############################################################
summary(data_fares$fare_amount)
#histogram on full domain
hist(data_fares$fare_amount,      main="Distribution of Fare Amount",      xlab="Fare Amount")
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
hist(data_fares$fare_amount,add = TRUE,col="yellow")
#histogram on restricted domain
hist(data_fares$fare_amount,xlim=c(0,80),200,      main="Distribution of Fare Amount",      xlab="Fare Amount")
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
hist(data_fares$fare_amount,200, xlim=c(0,80),add = TRUE,col="yellow")
# hist(log(data_fares$fare_amount))
# rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
# hist(log(data_fares$fare_amount),add = TRUE,col="yellow",
#      main="Distribution of Fare Amount",
#      xlab="Fare Amount")
###############################################################
#d.     What is the distribution of tip amount?
###############################################################
summary(data_fares$tip_amount)
#histogram on full domain
hist(data_fares$tip_amount,      main="Distribution of Tip Amount",      xlab="Tip Amount")
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
hist(data_fares$tip_amount,add = TRUE,col="yellow")
#histogram on restricted domain
hist(data_fares$tip_amount,200,xlim=c(0,20),      main="Distribution of Tip Amount",      xlab="Tip Amount")
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
hist(data_fares$tip_amount,200,xlim=c(0,20),add = TRUE,col="yellow")
#histogram on full domain
hist(data_fares$total_amount,main="Distribution of Total Amount",xlab="Total Amount")
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
hist(data_fares$total_amount,add = TRUE,col="yellow")
###############################################################
#e.     What is the distribution of total amount?
###############################################################
summary(data_fares$total_amount)
#histogram on full domain
hist(data_fares$total_amount,main="Distribution of Total Amount",xlab="Total Amount")
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
hist(data_fares$total_amount,add = TRUE,col="yellow")
#histogram on restricted domain
hist(data_fares$total_amount,200,xlim=c(0,100),      main="Distribution of Total Amount",      xlab="Total Amount")
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey")
hist(data_fares$total_amount,add = TRUE,col="yellow",200,xlim=c(0,100))
#other statistics for the dashboard
summary(data_fares$fares_amount)
summary(data_fares$total_amount-data_fares$fare_amount-data_fares$tip_amount)
###############################################################
#f.    What are top 5 busiest hours of the day?
###############################################################
#I assume "busiest hours" means "hours showing the highest number of pickup requests"
#create a column for pickup_hour
data_trip$pickup_hour<-as.POSIXlt(data_trip$pickup_datetime)$hour
#create a column for counting
data_trip$ones<-1
#count number of pickups grouped by pickup_hour 
busy_hours <- aggregate(data_trip$ones ~ data_trip$pickup_hour, data_trip, sum)
busy_hours
names(busy_hours)[names(busy_hours)=="data_trip$pickup_hour"] <- "pickup_hour"
names(busy_hours)[names(busy_hours)=="data_trip$ones"] <- "counter"
tripsum <- sum(busy_hours$counter)
busy_hours$perc<-busy_hours$counter/tripsum
ggplot(busy_hours,aes(x = pickup_hour,y = perc*100))+   geom_ribbon(aes(ymin=0, ymax=perc*100),                fill="lightgoldenrod2",                color="lightgoldenrod2") +    scale_x_continuous(breaks = seq(from = 0, to = 23, by = 1)) +   geom_point(size=3,              color="burlywood3") +   geom_line(color="burlywood3",              lwd=0.5) +   ggtitle("Number of Pickups per Hour every 100 Daily Pickups") +   xlab("Hour of the Day") +   theme(axis.title.y = element_blank(),         panel.grid.major = element_blank(),          panel.grid.minor = element_blank(),         text=element_text(size = 22))
#select top 5 pickup_hours
top5_hours<-busy_hours %>%  arrange(desc(busy_hours[,2])) %>% top_n(5)
top5_hours
###############################################################
#g.    What are the top 10 busiest locations of the city?
###############################################################
#I assume "busiest locations" means "locations showing the highest number of pickup requests". 
#In the following I will get rid of records not having latitude and/or longitude reported for pickups.
#Locations are grouped accordingly to their latitude and longitude. In order to find a balance 
#between being too much approximative (ie obtaining areas too large to be considered informative 
#locations) and too much precise (ie having a huge number of distinct locations) I have rounded 
#latitude and longitude to the 0.005 closest decimals.
data_trip<-data_trip[!(data_trip$pickup_latitude==0 | data_trip$pickup_longitude==0),]
data_trip$latpickup<-round(data_trip$pickup_latitude/0.005)*0.005
data_trip$slatpickup<-lapply(data_trip$latpickup,toString)
data_trip$latpickup<-NULL
data_trip$lonpickup<-round(data_trip$pickup_longitude/0.005)*0.005
data_trip$slonpickup<-lapply(data_trip$lonpickup,toString)
data_trip$lonpickup<-NULL
#build a trip identifier concatenating rounded latitude and longitude in string format
data_trip$trip_start<-paste(data_trip$slatpickup,data_trip$slonpickup,sep="|")
#remove redundant data
data_trip$slatpickup<-NULL
data_trip$slonpickup<-NULL
#groupby trip identifier and count
busy_locations<-aggregate(data_trip$ones ~ data_trip$trip_start, data_trip, sum)
names(busy_locations)[names(busy_locations)=="data_trip$trip_start"] <- "location"
names(busy_locations)[names(busy_locations)=="data_trip$ones"] <- "counter"
#total number of trip
tripsum<-sum(busy_locations$counter)
busy_locations$perc<-busy_locations$counter/tripsum
top10_loc<-busy_locations%>%  arrange(desc(busy_locations[,2])) %>% top_n(10)
#print top 10 busiest location identifier and trip number per location
top10_loc
#get address of busy locations
c <- unlist(strsplit(top10_loc$location, "[|]"))
temp = matrix(as.double(c), nrow=10, ncol=2,byrow=TRUE) 
top10_loc$lat <- temp[,1]
top10_loc$lon <- temp[,2]
top10_loc$address <- mapply(FUN = function(lon, lat) revgeocode(c(lon, lat)), top10_loc$lon, top10_loc$lat)
top10_loc
#build map for busy locations
ny_map<-get_map(location=c(-73.9308,40.7336),maptype = "satellite", zoom=11)
ny_map2<-get_map(location=c(-73.9874,40.7539),maptype = "satellite", zoom=13)
ny_map3<-get_map(location=c(-73.99,40.75),maptype = "roadmap", zoom=13)
#it takes a lot of time, use a sample instead
#ggmap(ny_map, extent = "device") + geom_point(aes(x = data_trip$pickup_longitude, y = data_trip$pickup_latitude), colour = "yellow", alpha = 0.1, size = 1, data = data_trip)
data_sample<-data_trip[sample(nrow(data_trip), 10000), ]
ggmap(ny_map, extent = "device") +    geom_point(aes(x = data_sample$pickup_longitude,                   y = data_sample$pickup_latitude),               colour = "yellow",               alpha = 0.1,               size = 1,               data = data_sample)
ggmap(ny_map, extent = "device") +    geom_density2d(data = data_sample,                   aes(x = data_sample$pickup_longitude,                       y = data_sample$pickup_latitude),                   size = 0.3) +    stat_density2d(data = data_sample,                   aes(x = data_sample$pickup_longitude,                      y = data_sample$pickup_latitude,                       fill = ..level..,                       alpha = ..level..),                   size = 0.01,                   geom = "polygon") +    scale_fill_gradient(low = "yellow", high = "red") +    scale_alpha(range = c(0.2, 0.8), guide = FALSE)
ggmap(ny_map2, extent = "device") +    geom_density2d(data = data_sample,                   aes(x = data_sample$pickup_longitude,                      y = data_sample$pickup_latitude),                   size = 0.3) +    stat_density2d(data = data_sample,                   aes(x = data_sample$pickup_longitude,                       y = data_sample$pickup_latitude,                       fill = ..level..,                      alpha = ..level..),                   size = 0.01,                   geom = "polygon") +    scale_fill_gradient(low = "yellow", high = "red") +    scale_alpha(range = c(0.4, 0.9), guide = FALSE)
ggmap(ny_map3, extent = "device") +    geom_density2d(data = data_sample, aes(x = data_sample$pickup_longitude,                                         y = data_sample$pickup_latitude), size = 0.3) +    stat_density2d(data = data_sample, aes(x = data_sample$pickup_longitude,                                         y = data_sample$pickup_latitud, fill = ..level..,                                         alpha = ..level..), size = 0.01, geom = "polygon") +    scale_fill_gradient(low = "yellow", high = "red") + scale_alpha(range = c(0.4, 0.9), guide = FALSE)
######################################################################
#h.     Which trip has the highest standard deviation of travel time?
######################################################################
#I assume "trip" means "a taxi run with a given trip_start and trip_end". "trip_start" has been defined 
#above. "trip_end" is defined below. Their concatenation defines the key for "trip".
#In the following I will get rid of records not having latitude and/or longitude reported for dropoffs
data_trip<-data_trip[!(data_trip$dropoff_latitude==0 | data_trip$dropoff_longitude==0),]
data_trip$latdropoff<-round(data_trip$dropoff_latitude/0.005)*0.005
data_trip$slatdropoff<-lapply(data_trip$latdropoff,toString)
data_trip$latdropoff<-NULL
data_trip$londropoff<-round(data_trip$dropoff_longitude/0.005)*0.005
data_trip$slondropoff<-lapply(data_trip$londropoff,toString)
data_trip$londropoff<-NULL
data_trip$trip_end<-paste(data_trip$slatdropoff,data_trip$slondropoff,sep="|")
data_trip$slatdropoff<-NULL
data_trip$slondropoff<-NULL
#trip_id variable
data_trip$trip_id<-paste(data_trip$trip_start,data_trip$trip_end,sep="|")
#compute standard deviation for every trip
trips<-aggregate(data_trip$trip_time_in_secs ~ data_trip$trip_id, data_trip, sd)
#get the trip with highest standard deviation and find pickup and dropoff locations
temp<-trips %>%  arrange(desc(trips[,2])) %>% top_n(10)
names(temp)[names(temp)=="data_trip$trip_id"] <- "trip_id"
names(temp)[names(temp)=="data_trip$trip_time_in_secs"] <- "trip_sd"
#print the top 10, I will use the first one for the dashboard
trip_text=list()
for(i in 1:10) {   coords=matrix(as.double(unlist(strsplit(temp$trip_id[i], "[|]"))), nrow=2, ncol=2,byrow=TRUE)   from=coords[1,]   to=coords[2,]   origin<-mapply(FUN = function(lon, lat) revgeocode(c(lon, lat)), from[2], from[1])   destination<-mapply(FUN = function(lon, lat) revgeocode(c(lon, lat)), to[2], to[1])   trip_text[i]=paste("Trip",i,"from",origin,"to",destination,"has",temp$trip_sd[i]) }
print(trip_text)
######################################################################
#i.	    Which trip has most consistent fares? 
######################################################################
#I assume each taxy run is uniquely identified by "hack licence" and "pickup time". So, I can
# build unique run_id's for data_fares and data_trip tables and join them
data_fares$run_id<-paste(data_fares$hack_license,data_fares$pickup_datetime,sep="|")
data_trip$run_id<-paste(data_trip$hack_license,data_trip$pickup_datetime,sep="|")
#I create a new dataframe merging data_fares and data_trip on run_id
df_merge=merge(x=data_trip,y=data_fares, by.x="run_id", by.y="run_id", all.x=TRUE)
#get rid of some column to free memory
df_merge$pickup_datetime.x<-NULL
df_merge$dropoff_datetime<-NULL
df_merge$hack_license.y<-NULL
df_merge$pickup_datetime.y<-NULL
#groupby and standard deviation computation for fare ampount
fares<-aggregate(df_merge$fare_amount ~ df_merge$trip_id, df_merge, sd)
#I want to keep track of numerosity for each trip
fares_c<-aggregate(df_merge$ones ~ df_merge$trip_id, df_merge, sum)
fares_merge=merge(x=fares,y=fares_c, by.x="df_merge$trip_id", by.y="df_merge$trip_id", all.x=TRUE)
names(fares_merge)[names(fares_merge)=="df_merge$trip_id"] <- "trip_id"
names(fares_merge)[names(fares_merge)=="df_merge$fare_amount"] <- "fare_sd"
names(fares_merge)[names(fares_merge)=="df_merge$ones"] <- "trip_count"
#exclude trip with less then 30 occurrencies
fares_merge<-fares_merge[(fares_merge$trip_count>1),]
#ordering ascending by fares sd
fares_merge<- fares_merge %>%  arrange((fares_merge$fare_sd))
#trying to get some extrainformation beyond numbers
trip_text=list()
for(i in 1:5) {     coords=matrix(as.double(unlist(strsplit(fares_merge$trip_id[i], "[|]"))), nrow=2, ncol=2,byrow=TRUE)     from=coords[1,]     to=coords[2,]     origin<-mapply(FUN = function(lon, lat) revgeocode(c(lon, lat)), from[2], from[1])     destination<-mapply(FUN = function(lon, lat) revgeocode(c(lon, lat)), to[2], to[1])     trip_text[i]=paste("Trip",i,"starts from",origin,"and end to to",destination) }
print(trip_text)
#Trip from JFK airport seem to be those with most consisten fares
library(httr)
WScall <- function(endpoint, parameters) {     result <- GET(endpoint, query = parameters)     return(result) }
ad <- "1600 Pennsylvania Avenue, Washington, DC"
urla <- "http://nominatim.openstreetmap.org/search"
paramA <- list(q = ad, addressdetails = 1, format = "json")
resA <- WScall(urla, paramA)
if (resA$status_code == 200) {  adrjson <- content(resA, as = "parsed") lat = adrjson[[1]]$lat lon = adrjson[[1]]$lon }
urlw <- "http://forecast.weather.gov/MapClick.php"
paramW <- list(lat = lat, lon = lon, FcstType = "json")
resW <- WScall(urlw, paramW)
if (resW$status_code == 200) { weajson <- content(resW, as = "parsed") weajson$currentobservation }
# Parte 2 - giocare con i dati
number = c(1, 2, 3)
animal = c('cat', 'dog', 'mouse')
df1 = data.frame(number, animal)
df1$animal
str(df1)
df2 = t(df1)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
adrjson
library(httr)
WScall <- function(endpoint, parameters) {     result <- GET(endpoint, query = parameters)     return(result) }
ad <- "1600 Pennsylvania Avenue, Washington, DC"
urla <- "http://nominatim.openstreetmap.org/search"
paramA <- list(q = ad, addressdetails = 1, format = "json")
resA <- WScall(urla, paramA)
resA$url
if (resA$status_code == 200) {  adrjson <- content(resA, as = "parsed") lat = adrjson[[1]]$lat lon = adrjson[[1]]$lon }
urlw <- "http://forecast.weather.gov/MapClick.php"
paramW <- list(lat = lat, lon = lon, FcstType = "json")
resW <- WScall(urlw, paramW)
resW$url
number = c(1, 2, 3)
animal = c('cat', 'dog', 'mouse')
df1 = data.frame(number, animal)
df1
df1$animal
str(df1)
df2 = t(df1)
df2
df2 = data.frame(c('cat',1), c('dog',2), c('mouse',3))
df2
df1
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", names = names)
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", names = names)
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names)
head(df)
df$GEOID = df$Census_Tract_Number * 100 + 10 ** 6 * df$County_Code + 10 ** 9 * df$State_Code
df$GEOID = with(df, Census_Tract_Number * 100 + 10 ** 6 * County_Code + 10 ** 9 * State_Code)
head(df)
df$GEOID = with(df, ((Census_Tract_Number * 100) + (10 ^ 6 * County_Code) + (10 ^ 9 * State_Code)) head(df)
head(df)
df$GEOID = with(df, ((Census_Tract_Number * 100) + (10 ^ 6 * County_Code) + (10 ^ 9 * State_Code)) )
na.omit(df)
head(df)
df$GEOID = with(df, ((Census_Tract_Number * 100) + (10 ^ 6 * County_Code) + (10 ^ 9 * State_Code)) )
na.omit(df)
df = na.omit(df)
head(df)
df$GEOID = with(df, ((Census_Tract_Number * 100) + (10 ^ 6 * County_Code) + (10 ^ 9 * State_Code)) )
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(df)
df$GEOID = with(df, ((as.number(Census_Tract_Number) * 100) + (10 ^ 6 * as.number(County_Code)) + (10 ^ 9 * as.number(State_Code))) )
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
head(df)
df$GEOID = NULL # ma perche' dovrei droppare una colonna?
head(df)
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
head(df)
df = df(-c(2,4)) # droppo righe 2 e 4
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(df)
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
head(df)
df$GEOID = NULL # ma perche' dovrei droppare una colonna?
df = df(-c(2,4)) # droppo righe 2 e 4
df = df[-c(2,4)] # droppo righe 2 e 4
head(df)
df = df[-c(2,4), ] # droppo righe 2 e 4
head(df)
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(df)
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
head(df)
df$GEOID = NULL # ma perche' dovrei droppare una colonna?
df = df[-c(2,4), ] # droppo righe 2 e 4
head(df)
rownames(df) <- df$State_Code #assegna indici al dataframe, multiindex con workaround non raccomandabile
row_names(df) <- df$State_Code #assegna indici al dataframe, multiindex con workaround non raccomandabile
rownames(df) <- df$State_Code #assegna indici al dataframe, multiindex con workaround non raccomandabile
head(df)
library(data.table)
install.packages('data.table')
rownames(df) <- df$State_Code #assegna indici al dataframe, multiindex con workaround non raccomandabile
library(data.table)
library(dplyr)
df <- add_rownames(df, df$State_Code)
df <- rownames_to_column(df, df$State_Code)
library(dplyr)
df <- rownames_to_column(df, df$State_Code)
df <- tibble::rownames_to_column(df, df$State_Code)
head(df)
rownames(df) = df$State_Code
rownames(df) = df[,1]
row.names(df) = df[,1]
head(df)
row.names(df) <- df[, 1]
rownames(df) <- df[, 1]
names(df) <- df[, 1]
head(df)
 df
## Loading data (and basic statistics / visualization)
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(df)
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
head(df)
df$GEOID = NULL # ma perche' dovrei droppare una colonna?
df = df[-c(2, 4),] # droppo righe 2 e 4
library(dplyr)
names(df) <- df[, 1]
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(df)
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
head(df)
df$GEOID = NULL # ma perche' dovrei droppare una colonna?
df = df[-c(2, 4),] # droppo righe 2 e 4
library(dplyr)
install.packages('tibble')
column_to_rownames(df, var = df$State_Code)
library(tibble)
column_to_rownames(df, var = df$State_Code)
install.packages('tibble')
column_to_rownames(df, var = df$State_Code)
column_to_rownames(df, var = df$State_Code)
has_rownames(df)
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(df)
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
install.packages('readr')
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(df)
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
head(df)
df$GEOID = NULL # ma perche' dovrei droppare una colonna?
df = df[-c(2, 4),] # droppo righe 2 e 4
library(tibble)
has_rownames(df)
column_to_rownames(df, var = df$State_Code)
column_to_rownames(df, var = "indice")
column_to_rownames(df, var = "State_Code")
column_to_rownames(df, var = State_Code)
column_to_rownames(df, df$State_Code)
has_rownames(df)
remove_rownames(df)
column_to_rownames(as.data.frame(df))
column_to_rownames(as.data.frame(df$State_Code))
column_to_rownames(df, df$State_Code)
df = as.data.frame(na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names)))
head(df)
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
head(df)
df$GEOID = NULL # ma perche' dovrei droppare una colonna?
df = df[-c(2, 4),] # droppo righe 2 e 4
library(tibble)
has_rownames(df)
column_to_rownames(df, df$State_Code)
remove_rownames(df)
column_to_rownames(df, df$State_Code)
names(df) <- df[, 1]
row.names(df) <- df$State_Code
str(df)
hist(df$PCT_AMT_FHA, bins=50, alpha=0.5)
str(df)
library(psych)
describeBy(df, df$PCT_AMT_FHA)
hist(df$PCT_AMT_FHA, breaks = 50, col = rgb(1, 0, 0, 0.5))
df$LOG_AMT_ALL = log1p(df$AMT_ALL)
head(df)
number = c(1, 2, 3)
animal = c('cat', 'dog', 'mouse')
df1 = data.frame(number, animal)
df1
df1$animal
## Verbs (operations) in Pandas
## Loading data (and basic statistics / visualization)
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = as.data.frame(na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names)))
head(df)
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
head(df)
df$GEOID = NULL # ma perche' dovrei droppare una colonna?
df = df[-c(2, 4),] # droppo righe 2 e 4
library(data.table)
dt <- as.data.table(df)
setkey(dt, State_Code, County_Code)
head(dt)
library(psych)
describeBy(df, df$PCT_AMT_FHA)
hist(df$PCT_AMT_FHA, breaks = 50, col = rgb(1, 0, 0, 0.5))
head(df$State_Code)
head(c('State_Code', 'County_Code'))
str(df$State_Code)
df[, 4]
df[3, 'State_Code']
df[0:3, "State_Code":'Census_Tract_Number']
df[3, 0]
df[0:3, 0:3]
head(subset(df, State_Code == 1))
head(subset(df, State_Code == 1 | Census_Tract_Number == 9613))
head(subset(df, State_Code == 01))
head(subset(df, State_Code == 33))
head(subset(df, State_Code == 33 | Census_Tract_Number == 9613))
head(df[df$State_Code == 5])
head(df[df$State_Code == 33])
head(df$State_Code == 33)
df[df$State_Code == 33]
df[State_Code == 33]
df2 = read_csv('~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.csv', sep = '\t')
head(df2)
df2 = as.data.frame(na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv")))
rm(df2, envir = as.environment(".GlobalEnv"))
df2 = as.data.frame(na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", headers=T)))
df2 = as.data.frame(na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", header=T)))
df2 = as.data.frame(na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names=T)))
df2 = as.data.frame(na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names=T, sep="\t")))
df2 = as.data.frame(na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names = T, sep = '\t')))
df2 = as.data.frame(na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names = T, delim = '\t')))
df2 = as.data.frame(na.omit(read_tsv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names = T, delim = '\t')))
df2 = as.data.frame(na.omit(read_tsv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names = T)))
head(df2)
df_joined = merge(df1, df2, by = 'GEOID')
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
head(df)
df_joined = merge(df1, df2, by = 'GEOID')
install.packages('htmltab')
library(htmltab)
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
head(tallest)
Sys.setlocale("LC_TIME", "C")
as.POSIXlt("July 4, 2016", format = "%B %d, %Y")
as.POSIXlt('Monday, July 4, 2016', format = "%A, %B %d, %Y")
as.POSIXlt('Tuesday, July 4th, 2016', format = "%A, %B %dth, %Y")
as.POSIXlt('Monday, July 4th, 2016 05:00 PM', format = "%A, %B %dth, %Y %I:%M %p")
as.POSIXlt('04/07/2016T17:20:13.123456', format = "%d/%m/%YT%H:%M:%OS")
as.Date(as.POSIXlt(1467651600000000000 / 1000000000, origin = "1970-01-01")) ** *
as.Date(as.POSIXlt(1467651600000000000 / 1000000000, origin = "1970-01-01"))
substring(tallest,c('╗┐ / ´╗┐','┬░N'))
substring(tallest,'╗┐ / ´╗┐','┬░N')
right = function(string, char) {     substr(string, nchar(string) - (char - 1), nchar(string)) }
left = function(string, char) {     substr(string, 1, char) }
substring(tallest,'╗┐ / ´╗┐','┬░N')
right(tallest$Coordinates,'/')
length(tallest$c)
length(tallest$Co)
length(tallest$Coordinates[1])
length(tallest$Coordinates[2])
length(tallest$Coordinates[2])parts = strsplit(string, "/")
parts = strsplit(tallest$Coordinates, "/")
parts
parts = strsplit(tallest$Coordinates, "╗┐ / ´╗┐")
parts
str_extract_all(tallest$Coordinates, "\\d+")
library(stringi)
install.packages('stringi')
library(stringi)
str_extract_all(tallest$Coordinates, "\\d+")
str_extract(tallest$Coordinates, "\\d+")
library(stringi)
str_extract(tallest$Coordinates, "\\d+")
str_extract_all(tallest$Coordinates, "\\d+")
stri_extract_all(test, regex = "\\d+")[[1]]
stri_extract_all(test, regex = "\\d+")
stri_extract_all(tallest$Coordinates, regex = "\\d+")
stri_extract_all(tallest$Coordinates, regex = "*\\d+*")
stri_extract_all(tallest$Coordinates, regex = "\\d+*")
stri_extract_all(tallest$Coordinates, regex = "\\d+")
stri_extract_all(tallest$Coordinates, regex = "(\-?\d+(\.\d+)?)")
stri_extract_all(tallest$Coordinates, regex = "\\d+(\.\d+")
stri_extract_all(tallest$Coordinates, regex = "(\-?\d+(\.\d+)?)")
stri_extract_all(tallest$Coordinates, regex = "\\d+(\.\d+)")
stri_extract_all(tallest$Coordinates, regex = "\\d+(.d+)")
stri_extract_all(tallest$Coordinates, regex = "\\d+.d+)")
stri_extract_all(tallest$Coordinates, regex = "\\d+\.\d+)")
stri_extract_all(tallest$Coordinates, regex = "\\d+(\.\d+)")
stri_extract_all(tallest$Coordinates, regex = "\\d+\.\d+")
stri_extract_all(tallest$Coordinates, perl("-?\\d+\\.?\\d*"))
stri_extract_all(tallest$Coordinates, regexec("-?\\d+\\.?\\d*"))
stri_extract_all(tallest$Coordinates, regex("-?\\d+\\.?\\d*"))
stri_extract_all(test, regex = "\\d+")
stri_extract_all(test, regex = "-?\\d+\\.?\\d*")
stri_extract_all(test, tallest$Coo = "-?\\d+\\.?\\d*")
stri_extract_all(test, tallest$Coordinates = "-?\\d+\\.?\\d*")
stri_extract_all(tallest$Coordinates, regex ="-?\\d+\\.?\\d*")
library(htmltab)
library(stringi)
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
tallest <- htmltab(doc = url, which = 3)
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
tallest$Coordinates = str_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*")
tallest$Coordinates = str_extract_all(tallest$Coordinates, '\\w{3,}')[[1]], collapse = ' ')
library(htmltab)
library(stringi)
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
tallest$Coordinates = str_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*")
tallest$Coordinates = str_extract_all(tallest$Coordinates, '\\w{3,}')[[1]], collapse = ' ')
tallest$Coordinates = stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*")
tallest$Coordinates = stri_extract_all(tallest$Coordinates, '\\w{3,}')[[1]], collapse = ' ')
tallest$Coordinates = stri_extract_all(tallest$Coordinates, '\\w{3,}')
tallest$Coordinates = stri_extract_all(tallest$Coordinates, '.{3,}')
tallest$Coordinates = stri_extract_all(tallest$Coordinates, regex =  '.{3,}')
head(tallest)
tallest <- htmltab(doc = url, which = 3)
head(tallest)
tallest$Coordinates = stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*")
head(tallest)
tallest$Coordinates = stri_extract_all(tallest$Coordinates, regex =  '.{3,}')
head(tallest)
tallest$lat = tail(tallest$Coordinates, n = 2)
tallest <- htmltab(doc = url, which = 3)
tallest$Coordinates = stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*")
tallest <- htmltab(doc = url, which = 3)
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*")
tl = stri_extract_all(tallest$Coordinates, regex = '.{3,}')
tl
tallest <- htmltab(doc = url, which = 3)
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*")
tl = stri_extract_all(tallest$Coordinates, regex = '.{3,}')
tl
tl = stri_extract_all(tl, regex = "-?\\d+\\.?\\d*")
tl
library(htmltab)
library(stringi)
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
tl = stri_extract_all(tallest$Coordinates, regex = '.{3,}')
tl = stri_extract_all(tl, regex = "-?\\d+\\.?\\d*")
tl
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*")
tl = stri_extract_all(tallest$Coordinates, regex = '.{3,}')
tl
stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*")
stri_extract_all(stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*"), regex = '.{3,}')
tl = stri_extract_all(stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d*"), regex = '.{3,}')
tail(tl, n = 2)
stri_extract_all(tallest$Coordinates, regex = "-?\\d{2}+\\.?\\d{6}*")
tl = stri_extract_all(stri_extract_all(tallest$Coordinates, regex = "-?\\d{2}+\\.?\\d{6}"), regex = '.{3,}')
tl
stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d{6}")
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d+\\.?\\d{6}")
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
head(tallest)
tl = stri_extract_all(string, regex = "-?\\d{1,3+\\.?\\d{4,6}")
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3+\\.?\\d{4,6}")
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")
tl
as.data.frame(tl)
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")
tl
stri_extract_all_boundaries
stri_extract_all_boundaries(tl)
stri_extract_first_words(tl)
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")
stri_extract_first_words(tl)
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")
as.data.frame(tl)
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")
tallest$lon = tl[[2]]
str(tl)
unlist(tl)
tl = unlist(tl)
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")
tl = data.frame(matrix(unlist(tl)))
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")
tl = data.frame(tl)
tallest$lon = tl[,4]
tallest$lon = tl[,4]
tallest$lon = tl$4
tl
tl = data.frame(stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}"))
tallest$lon = tl[[4]]
tl[4]
tl = stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")
as.data.t
as.data.table(tl)
tl2= as.data.table(tl)
t(tl2)
tl = t(as.data.table(stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")))
rl
tl
tallest$lon = tl[,4]
tallest$lat = tl[, 3]
head(tallest)
### Pandas HTML data import example
library(htmltab)
library(stringi)
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
tl = t(as.data.table(stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")))
tallest$Longitude = tl[, 4]
tallest$Latitude = tl[, 3]
head(tallest)
library(htmltab)
library(stringi)
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
tl = t(as.data.table(stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")))
require(data.table)
tl = t(as.data.table(stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")))
library(htmltab)
library(stringi)
require(data.table)
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
tl = t(as.data.table(stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")))
library(htmltab)
library(stringi)
require(data.table)
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
tl = t(as.data.table(stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")))
tallest$Latitude = tl[, 3]
tallest$Longitude = tl[, 4]
rm(tl)
head(tallest)
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = as.data.frame(na.omit(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names)))
df = as.data.frame(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
library(readr)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
df = as.data.frame(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(df)
df$GEOID = with(df, ((as.numeric(Census_Tract_Number) * 100) + (10 ^ 6 * as.numeric(County_Code)) + (10 ^ 9 * as.numeric(State_Code))))
df$GEOID = with(df, as.numeric(Census_Tract_Number) * 100 + 10 ^ 6 * as.numeric(County_Code) + 10 ^ 9 * as.numeric(State_Code)  )
head(df)
df[1] = NULL
head(df)
df[,1] = NULL
head(df)
df = as.data.frame(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(df)
df[,1] = NULL
head(df)
df[1,] = NULL
df[-c(1)]
df[-1,]
df = df[-1,]
df = as.data.frame(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(df)
df = df[-1,]
head(df)
str(df)
library(psych)
describeBy(df, df$PCT_AMT_FHA)
describeBy(df)
hist(df$PCT_AMT_FHA, bins = 50, col = rgb(1, 0, 0, 0.5))
df$LOG_AMT_ALL = log1p(df$AMT_ALL)
hist(df$LOG_AMT_ALL, bins = 50, col = rgb(1, 0, 0, 0.5))
hist(df$PCT_AMT_FHA, col = rgb(1, 0, 0, 0.5))
df$LOG_AMT_ALL = log1p(df$AMT_ALL)
hist(df$LOG_AMT_ALL, col = rgb(1, 0, 0, 0.5))
install.packages(readr)
# Indici su data frame
library(data.table)
dt <- as.data.table(df)
setkey(dt, State_Code, County_Code)
head(dt)
data.frame(unclass(summary(dt)))
head(dt$State_Code)
head(dt$State_Code, dt$County_Code))
head(dt$State_Code, dt$County_Code)
head(c(dt$State_Code, dt$County_Code))
head(dt[State_Code, County_Code])
head(dt[,c(State_Code, County_Code)])
str(df$State_Code)
head(dt[, County_Code, keyby = State_Code,])
dt[, County_Code, keyby = State_Code,]
df[, 4]
dt[4 , ]
dt[3, 'State_Code']
dt[56, 'State_Code']
dt[500, 'State_Code']
dt[550, 'State_Code']
dt[12550, 'State_Code']
dt[12550,]
dt[12550, 'State_Code']
dt[0:3, "State_Code":'Census_Tract_Number'] ############## rivedere
head(dt[, County_Code, keyby = State_Code])
dt[12545:12550, County_Code, keyby = State_Code] ############## rivedere
dt[3, 0]
dt[3, 5]
df[3:5, 2:4]
dt[3:5, 2:4]
describeBy(dt, df$PCT_AMT_FHA)
library(psych)
describeBy(dt, df$PCT_AMT_FHA)
describeBy(dt)
hist(dt$PCT_AMT_FHA, col = rgb(1, 0, 0, 0.5))
dt$LOG_AMT_ALL = log1p(df$AMT_ALL)
hist(dt$LOG_AMT_ALL, col = rgb(1, 0, 0, 0.5))
head(subset(dt, State_Code == 33))
head(subset(dt, State_Code == 33 | Census_Tract_Number == 9613))
head(subset(dt, State_Code == 33 & Census_Tract_Number == 9613))
head(subset(dt, State_Code == 33 & Census_Tract_Number == 9656.00))
head(subset(dt, State_Code == 33 & Census_Tract_Number == 9656))
head(subset(dt, State_Code == 33 | Census_Tract_Number == 9656))
head(subset(dt, State_Code == 33 | Census_Tract_Number == 9656.00))
subset(dt, State_Code == 33 | Census_Tract_Number == 9656.00)
head(subset(dt, State_Code == 33 | Census_Tract_Number == 9613))
head(subset(dt, (State_Code == 33) | (Census_Tract_Number == 9613)))
dt[State_Code == 33] ########### rivedere
dt[State_Code == 5]
dt[dt$State_Code == 5]
dt[State_Code == 5]
dt[State_Code == 33]
df2 = read_csv('~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.csv', sep = '\t')
df2 = as.data.table(read_tsv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names = T))
head(df2)
df_joined = merge(df1, df2, by = 'GEOID') ####################rivedere
df_joined = merge(dt, dt2, by = 'GEOID') ####################rivedere
head(df2)
dt_joined = dt[dt2, on = "GEOID"]
dt2 = as.data.table(read_tsv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names = T))
head(dt2)
dt_joined = dt[dt2, on = "GEOID"]
df$GEOID = with(df, as.numeric(Census_Tract_Number) * 100 + 10 ^ 6 * as.numeric(County_Code) + 10 ^ 9 * as.numeric(State_Code)  )
dt <- as.data.table(df)
dt$GEOID = with(dt, as.numeric(Census_Tract_Number) * 100 + 10 ^ 6 * as.numeric(County_Code) + 10 ^ 9 * as.numeric(State_Code)  )
head(dt)
dt_joined = dt[dt2, on = "GEOID"]
head(df_joined)
dt2 = as.data.table(read_tsv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names = T))
head(dt2)
dt_joined = dt[dt2, on = "GEOID"]
head(df_joined)
head(dt)
str(dt)
str(dt2)
dt$GEOID = as.character(with(dt, as.numeric(Census_Tract_Number) * 100 + 10 ^ 6 * as.numeric(County_Code) + 10 ^ 9 * as.numeric(State_Code)  ))
head(df)
head(dt)
head(dt)
str(dt)
dt_joined = dt[dt2, on = "GEOID"]
head(df_joined)
head(dt_joined)
usps_groups = dt_joined[,, by = USPS]
usps_groups
usps_groups = dt_joined[,, keyby = USPS]
usps_groups
usps_groups = dt_joined[, sum(USPS), keyby = USPS]
usps_groups = dt_joined[, factor(USPS), keyby = USPS]
usps_groups
usps_groups = dt_joined[, level(USPS), keyby = USPS]
usps_groups = dt_joined[, , keyby = USPS]
usps_groups
usps_groups = dt_joined[, , by = USPS]
usps_groups
usps_groups = dt_joined[, sum(ALAND), by = USPS]
usps_groups
dtbystate = dt[order(State_Code),]
dtbystate = dt[order(State_Code)]
dtbyAMTFHA = dt[order(AMT_FHA)]
unique(dt)
head(unique(dt))
count(unique(dt))
nrow(unique(dt))
dt[is.na(dt)]
is.na(dt)
head(is.na(dt))
naomit = dt[na.omit[dt]]
naomit = na.omit(dt)
dfna0 = dt[is.na(dt)] <- 0
dt[is.na(dt)] <- 0
dtstring = dt[grep("A", df$PCT_AMT_FHA),]
dtstring = dt[grep("A", dt_joined$USPS),]
install.packages('stringr')
install.packages('dplyr')
library(stringr)
library(dplyr)
dt_joined %>%   filter(str_detect(USPS, "A"))
dtstring = dt_joined %>% filter(str_detect(USPS, "A"))
require(bizdays)
create.calendar("Brazil/ANBIMA", holidaysANBIMA, weekdays = c("saturday", "sunday"))
business_days = bizseq('2016-01-01', '2016-12-31', "Brazil/ANBIMA")
business_days
dtimed = data.table(x = business_days, y = seq(1,length(business_days)) dtimed = data.table(x = business_days, y = seq(1,length(business_days))) dtimed = data.table(x = business_days, y = seq(1,length(business_days)))
library(data.table)
dtimed = data.table(x = business_days, y = seq(1,length(business_days)))
setkey(dtimed, dtimed[])
seq(1, length(business_days))
dtimed = data.table(x = business_days, y = seq(1, length(business_days)), col_names = 'bizdays','numero')
dtimed = data.table(x = business_days, y = seq(1, length(business_days)))
setkey(dtimed, dtimed$x)
setkey(dtimed, x)
str(dtimed)
d <- c("2009-03-07 12:00", "2009-03-08 12:00", "2009-03-28 12:00", "2009-03-29 12:00", "2009-10-24 12:00", "2009-10-25 12:00", "2009-10-31 12:00", "2009-11-01 12:00")
t1 <- as.POSIXct(d, "America/Los_Angeles")
cbind(US = format(t1), UK = format(t1, tz = "Europe/London"))
## Loading data (and basic statistics / visualization)
library(readr)
library(data.table)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
dt = as.data.table(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(dt)
dt$GEOID = as.character(with(dt, as.numeric(Census_Tract_Number) * 100 + 10 ^ 6 * as.numeric(County_Code) + 10 ^ 9 * as.numeric(State_Code)  ))
head(dt)
df$GEOID = NULL # ma perche' dovrei droppare una colonna?
df = dt[-1,]
# Indici su data frame
setkey(dt, State_Code, County_Code)
head(dt)
data.frame(unclass(summary(dt)))
str(df)
library(psych)
describeBy(dt, df$PCT_AMT_FHA)
describeBy(dt)
hist(dt$PCT_AMT_FHA, col = rgb(1, 0, 0, 0.5))
dt$LOG_AMT_ALL = log1p(df$AMT_ALL)
hist(dt$LOG_AMT_ALL, col = rgb(1, 0, 0, 0.5))
## Indexing data frames
head(dt$State_Code)
head(dt[,c(State_Code, County_Code)])
head(dt[, County_Code, keyby = State_Code])
str(df$State_Code)
dt[12550,]
dt[12550, 'State_Code']
dt[12545:12550, County_Code, keyby = State_Code]
dt[3, 5]
dt[3:5, 2:4]
#filtering data
head(subset(dt, State_Code == 33))
head(subset(dt, (State_Code == 33) | (Census_Tract_Number == 9613)))
dt[State_Code == 33]
#joining data
dt2 = as.data.table(read_tsv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names = T))
head(dt2)
dt_joined = dt[dt2, on = "GEOID"]
head(dt_joined)
grouped = dt_joined[, list(sumNUM = sum('NUM_ALL'), sumFHA = sum('NUM_FHA')), by = .('State_Code', 'County_Code')]
 dt_joined[, list(sumNUM = sum('NUM_ALL'), sumFHA = sum('NUM_FHA')), by = .('State_Code', 'County_Code')]
head(unique(dt))
nrow(unique(dt))
install.packages(c(httr, readr, data.table, psych, htmltab, stringi, bizdays, stringr, dyplr, timeDate))
install.packages(c('httr', 'readr', 'data.table', 'psych', 'htmltab', 'stringi', 'bizdays', 'stringr', 'dyplr', 'timeDate'))
grouped = group_by(dt_joined, State_Code, County_Code)
bizdays::offset(july4, 5, cal)
bizdays::offset(july4, -1, cal)
library(bizdays)
create.calendar(name = 'ANBIMA', holidays = holidaysANBIMA, weekdays = c('saturday', 'sunday'))
bizdays.options$set(default.calendar = 'ANBIMA')
cal = bizdays.options$get("default.calendar")
bizdays::offset(july4, 5, cal)
bizdays::offset(july4, -1, cal)
bizdays::offset(last_day(ymd(20160704)), 0, cal) # last business day of the month.
install.packages('lubridate')
library(lubridate)
bizdays::offset(last_day(ymd(20160704)), 0, cal) # last business day of the month.
last_day <- function(date) {     ceiling_date(date, "month") - days(1) }
last_day(ymd(20160704))
bizdays::offset(last_day(ymd(20160704)), 0, cal) # last business day of the month.
library(zoo)
grouped = group_by(dt_joined, State_Code, County_Code)
library(dplyr)
grouped = group_by(dt_joined, State_Code, County_Code)
grouped1 <- summarise(grouped, NUM_ALL = sum(NUM_ALL), NUM_FHA = sum(NUM_FHA))
head(grouped1)
dff <- as.data.frame(matrix(1:24, ncol = 6, nrow = 4, byrow = TRUE))
head(sin(dff))
dff
apply(dff, 1:2, function(x) sprintf("%.2f", x))
apply(dff, 2, function(x) max(x) - min(x))
apply(dff, 1, function(x) max(x) - min(x))
# Librerie da installare
install.packages(c('httr', 'readr', 'data.table', 'psych', 'htmltab', 'stringi', 'bizdays', 'stringr', 'dyplr', 'lubridate', 'zoo'))
# Parte 1 - prendere i dati
library(httr)
WScall <- function(endpoint, parameters) {     result <- GET(endpoint, query = parameters)     return(result) }
ad <- "1600 Pennsylvania Avenue, Washington, DC"
urla <- "http://nominatim.openstreetmap.org/search"
paramA <- list(q = ad, addressdetails = 1, format = "json")
resA <- WScall(urla, paramA)
if (resA$status_code == 200) {     adrjson <- content(resA, as = "parsed")     lat = adrjson[[1]]$lat     lon = adrjson[[1]]$lon }
urlw <- "http://forecast.weather.gov/MapClick.php"
paramW <- list(lat = lat, lon = lon, FcstType = "json")
resW <- WScall(urlw, paramW)
if (resW$status_code == 200) {     weajson <- content(resW, as = "parsed")     weajson$currentobservation }
# Parte 2 - giocare con i dati
number = c(1, 2, 3)
animal = c('cat', 'dog', 'mouse')
df1 = data.frame(number, animal)
df1
df1$animal
## Verbs (operations) in Pandas
## Loading data (and basic statistics / visualization)
library(readr)
library(data.table)
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
dt = as.data.table(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(dt)
dt$GEOID = as.character(with(dt, as.numeric(Census_Tract_Number) * 100 + 10 ^ 6 * as.numeric(County_Code) + 10 ^ 9 * as.numeric(State_Code)  ))
head(dt)
df$GEOID = NULL # ma perche' dovrei droppare una colonna?
df = dt[-1,]
# Indici su data frame
setkey(dt, State_Code, County_Code)
head(dt)
data.frame(unclass(summary(dt)))
dt$GEOID = as.character(with(dt, as.numeric(Census_Tract_Number) * 100 + 10 ^ 6 * as.numeric(County_Code) + 10 ^ 9 * as.numeric(State_Code)))
# Indici su data frame
setkey(dt, State_Code, County_Code)
head(dt)
data.frame(unclass(summary(dt)))
str(df)
library(psych)
describeBy(dt, df$PCT_AMT_FHA)
library(psych)
describeBy(dt, df$PCT_AMT_FHA)
str(dt)
describeBy(dt, dt$PCT_AMT_FHA)
describeBy(dt)
hist(dt$PCT_AMT_FHA, col = rgb(1, 0, 0, 0.5))
dt$LOG_AMT_ALL = log1p(df$AMT_ALL)
hist(dt$LOG_AMT_ALL, col = rgb(1, 0, 0, 0.5))
## Indexing data frames
head(dt$State_Code)
head(dt[,c(State_Code, County_Code)])
head(dt[, County_Code, keyby = State_Code])
str(df$State_Code)
dt[12550,]
dt[12550, 'State_Code']
dt[12545:12550, County_Code, keyby = State_Code]
dt[3, 5]
dt[3:5, 2:4]
#filtering data
head(subset(dt, State_Code == 33))
head(subset(dt, (State_Code == 33) | (Census_Tract_Number == 9613)))
dt[State_Code == 33]
head(dt[dt[, "State_Code"] == 1,])
dt[State_Code == 33]
#joining data
dt2 = as.data.table(read_tsv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/2013_Gaz_tracts_national.tsv", col_names = T))
head(dt2)
dt$GEOID = as.character(with(dt, as.numeric(Census_Tract_Number) * 100 + 10 ^ 6 * as.numeric(County_Code) + 10 ^ 9 * as.numeric(State_Code)))
dt_joined = dt[dt2, on = "GEOID"]
head(dt_joined)
#aggregating data
usps_groups = dt_joined[, sum(ALAND), by = USPS]
usps_groups = group_by(df_joined, USPS)
usps_groups = group_by(dt_joined, USPS)
usps_groups
group_AK_5 <- filter(usps_groups, USPS == 'AK')[5,]
df_by_state <- summarise(usps_groups, count = n(), AMT_FHA = sum(AMT_FHA), AMT_ALL = sum(AMT_ALL), NUM_FHA = sum(NUM_FHA), NUM_ALL = sum(NUM_ALL))
head(df_by_state)
df_by_state$PCT_AMT_FHA <- 100.0 * df_by_state$AMT_FHA / df_by_state$AMT_ALL
hist(df_by_state$PCT_AMT_FHA, breaks = 20)
#a specific aggregation function per column:
df_by_state2 <- summarise(usps_groups, count = n(), sum_NUM_FHA = sum(NUM_FHA), mean_NUM_ALL = mean(NUM_ALL))
head(df_by_state2)
dplyr::arrange(usps_groups, desc(INTPTLAT))[1,]
names(usps_groups)
#               
farthest_north <- function(state_df) {     result <- dplyr::arrange(state_df, desc(INTPTLAT))[1,]     return(result) }
northest <- summarise(usps_groups, farthest_north = max(INTPTLAT))
northest
dtbystate = dt[order(State_Code)]
dtbyAMTFHA = dt[order(AMT_FHA)]
dtbystate
dtbyAMTFHA
library(zoo)
dt[, c('GEOID')][1:10]
is.na(dt[, c('GEOID')])[1:10]
length(dt[, c('GEOID')])
length(na.omit(dt[, c('GEOID')]))
dt$FILL0 <- dt$GEOID
dt$FILL0[which(is.na(dt$GEOID))] <- 0
dt$FILL_mean <- dt$GEOID
dt$FILL_mean[which(is.na(dt$GEOID))] <- mean(dt$GEOID, na.rm = TRUE)
dt$FILL_inter <- dt$GEOID
dt$FILL_inter <- na.approx(dt$FILL_inter)
dt[, c('GEOID', 'FILL0', 'FILL_mean', 'FILL_inter')][1:10,]
text_series <- as.character(sprintf("%.2f", dt$GEOID))
text_series[which(is.na(dt$GEOID))] <- "nan"
text_series[1:10]
strsplit(text_series[1:10], ".")
names = c("State_Code", "County_Code", "Census_Tract_Number", "NUM_ALL", "NUM_FHA", "PCT_NUM_FHA", "AMT_ALL", "AMT_FHA", "PCT_AMT_FHA")
dt = as.data.table(read_csv("~/Visual Studio 2017/Projects/MABIDA2017/Gigli/Management science/Data/fha_by_tract.csv", col_names = names))
head(dt)
dt$GEOID = as.character(with(dt, as.numeric(Census_Tract_Number) * 100 + 10 ^ 6 * as.numeric(County_Code) + 10 ^ 9 * as.numeric(State_Code)  ))
text_series <- as.character(sprintf("%.2f", dt$GEOID))
str(dt)
text_series <- sprintf("%.2f", dt$GEOID)
text_series <- dt$GEOID
text_series[which(is.na(dt$GEOID))] <- "nan"
text_series[1:10]
strsplit(text_series[1:10], ".")
mean(dt$GEOID, na.rm = TRUE)
## Manipulating strings
library(stringr)
library(dplyr)
dtstring = dt_joined %>% filter(str_detect(USPS, "A"))
## Indices in Pandas
s1 <- c(1, 2, 3)
names(s1) = c('a', 'b', 'c')
s2 <- c(3, 2, 1)
names(s2) = c('c', 'b', 'a')
s1 + s2
s3 <- c(3, 2, 1)
names(s3) = c('c', 'd', 'e')
s1 + s3
append(s1, s3)
## Function application and mapping
dff <- as.data.frame(matrix(1:24, ncol = 6, nrow = 4, byrow = TRUE))
head(sin(dff))
dff
apply(dff, 1:2, function(x) sprintf("%.2f", x))
apply(dff, 2, function(x) max(x) - min(x))
apply(dff, 1, function(x) max(x) - min(x))
### Pandas HTML data import example     
library(htmltab)
library(stringi)
install.packages('stringi')
library(stringi)
library(htmltab)
library(stringi)
require(data.table)
url <- "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
tallest <- htmltab(doc = url, which = 3)
tl = t(as.data.table(stri_extract_all(tallest$Coordinates, regex = "-?\\d{1,3}+\\.?\\d{4,6}")))
install.packages("stringi", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
## Pandas Timestamps
Sys.setlocale("LC_TIME", "C")
as.POSIXlt("July 4, 2016", format = "%B %d, %Y")
as.POSIXlt('Monday, July 4, 2016', format = "%A, %B %d, %Y")
as.POSIXlt('Tuesday, July 4th, 2016', format = "%A, %B %dth, %Y")
as.POSIXlt('Monday, July 4th, 2016 05:00 PM', format = "%A, %B %dth, %Y %I:%M %p")
as.POSIXlt('04/07/2016T17:20:13.123456', format = "%d/%m/%YT%H:%M:%OS")
as.Date(as.POSIXlt(1467651600000000000 / 1000000000, origin = "1970-01-01"))
july4 = as.POSIXct('Monday, July 4th, 2016 05:00 PM', format = "%A, %B %dth, %Y %I:%M %p", tz = "US/Eastern")
labor_day = as.POSIXct('9/5/2016 12:00', format = "%d/%m/%Y %H:%M", tz = "US/Eastern")
thanksgiving = as.POSIXct('11/24/2016 16:00', format = "%m/%d/%Y %H:%M")
labor_day - july4
library(bizdays)
library(lubridate)
library(bizdays)
library(lubridate)
last_day <- function(date) {     ceiling_date(date, "month") - days(1) }
last_day(ymd(20160704))
create.calendar(name = 'ANBIMA', holidays = holidaysANBIMA, weekdays = c('saturday', 'sunday'))
bizdays.options$set(default.calendar = 'ANBIMA')
cal = bizdays.options$get("default.calendar")
bizdays::offset(july4, 5, cal)
bizdays::offset(july4, -1, cal)
bizdays::offset(last_day(ymd(20160704)), 0, cal) # last business day of the month.
require(bizdays)
business_days = bizseq('2016-01-01', '2016-12-31', "ANBIMA")
business_days
dtimed = data.table(x = business_days, y = seq(1, length(business_days)))
setkey(dtimed, x)
d <- c("2009-03-07 12:00", "2009-03-08 12:00", "2009-03-28 12:00", "2009-03-29 12:00", "2009-10-24 12:00", "2009-10-25 12:00", "2009-10-31 12:00", "2009-11-01 12:00")
t1 <- as.POSIXct(d, "America/Los_Angeles")
cbind(US = format(t1), UK = format(t1, tz = "Europe/London"))
## Multi-indices, stacking, and pivot tables
grouped = group_by(dt_joined, State_Code, County_Code)
grouped1 <- summarise(grouped, NUM_ALL = sum(NUM_ALL), NUM_FHA = sum(NUM_FHA))
head(grouped1)
IRkernel::installspec(user = FALSE)
install.packages("digest", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
library("digest", lib.loc="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
install.packages("digest", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
IRkernel::installspec(user = FALSE)
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest')) devtools::install_github('IRkernel/IRkernel') # Don’t forget step 2/2!
IRkernel::installspec() IRkernel::installspec()
IRkernel::installspec(user = FALSE)
install.packages("stringi", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
library("stringi", lib.loc="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
IRkernel::installspec()
IRkernel::installspec(user = FALSE)
IRkernel::installspec()
install.packages("stringi", lib="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
library("stringi", lib.loc="C:/Users/GiulioVannini/Documents/R/win-library/3.3")
IRkernel::installspec()
IRkernel::installspec(user = FALSE)
IRkernel::installspec()
IRkernel::installspec()
IRkernel::installspec(user = FALSE)
IRkernel::installspec()
IRkernel::installspec(user = FALSE)
usps_groups = group_by(dt_joined, USPS)
library(dplyr)
usps_groups = group_by(dt_joined, USPS)
library(tidyverse)
install.l
install.packages(tidyverse)
library(tidyverse)
install.packages('tidyverse')
iris
head(iris)
iris$mplen = mean(iris$Petal.Length)
iris$mpwid = mean(iris$Petal.Width)
iris$mlw = iris$mpwid - iris$mplen
attach(iris)
select(Species, Petal.Width, Petal.Length)
library(tidyverse)
library(magrittr)
select(Species, Petal.Width, Petal.Length)
rm(iris, envir = as.environment(".GlobalEnv"))
iris$mlw = mean(iris$Petal.Width - mean(iris$Petal.Length) iris$mlw = mean(iris$Petal.Width - mean(iris$Petal.Length) iris$mlw = mean(iris$Petal.Width) - mean(iris$Petal.Length)
group_by(iris, mean(iris$Petal.Width - mean(iris$Petal.Length)) group_by(iris, mean(iris$Petal.Width - mean(iris$Petal.Length))   )
group_by(iris, mean(iris$Petal.Width) - mean(iris$Petal.Length)) 
group_by(iris, Species, mean(iris$Petal.Width) - mean(iris$Petal.Length)) 
group_by(iris, Species, mean(iris$Petal.Width) - mean(iris$Petal.Length)) 
group_by(iris, mean(iris$Petal.Width) - mean(iris$Petal.Length)) 
head(filter( group_by(iris, mean(iris$Petal.Width) - mean(iris$Petal.Length))  group_by(iris, meadia = mean(iris$Petal.Width) - mean(iris$Petal.Length)) 
group_by(iris, media = (mean(iris$Petal.Width) - mean(iris$Petal.Length))) 
rm(iris, envir = as.environment(".GlobalEnv"))
group_by(iris, media = (mean(iris$Petal.Width) - mean(iris$Petal.Length))) 
iris$mplen = mean(iris$Petal.Length)
iris$mpwid = mean(iris$Petal.Width)
group_by(iris, media = (mean(iris$Petal.Width) - mean(iris$Petal.Length))) 
group_by(iris, Species ) 
specie = group_by(iris, Species) 
summarise(specie, count = n())
summarise(specie, mplen = mean(iris$Petal.Length), mpwid = mean(iris$Petal.Width))
summarise(specie, avgwid= mean(Petal.Width), avglen = mean(Petal.Lenght))
mutate(f2, diff = avg_length - avg_width)
filter(f3, diff = max(diff))
irisexe <- group_by(iris, Species) %>%     summarise(specie, avgwid = mean(Petal.Width), avglen = mean(Petal.Lenght)) %>% mutate(diff = avg_length - avg_width) %>% filter(diff = max(diff)) %>% irisexe <- group_by(iris, Species) %>%     summarise(specie, avgwid = mean(Petal.Width), avglen = mean(Petal.Lenght)) %>% mutate(diff = avg_length - avg_width) %>% filter(diff = max(diff))
irisexe <- group_by(iris, Species) %>% summarise(specie, avgwid = mean(Petal.Width) && avglen = mean(Petal.Lenght)) %>% mutate(diff = avg_length - avg_width) %>% filter(diff = max(diff))
instal.packages(foreach)
install.packages(foreach)
install.packages(forecast)
install.packages('forecast')
install.packages('tseries')
help(stl)
d = mat_well[,2:3]
avgclust <- hclust(d, method = 'average')
## cluster  ben separati set.seed(10) n=20; scale=1;       mx=0; my=0;        x=rnorm(n)*scale+mx       y=rnorm(n)*scale+my       mx=8; my=0;       x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my))       mx=4; my=8;       x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my)) mat_well<-cbind(c(rep(1,n),rep(2,n),rep(3,n)),x,y) colnames(mat_well)<-c('G-vero','x','y') plot(x, y, pch = 19, col = mat_well[, 1])
d = mat_well[,2:3]
avgclust <- hclust(d, method = 'average')
avgclust <- hclust(dist(d), method = 'average')
sinclust <- hclust(dist(d), method = 'single')
clusters <- hclust(dist(iris[, 3:4]))
plot(clusters)
plot(sinclust)
clusters <- hclust(dist(iris[, 3:4]))
plot(clusters)
sinclust <- hclust(dist(d), method = 'single')
plot(sinclust)
silhouette(sinclust)
library(cluster)
silhouette(sinclust)
silhouette(sinclust)
silhouette(sinclust$dist.method)
silhouette(sinclust$call)
silhouette(sinclust$height)
silhouette(sinclust$labels)
silhouette(sinclust$merge)
silhouette(sinclust$method)
silhouette(sinclust$order)
silhouette(sinclust)
library(fpc)
cluster.stats(d, fit1$cluster, fit2$cluster)
install.packages('fpc')
library(fpc)
cluster.stats(d, fit1$cluster, fit2$cluster)
pamclust <- pam(dist(d), 4)
kmeansclust <- kmeans(dist(d),4)
silhouette(pamclust)
silhouette(pamclustm mat_well)
silhouette(pamclust, mat_well)
dfcluster = function(data) {     d = dist(data[, 2:3])     average <- cutree(hclust(d, method = 'average'), 3)     single <- cutree(hclust(d, method = 'single'), 3)     complete <- cutree(hclust(d, method = 'complete'), 3)     ward <- cutree(hclust(d, method = 'ward.D'), 3)     pam <- pam(d, 3)     kmeans <- kmeans(d, 3)     df = data.frame(mat_well[, 1], average, single, complete, ward, pam$clustering, kmeans$cluster)     colnames(df) = c("orig", "avg", "sin", "compl", "ward", "pam", "kmeans")     return(df) }
dfcluster = function(data) {     d = dist(data[, 2:3])     average <- cutree(hclust(d, method = 'average'), 3)     single <- cutree(hclust(d, method = 'single'), 3)     complete <- cutree(hclust(d, method = 'complete'), 3)     ward <- cutree(hclust(d, method = 'ward.D'), 3)     pam <- pam(d, 3)     kmeans <- kmeans(d, 3)     df = data.frame(mat_well[, 1], average, single, complete, ward, pam$clustering, kmeans$cluster)     colnames(df) = c("orig", "avg", "sin", "compl", "ward", "pam", "kmeans")     return(df) }
set.seed(10) n=20; scale=1;       mx=0; my=0;        x=rnorm(n)*scale+mx       y=rnorm(n)*scale+my       mx=8; my=0;       x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my))       mx=4; my=8;       x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my)) mat_well<-cbind(c(rep(1,n),rep(2,n),rep(3,n)),x,y) colnames(mat_well)<-c('G-vero','x','y') plot(x, y, pch = 19, col = mat_well[, 1])
dfcluster(mat_well)
library(cluster)
dfcluster(mat_well)
set.seed(10); n<-100 i<-1:n          a=i*.0628319;          x=cos(a)+(i>50)+rnorm(n)*.1;          y=sin(a)+(i>50)*.3+rnorm(n)*.1; mat_nconv<-cbind(c(rep(1,n/2),rep(2,n/2)),x,y) colnames(mat_nconv)<-c('G-vero','x','y')
dfcluster(mat_nconv)
set.seed(10)  n=50; scale=.8;       mx=0; my=0;        x=rnorm(n)*scale+mx       y=rnorm(n)*scale+my       mx=3; my=0;        x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my))       mx=1; my=2;        x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my)) mat_poor<-cbind(c(rep(1,n),rep(2,n),rep(3,n)),x,y) colnames(mat_poor)<-c('G-vero','x','y') plot(x,y,pch=19,col=mat_poor[,1])
dfcluster(mat_poor)
##################################################### library(cluster) dfcluster = function(data) {     d = dist(data[, 2:3])     average <- cutree(hclust(d, method = 'average'), 3)     single <- cutree(hclust(d, method = 'single'), 3)     complete <- cutree(hclust(d, method = 'complete'), 3)     ward <- cutree(hclust(d, method = 'ward.D'), 3)     pam <- pam(d, 3)     kmeans <- kmeans(d, 3)     df = data.frame(mat_well[, 1], average, single, complete, ward, pam$clustering, kmeans$cluster)     colnames(df) = c("orig", "avg", "sin", "compl", "ward", "pam", "kmeans")     return(df) }
dfcluster(mat_poor)
library(cluster) dfcluster = function(data) {     d = dist(data[, 2:3])     average <- cutree(hclust(d, method = 'average'), 3)     single <- cutree(hclust(d, method = 'single'), 3)     complete <- cutree(hclust(d, method = 'complete'), 3)     ward <- cutree(hclust(d, method = 'ward.D'), 3)     pam <- pam(d, 3)     kmeans <- kmeans(d, 3)     df = data.frame(data[, 1], average, single, complete, ward, pam$clustering, kmeans$cluster)     colnames(df) = c("orig", "avg", "sin", "compl", "ward", "pam", "kmeans")     return(df) }
dfcluster(mat_poor)
dfcluster(mat_nconv)
set.seed(10); n<-100 i<-1:n          a=i*.0628319;          x=cos(a)+(i>50)+rnorm(n)*.1;          y=sin(a)+(i>50)*.3+rnorm(n)*.1; mat_nconv<-cbind(c(rep(1,n/2),rep(2,n/2)),x,y) colnames(mat_nconv)<-c('G-vero','x','y')
dfcluster(mat_nconv)
set.seed(19); n=40       ma=8; mb=0;       a=rnorm(n)*6+ma;       b=rnorm(n)+mb;       ma=6; mb=8;         a<-c(a,rnorm(n)*6+ma)       b<-c(b,b=rnorm(n)+mb)          x=a-b;          y=a+b; mat_lunghi<-cbind(c(rep(1,n),rep(2,n)),x,y) colnames(mat_lunghi)<-c('G-vero','x','y')
dfcluster(mat_lunghi)
dftest = dfcluster(mat_nconv)
colSums(dftest$orig == dftest$kmeans) / length(dftest$orig)*100
paste0(round(100 * length(intersect(dftest$orig, dftest$avg)) / nrow(df)), "%")
paste0(round(100 * length(intersect(dftest$orig, dftest$avg)) / nrow(df)), "%")
paste0(round(100 * length(intersect(dftest$orig, dftest$avg)) / nrow(dftest)), "%")
paste0(round(100 * length(sum(dftest$orig == dftest$avg)) / nrow(dftest)), "%")
dftest = dfcluster(mat_lunghi)
paste0(round(100 * length(sum(dftest$orig == dftest$avg)) / nrow(dftest)), "%")
paste0(round(100 * length(sum(dftest$orig == dftest$kmeans)) / nrow(dftest)), "%")
paste0(round(100 * length(sum(dftest$orig == dftest$kmeans)) / nrow(dftest)), "%")
paste0(round(100 * length(sum(dftest$orig == dftest$single)) / nrow(dftest)), "%")
dftest = dfcluster(mat_well)
set.seed(10) n=20; scale=1;       mx=0; my=0;        x=rnorm(n)*scale+mx       y=rnorm(n)*scale+my       mx=8; my=0;       x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my))       mx=4; my=8;       x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my)) mat_well<-cbind(c(rep(1,n),rep(2,n),rep(3,n)),x,y) colnames(mat_well)<-c('G-vero','x','y') plot(x, y, pch = 19, col = mat_well[, 1])
dftest = dfcluster(mat_well)
paste0(round(100 * length(sum(dftest$orig == dftest$single)) / nrow(dftest)), "%")
paste0(round(100 * length(sum(dftest$orig == dftest$avg)) / nrow(dftest)), "%")
paste0(round(100 * length(sum(dfw$orig == dfw$avg)) / nrow(dfw)), "%")
dfw = dfcluster(mat_well)
paste0(round(100 * length(sum(dfw$orig == dfw$avg)) / nrow(dfw)), "%")
cluster.stats(d, average, mat_well[,1])
library(fpc)
cluster.stats(d, average, mat_well[,1])
cluster.stats(dfw, average, mat_well[,1])
library(fpc)
library(cluster) library(fpc) dfcluster = function(data) {     d = dist(data[, 2:3])     average <- cutree(hclust(d, method = 'average'), 3)     single <- cutree(hclust(d, method = 'single'), 3)     complete <- cutree(hclust(d, method = 'complete'), 3)     ward <- cutree(hclust(d, method = 'ward.D'), 3)     pam <- pam(d, 3)     kmeans <- kmeans(d, 3)     cluster.stats(d, average, data[, 1])     df = data.frame(data[, 1], average, single, complete, ward, pam$clustering, kmeans$cluster)     colnames(df) = c("orig", "avg", "sin", "compl", "ward", "pam", "kmeans")     return(df) }
dfw = dfcluster(mat_well)
d = dist(mat_nconv[, 2:3]) average <- cutree(hclust(d, method = 'average'), 3) single <- cutree(hclust(d, method = 'single'), 3) complete <- cutree(hclust(d, method = 'complete'), 3) ward <- cutree(hclust(d, method = 'ward.D'), 3) pam <- pam(d, 3) kmeans <- kmeans(d, 3) cluster.stats(d, average, mat_nconv[, 1])
avgcomp = cluster.stats(d, average, mat_nconv[, 1])
sincomp = cluster.stats(d, single, mat_nconv[, 1])
comcomp = cluster.stats(d, complete, mat_nconv[, 1])
warcomp = cluster.stats(d, ward, mat_nconv[, 1])
pamcomp = cluster.stats(d, pam, mat_nconv[, 1])
kmecomp = cluster.stats(d, kmeans, mat_nconv[, 1])
d = dist(mat_nconv[, 2:3]) average <- cutree(hclust(d, method = 'average'), 3) single <- cutree(hclust(d, method = 'single'), 3) complete <- cutree(hclust(d, method = 'complete'), 3) ward <- cutree(hclust(d, method = 'ward.D'), 3) pam <- pam(d, 3) kmeans <- kmeans(d, 3)
avgcomp = cluster.stats(d, average, mat_nconv[, 1]) sincomp = cluster.stats(d, single, mat_nconv[, 1]) comcomp = cluster.stats(d, complete, mat_nconv[, 1]) warcomp = cluster.stats(d, ward, mat_nconv[, 1]) pamcomp = cluster.stats(d, pam, mat_nconv[, 1]) kmecomp = cluster.stats(d, kmeans, mat_nconv[, 1])
avgcomp = cluster.stats(d, average)
plot(avgcomp)
cluster.stats(d, average)
library(cluster) library(fpc) dfcluster = function(data) {     d = dist(data[, 2:3])     average <- cutree(hclust(d, method = 'average'), 3)     single <- cutree(hclust(d, method = 'single'), 3)     complete <- cutree(hclust(d, method = 'complete'), 3)     ward <- cutree(hclust(d, method = 'ward.D'), 3)     pam <- pam(d, 3)     kmeans <- kmeans(d, 3)     df = data.frame(data[, 1], average, single, complete, ward, pam$clustering, kmeans$cluster)     colnames(df) = c("orig", "avg", "sin", "compl", "ward", "pam", "kmeans")     return(df) } ## cluster  ben separati set.seed(10) n=20; scale=1;       mx=0; my=0;        x=rnorm(n)*scale+mx       y=rnorm(n)*scale+my       mx=8; my=0;       x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my))       mx=4; my=8;       x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my)) mat_well<-cbind(c(rep(1,n),rep(2,n),rep(3,n)),x,y) colnames(mat_well)<-c('G-vero','x','y') plot(x, y, pch = 19, col = mat_well[, 1]) dfw = dfcluster(mat_well)
set.seed(10)  n=50; scale=.8;       mx=0; my=0;        x=rnorm(n)*scale+mx       y=rnorm(n)*scale+my       mx=3; my=0;        x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my))       mx=1; my=2;        x=c(x,(rnorm(n)*scale+mx))       y=c(y,(rnorm(n)*scale+my)) mat_poor<-cbind(c(rep(1,n),rep(2,n),rep(3,n)),x,y) colnames(mat_poor)<-c('G-vero','x','y') plot(x,y,pch=19,col=mat_poor[,1]) dfp= dfcluster(mat_poor)
set.seed(19); n=40       ma=8; mb=0;       a=rnorm(n)*6+ma;       b=rnorm(n)+mb;       ma=6; mb=8;         a<-c(a,rnorm(n)*6+ma)       b<-c(b,b=rnorm(n)+mb)          x=a-b;          y=a+b; mat_lunghi<-cbind(c(rep(1,n),rep(2,n)),x,y) colnames(mat_lunghi)<-c('G-vero','x','y') plot(x,y,pch=19,col=mat_lunghi[,1]) dfl = dfcluster(mat_lunghi)
set.seed(10); n<-100 i<-1:n          a=i*.0628319;          x=cos(a)+(i>50)+rnorm(n)*.1;          y=sin(a)+(i>50)*.3+rnorm(n)*.1; mat_nconv<-cbind(c(rep(1,n/2),rep(2,n/2)),x,y) colnames(mat_nconv)<-c('G-vero','x','y') plot(x,y,pch=19,col=mat_nconv[,1]) dfn = dfcluster(mat_nconv)
library(compare)
install.packages('compare')
compareEqual(dfp$orig, dfp$kmeans)
library(compare)
compareEqual(dfp$orig, dfp$kmeans)
compare(dfp$orig, dfp$kmeans)
match(dfp$orig, dfp$kmeans)
test= match(dfp$orig, dfp$kmeans)
rm(test, envir = as.environment(".GlobalEnv"))
paste0(round(100 * length(sum(dfp$orig == dfp$kmeans)) / nrow(dfw)), "%")
paste0(round(100 * length(sum(dfp$orig == dfp$kmeans)) / nrow(dfp)), "%")
sum(dfp$orig == dfp$kmeans)
sum(dfp$orig == dfp$avg)
paste0(round(100 * length(sum(dfp$orig == dfp$kmeans)) / nrow(dfp$orig)), "%")
paste0(round(100 * length(sum(dfp$orig == dfp$kmeans)) / nrow(dfp)), "%")
sum(dfp$orig == dfp$avg)
nrow(dfp)
sum(dfp$orig == dfp$avg) / nrow(dfp)
(sum(dfp$orig == dfp$avg) / nrow(dfp)) * 100
paste0((sum(dfp$orig == dfp$avg) / nrow(dfp)) * 100, "%")
paste0(round((sum(dfp$orig == dfp$avg) / nrow(dfp)) * 100), "%")
dfperc = function(data) {  denom = nrow(data) avgp = paste0(round((sum(data$orig == data$avg) / denom) * 100), "%") sinp = paste0(round((sum(data$orig == data$sin) / denom) * 100), "%") comp = paste0(round((sum(data$orig == data$compl) / denom) * 100), "%") wardp = paste0(round((sum(data$orig == data$ward) / denom) * 100), "%") pamp = paste0(round((sum(data$orig == data$pam) / denom) * 100), "%") kmep = paste0(round((sum(data$orig == data$kmeans) / denom) * 100), "%")     perc = c(avgp,sinp,comp,wardp,pamp,kmep)       return(perc) }
dfperc(dfw)
dfperc(dfn)
dfperc(dfl)
dfw = dfperc(dfcluster(mat_well))
dfw = dfcluster(mat_well)
dfperc(dfw)
dfw = dfperc(dfcluster(mat_well))
dfperc(dfcluster(mat_well))
dfperc(dfcluster(mat_poor))
dfperc(dfcluster(mat_lunghi))
dfperc(dfcluster(mat_nconv))
install.packages('C50')
library(C50)
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Grassini/Esame/Decision Trees/")
crx <- read.table( file="expanded_AGARICUS_LEPIOTA.txt", header=TRUE, sep="," )
head(crx)
summary(crx)
set.seed(22)
crx <- crx[sample(nrow(crx)),] # Mischia i record, prima di estratte training e test set
crx$veil.type=NULL # ha un solo valore
X <- crx[,-1]
y <- crx[,1]
# Creazione training set e test set
trainX <- X[1:8000,]
trainy <- y[1:8000]
testX <- X[8001:8416,]
testy <- y[8001:8416]
# Realizzo il modello
model <- C5.0(trainX, trainy)
summary(model)
plot(model)
#levels(crx[,1])
cost_matrix<-matrix(c(0,10,1,0),2,2,byrow=TRUE)
cost_matrix
# Con variabile dipendente categorica/discreta #install.packages("C50") library(C50) setwd("~/Visual Studio 2017/Projects/MABIDA2017/Grassini/Esame/Decision Trees/") crx <- read.table( file="expanded_AGARICUS_LEPIOTA.txt", header=TRUE, sep="," ) head(crx) summary(crx) set.seed(22) crx <- crx[sample(nrow(crx)),] # Mischia i record, prima di estratte training e test set crx$veil.type=NULL # ha un solo valore X <- crx[,-1] y <- crx[,1] # Creazione training set e test set trainX <- X[1:8000,] trainy <- y[1:8000] testX <- X[8001:8416,] testy <- y[8001:8416] # Realizzo il modello model <- C5.0(trainX, trainy) summary(model) plot(model) # Aggiungo una matrice di costi per penalizzare l'erronea classificazione dei funghi #levels(crx[,1]) cost_matrix<-matrix(c(0,10,1,0),2,2,byrow=TRUE) cost_matrix rownames(cost_matrix)<-levels(crx[,1]) colnames(cost_matrix)<-levels(crx[,1]) model.1<-C5.0(trainX,trainy,costs=cost_matrix) summary(model.1) plot(model.1)
# Usiamo model su test set per vedere se l'albero realizzato è generalizzabile
predizione<-predict(model.1,testX,type='class') ## classe assegnata dalla regola
summary(predizione)
table(testy,predizione)
head(crx)
summary(crx)
probabilita<-predict(model,testX,type='prob')  ## probabilità
str(probabilita)
head(probabilita)
dove_commestibile<-which(probabilita[,1]>.5)
dove_nonCommestibile<-which(probabilita[,2]>.5)
#### File (adjust path) file.data <- "~/Visual Studio 2017/Projects/MABIDA2017/Grassini/Esame/Decision Trees/Income-training.csv" remove <- c("", "Numero.di.FamiglieProvincia", "Numero.di.FamiglieRegione") #### Read data data <- read.table(file = file.data, header = TRUE, sep = ",",                     na.strings = "NA", colClasses = NA, check.names = FALSE, comment.char = "") ind <- !( colnames(data) %in% remove ) data <- data[, ind, drop = FALSE]
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
db=read.csv2("~/Visual Studio 2017/Projects/MABIDA2017/Grassini/Esame/ARULES/2014_sqf_web.csv", header=T, sep = ";")
# Con variabile dipendente categorica/discreta
#install.packages("C50")
library(C50)
setwd("~/Visual Studio 2017/Projects/MABIDA2017/Grassini/Esame/Decision Trees/")
crx <- read.table( file="expanded_AGARICUS_LEPIOTA.txt", header=TRUE, sep="," )
head(crx)
summary(crx)
set.seed(22)
crx <- crx[sample(nrow(crx)),] # Mischia i record, prima di estratte training e test set
crx$veil.type=NULL # ha un solo valore
X <- crx[,-1]
y <- crx[,1]
# Creazione training set e test set
trainX <- X[1:8000,]
trainy <- y[1:8000]
testX <- X[8001:8416,]
testy <- y[8001:8416]
# Realizzo il modello
model <- C5.0(trainX, trainy)
summary(model)
plot(model)
# Aggiungo una matrice di costi per penalizzare l'erronea classificazione dei funghi
#levels(crx[,1])
cost_matrix<-matrix(c(0,10,1,0),2,2,byrow=TRUE)
cost_matrix
rownames(cost_matrix)<-levels(crx[,1])
colnames(cost_matrix)<-levels(crx[,1])
model.1<-C5.0(trainX,trainy,costs=cost_matrix)
summary(model.1)
plot(model.1)
# Usiamo model su test set per vedere se l'albero realizzato è generalizzabile
predizione<-predict(model.1,testX,type='class') ## classe assegnata dalla regola
summary(predizione)
table(testy,predizione)
probabilita<-predict(model,testX,type='prob')  ## probabilità
str(probabilita)
head(probabilita)
dove_commestibile<-which(probabilita[,1]>.5)
dove_nonCommestibile<-which(probabilita[,2]>.5)
plotly_arules(gro)
# Carico le librerie e inizializzo l'oggetto Groceries
library(arules)
library(datasets)
library(arulesViz)
data(Groceries)
####################################################################################
# 1) Esplorare l'oggetto e capire come è fatto (che cosa contiene ecc.)
####################################################################################
summary(Groceries) #prodotti dal più presente sugli scontrini al meno presente.
#la media dei prodotti per scontrino e altre informazioni sulla distribuzione che ha una coda lunga sulla destra.
str(Groceries)
Groceries@itemInfo$labels #nommi item colonna viene elenco singoli prodotti delle 169 colonne del dataset
Groceries@itemInfo$level2 #settore merceologico livello di aggregazione superiore, di nuovo elenco 55 nomi ma alcuni ripetuti, perchè mela e pera adesso risultano semplicemente frutta
#frankfurter e altro sono tutte salsicce livello aggregazione bene + generico
Groceries@itemInfo$level1 #reparto ancora più generico abbiamo 10 reparti dataset aggregato secondo reparto del bene
####################################################################################
# 2) Creare un grafico mostrando i 10 item più frequenti.
####################################################################################
# Carico le librerie e inizializzo l'oggetto Groceries
library(arules)
library(datasets)
library(arulesViz)
data(Groceries)
####################################################################################
# 1) Esplorare l'oggetto e capire come è fatto (che cosa contiene ecc.)
####################################################################################
summary(Groceries) #prodotti dal più presente sugli scontrini al meno presente.
itemFrequencyPlot(Groceries, topN = 10, frequency = "absolute")
itemFrequencyPlot(Groceries, topN = 10, frequency = "relative")
itemFrequencyPlot(Groceries, topN = 10, frequency = "absolute")
itemFrequencyPlot(Groceries, topN = 10, frequency = "absolute")
itemFrequencyPlot(Groceries, topN = 10, frequency = "relative")
itemFrequencyPlot(Groceries, topN = 10, frequency = "absolute")
itemFrequencyPlot(Groceries, topN = 10, frequency = "relative")
itemFrequencyPlot(Groceries, topN = 10, frequency = "absolute")
itemFrequencyPlot(Groceries, topN = 10, frequency = "relative")
####################################################################################
#3) Trovare le regole basate con supporto minimo equivalente a un numero prefissato di transazioni
####################################################################################
rules = apriori(Groceries, parameter = list(support = 0.002, minlen = 2, target = "rules"))
rules = apriori(Groceries, parameter = list(support = 0.002, minlen = 2, target = "rules"))
summary(rules)
inspect(rules)
gro = apriori(Groceries, parameter = list(supp = 0.002, conf = 0.82, minlen = 2, target = 'rules'))
gro = apriori(Groceries, parameter = list(supp = 0.002, conf = 0.82, minlen = 2, target = 'rules'))
# Ricordare: la lift è guale a P(B|A)
gro = sort(gro, by = 'lift', decreasing = T)
inspect(gro)
summary(gro)
inspect(gro)
options(digits = 2)
gro = sort(gro, by = 'lift', decreasing = T)
inspect(gro)
options(digits = 2)
rules = apriori(Groceries, parameter = list(support = 0.002, minlen = 2, target = "rules"))
summary(rules)
inspect(rules)
inspectDT(gro)
plot(gro, xlim = c(0.002))
plot(gro, xlim = c(0.002))
plot(gro, xlim = c(0.002, 0.0035), ylim = C(0.80, 0.90))
plot(gro, xlim = c(0.002))
plotly_arules(gro)
plot(gro, xlim = c(0.002, 0.0035), ylim = C(0.80, 0.90))
plot(gro, method = "grouped")
plot(gro, method = "graph", interactive = TRUE, shading = NA)
plot(gro, method = 'paracoord', control = list(reorder = TRUE))
gro = sort(gro, by = "confidence", decreasing = TRUE)
gro = sort(gro, by = "confidence", decreasing = TRUE)
inspect(gro[1:20])
inspect(gro[1:20])
inspectDT(gro)
inspect(gro)
gromilk <- apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.08), appearance = list(default = "lhs", rhs = "whole milk"), control = list(verbose = F))
gromilk <- sort(gromilk, decreasing = TRUE, by = "confidence")
inspect(gromilk)
milk[1:10]
inspect(gromilk[1:10])
inspect(gromilk[1:10])
summary(Groceries) #prodotti dal più presente sugli scontrini al meno presente.
#la media dei prodotti per scontrino e altre informazioni sulla distribuzione che ha una coda lunga sulla destra.
str(Groceries)
gro = sort(gro, by = "confidence", decreasing = TRUE)
# intanto ordinerei per confidence visto che mi dice quanto sono sicuro che la regola valga
inspect(gro)
